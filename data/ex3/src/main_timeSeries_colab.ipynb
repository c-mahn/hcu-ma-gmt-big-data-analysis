{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHrIvNTRSWrz"
      },
      "source": [
        "# Exercise_3:\n",
        "# Time Series Prediction using LSTM\n",
        "# TODO start**\n",
        "# Names: \n",
        "# Date: \n",
        "# TODO end**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hec1UcvkSWr8"
      },
      "source": [
        "#%% import modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# seaborn is a libraray which works on top of matplotlib and offers more visualitation tools\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for noramlization\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94PTwOLASWsE"
      },
      "source": [
        "# %% CUDA for PyTorch\n",
        "# Right at the beginning: check if a cuda compatible GPU is available in your computer. \n",
        "# If so, set device = cuda:0 which means that later all calculations will be performed on the graphics card. \n",
        "# If no GPU is available, the calculations will run on the CPU, which is also absolutely sufficient for the examples in these exercises.\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "#cudnn.benchmark = True\n",
        "# device ='cuda:0'\n",
        "\n",
        "if device.type == 'cpu':\n",
        "    device_num = 0\n",
        "    print('No GPU available.')\n",
        "else:\n",
        "    device_num = torch.cuda.device_count()\n",
        "    print('Device:', device, '-- Number of devices:', device_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O5gD7R4SjN7"
      },
      "source": [
        "# Mounting Google Drive locally \n",
        "from google.colab import drive\n",
        "#drive.mount(\"/content/drive\", force_remount=True)\n",
        "drive.mount('/content/drive')\n",
        "# you can also choose one of the other options to load data\n",
        "# therefore see https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_akmf_LGSWsJ"
      },
      "source": [
        "# %% read pellworm data\n",
        "# TODO start**    \n",
        "#...\n",
        "# data = ???   \n",
        "# TODO end**    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhSO7S8VSWsO"
      },
      "source": [
        "#%% load flight passenger data from seaborn library\n",
        "# there are some datasets in the seaborn library included\n",
        "print(sns.get_dataset_names())\n",
        "# TODO start** \n",
        "# we can use e.g. the flight dataset\n",
        "flight_data = sns.load_dataset(...)\n",
        "#...\n",
        "# data = ???\n",
        "# TODO end**    \n",
        "\n",
        "# TODO start**\n",
        "# Explain differences\n",
        "# TODO end**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shT8Ymc6SWsU"
      },
      "source": [
        "#%% cut the end of the time series as test data\n",
        "# TODO start** \n",
        "test_data_size =\n",
        "# TODO end** \n",
        "# trva = train and val\n",
        "trva_data = data[:-test_data_size]\n",
        "test_data = data[-test_data_size:] \n",
        "print('Length of train data:', len(trva_data))\n",
        "print('Length of test data:', len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPZ6mk8iSWsY"
      },
      "source": [
        "#%% normalize data with MinMax Scaler\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "trva_data_normalized = scaler.fit_transform(trva_data.reshape(-1, 1))\n",
        "print(trva_data_normalized[:5])\n",
        "print(trva_data_normalized[-5:])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaDgPkZ_SWsc"
      },
      "source": [
        "#%% convert to Torch Tensor\n",
        "# view(-1) is similar to reshape for tensor and the array size is inferred for the -1 dimension\n",
        "trva_data_normalized = torch.FloatTensor(trva_data_normalized).view(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlNZYoqYSWsf"
      },
      "source": [
        "#%% create sequences\n",
        "# function to create sequences of input (passenger of train_window months) and ...\n",
        "# ... output passenger of train_window+1 month\n",
        "def create_inout_sequences(input_data, tw):\n",
        "    inout_seq = []\n",
        "    L = len(input_data)\n",
        "    for i in range(L-tw):\n",
        "        train_seq = input_data[i:i+tw]\n",
        "        train_label = input_data[i+tw:i+tw+1]\n",
        "        inout_seq.append((train_seq ,train_label))\n",
        "    return inout_seq\n",
        "\n",
        "# Set an appropiate Train window\n",
        "# TODO start**\n",
        "train_window =\n",
        "# TODO end**\n",
        "\n",
        "# create sequences \n",
        "trva_inout_seq = create_inout_sequences(trva_data_normalized, train_window)\n",
        "\n",
        "# print the first 3 sequences\n",
        "print(trva_inout_seq[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQo-3khASWsj"
      },
      "source": [
        "#%% Split train sequences in train and val sequences\n",
        "# remember: we ahve already taken test data -> end of time series\n",
        "sample_num = {'all': len(trva_inout_seq), \n",
        "              'train': int(0.8*len(trva_inout_seq)),\n",
        "              'val': int(0.2*len(trva_inout_seq))}\n",
        "\n",
        "# idx shuffle\n",
        "idx = list(range(sample_num['all']))\n",
        "np.random.shuffle(idx)\n",
        "# assign idx to each sample\n",
        "sample_idx = {'all': idx[:], \n",
        "              'train': idx[0:sample_num['train']],\n",
        "              'val': idx[sample_num['train']:]}\n",
        "\n",
        "train_inout_seq = [ trva_inout_seq[i] for i in sample_idx['train']]\n",
        "val_inout_seq = [ trva_inout_seq[i] for i in sample_idx['val']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZWdLUBhSWso"
      },
      "source": [
        "#%% LSTM architecture\n",
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim,\n",
        "                    num_layers):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define the LSTM layer\n",
        "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
        "\n",
        "        # Define the output layer\n",
        "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # This is what we'll initialise our hidden state as\n",
        "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Forward pass through LSTM layer\n",
        "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
        "        # shape of self.hidden: (a, b), where a and b both \n",
        "        # have shape (num_layers, batch_size, hidden_dim).\n",
        "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
        "        \n",
        "        # Only take the output from the final timestep\n",
        "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
        "        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
        "        return y_pred.view(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2DmUkvLSWsr"
      },
      "source": [
        "#%% create an instance of class LSTM\n",
        "# Set an appropiate Train window\n",
        "# TODO start**\n",
        "input_dim =  #feature dimension of input\n",
        "hidden_dim =  # number of hidden neurons in LSTM\n",
        "batch_size = 1 #we do not merge the sequences into batches. Therefore the batch_size is 1.\n",
        "output_dim =  # feature dimension of output\n",
        "num_lstm_layers =         \n",
        "net = LSTM()\n",
        "# TODO end**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y25F86Y_SWsv"
      },
      "source": [
        "#%% Send networks to GPU (if you have one which supports cuda) for faster computations\n",
        "# The network itself must also be sent to the GPU. Either you write net = RegressNet() and then later net.to(device) or directly net = RegressNet().to(device)\n",
        "# The latter option may have the advantage that the instance net is created directly on the GPU, whereas in variant 1 it must first be sent to the GPU.\n",
        "if device_num>1:\n",
        "    print(\"Let's use\", device_num, \"GPU's\")\n",
        "    net = nn.DataParallel(net)\n",
        "net.to(device) \n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v49fGi7OSWsz"
      },
      "source": [
        "#%% specify hyperparameter loss function and optimizer\n",
        "# TODO start**\n",
        "num_epochs = \n",
        "num_learning_rate = \n",
        "loss_function = \n",
        "optimizer = \n",
        "#print(net)\n",
        "# TODO end**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfkbJR9cSWs4"
      },
      "source": [
        "#%% Train the Model\n",
        "\n",
        "# TODO start**\n",
        "loss_epoch_train = []\n",
        "loss_epoch_val = []\n",
        "for epoch in range(num_epochs):\n",
        "    loss_seq_train = []\n",
        "    loss_seq_val = []\n",
        "    \n",
        "    for seq, labels in train_inout_seq:\n",
        "        seq, labels = seq.to(device), labels.to(device)\n",
        "        net.hidden = net.init_hidden()\n",
        "        optimizer.zero_grad()\n",
        "        y_pred_train = net(seq)\n",
        "        \n",
        "        seq_loss = loss_function(y_pred_train, labels)\n",
        "        loss_seq_train.append(seq_loss.data.cpu().numpy())\n",
        "        seq_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "       \n",
        "    for seq, labels in val_inout_seq:\n",
        "        seq, labels = seq.to(device), labels.to(device)\n",
        "        y_pred_val = net(seq)\n",
        "        \n",
        "        seq_loss = loss_function(y_pred_val, labels)\n",
        "        loss_seq_val.append(seq_loss.data.cpu().numpy())\n",
        "        \n",
        "    loss_epoch_train.append(np.mean(loss_seq_train))\n",
        "    loss_epoch_val.append(np.mean(loss_seq_val))\n",
        "    print('Epoch '+str(epoch)+'/'+str(num_epochs)+': Train-Loss: '+str(np.round(loss_epoch_train[-1],4))+'; Val-Loss: '+str(np.round(loss_epoch_val[-1],4)))        \n",
        "# TODO end**\n",
        "    \n",
        "plt.figure()\n",
        "plt.plot(range(num_epochs), loss_epoch_train, color='r', marker='.', label = 'train-loss')\n",
        "plt.plot(range(num_epochs), loss_epoch_val, color='b', marker='.', label = 'val-loss')\n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.savefig('/content/drive/My Drive/bda_lab/ex3/results/lstm_loss.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5RDqTSLSWs-"
      },
      "source": [
        "#%% Make predictions\n",
        "# TODO start**\n",
        "fut_pred = \n",
        "# TODO end**\n",
        "# the last train_window trva data are the first input for the prediction...\n",
        "test_inputs = trva_data_normalized[-train_window:].tolist()\n",
        "print(test_inputs)\n",
        "\n",
        "\n",
        "# ...and each new prediction is append to the input\n",
        "net.eval()\n",
        "\n",
        "for i in range(fut_pred):\n",
        "    seq = torch.FloatTensor(test_inputs[-train_window:])\n",
        "    seq = seq.to(device)\n",
        "    with torch.no_grad():\n",
        "        net.hidden = net.init_hidden()\n",
        "        test_inputs.append(net(seq).item())\n",
        "\n",
        "#print only the predictions (the last fut_pred elements in the list)\n",
        "print(test_inputs[-fut_pred:])\n",
        "\n",
        "\n",
        "# since we normalized the dataset for training the predicted values are also normalized\n",
        "# undo with inverse normalization\n",
        "actual_predictions = scaler.inverse_transform(np.array(test_inputs[train_window:] ).reshape(-1, 1))\n",
        "print(actual_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyp5VKQRSWtB"
      },
      "source": [
        "#%% plot for whole time window\n",
        "plt.title('Passenger Prediction 12 Months')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Passengers')\n",
        "plt.grid(True)\n",
        "plt.autoscale(axis='x', tight=True)\n",
        "# matplotlib default colors\n",
        "# print(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
        "plt.plot(trva_data, color='#1f77b4', label='train+val reference')\n",
        "x_ref = np.arange(trva_data.shape[0], trva_data.shape[0]+test_data.shape[0], 1)\n",
        "x_pred = np.arange(trva_data.shape[0], trva_data.shape[0]+fut_pred, 1)\n",
        "plt.plot(x_ref,test_data, color='#ff7f0e', label='test reference')\n",
        "plt.plot(x_pred,actual_predictions, color='#2ca02c', label='test prediction')\n",
        "plt.savefig('/content/drive/My Drive/bda_lab/ex3/results/lstm_forecast.png')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5a_dtQQSWtE"
      },
      "source": [
        "#%% zoom plot for test area\n",
        "plt.title('Zoom: Test Prediction')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Passengers')\n",
        "plt.grid(True)\n",
        "plt.autoscale(axis='x', tight=True)\n",
        "plt.plot(x_ref,test_data, color='#ff7f0e', label='test reference')\n",
        "plt.plot(x_pred,actual_predictions, color='#2ca02c', label='test prediction')\n",
        "plt.savefig('/content/drive/My Drive/bda_lab/ex3/results/lstm_test_zoom.png')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}