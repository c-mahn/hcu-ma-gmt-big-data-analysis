{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRbft71zpovU"
      },
      "source": [
        "# Exercise_2\n",
        "# main simple regression WHO data\n",
        "# TODO**\n",
        "# Names of group members: \n",
        "# Date:\n",
        "# TODO**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V55Tqr8_pove"
      },
      "source": [
        "#%% Load modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import csv\n",
        "\n",
        "# torch modules\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# plot module\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApFO2bmNpovl"
      },
      "source": [
        "# %% CUDA for PyTorch\n",
        "# Right at the beginning: check if a cuda compatible GPU is available in your computer. \n",
        "# If so, set device = cuda:0 which means that later all calculations will be performed on the graphics card. \n",
        "# If no GPU is available, the calculations will run on the CPU, which is also absolutely sufficient for the examples in these exercises.\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "#cudnn.benchmark = True\n",
        "\n",
        "if device.type == 'cpu':\n",
        "    device_num = 0\n",
        "    print('No GPU available.')\n",
        "else:\n",
        "    device_num = torch.cuda.device_count()\n",
        "    print('Device:', device, '-- Number of devices:', device_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEq8B2lJqeZ_"
      },
      "source": [
        "# Mounting Google Drive locally \n",
        "from google.colab import drive\n",
        "#drive.mount(\"/content/drive\", force_remount=True)\n",
        "drive.mount('/content/drive')\n",
        "# you can also choose one of the other options to load data\n",
        "# therefore see https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDuUbHGwpovs"
      },
      "source": [
        "# %% read data\n",
        "# path to WHO data\n",
        "data_path = '/content/drive/My Drive/bda_lab/ex2/data/life-expectancy-who/Life Expectancy Data.csv'\n",
        "\n",
        "# read csv sheet with pandas\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# drop each row where is nan data\n",
        "df = df.dropna()\n",
        "# drop each row where is not at least 21 not nan data\n",
        "# df = df.dropna(thresh=21) # there are many nan in row 4 12 14, which can cause errors\n",
        "\n",
        "# get numpy out of pandas dataframe\n",
        "data = df.values\n",
        "# data=df.to_numpy()\n",
        "\n",
        "# get column names to see, which columns we have to extract as x and y\n",
        "column_names = np.array(df.columns[:], dtype=np.str)\n",
        "\n",
        "# TODO**\n",
        "print('Dimension of the dataset:', )\n",
        "# TODO**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxMKROTGpovx"
      },
      "source": [
        "# %% split in X and Y\n",
        "# extract any feature you want as X \n",
        "# extract target values as Y\n",
        "# TODO**\n",
        "x = np.array(data[], dtype=np.float32)\n",
        "y = np.array(data[], dtype=np.float32)\n",
        "# TODO**\n",
        "print('x shape:', x.shape)\n",
        "print('y shape:', y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArJIaoXdpov3"
      },
      "source": [
        "# %% normalize X and Y between (0,1). If multiple features in X are selected, each feature is normalized individually\n",
        "scale_x = np.max(x, axis=0)\n",
        "scale_y = np.max(y, axis=0)\n",
        "x = x/scale_x\n",
        "y = y/scale_y\n",
        "print('Scale_x:',scale_x)\n",
        "print('Scale_y:',scale_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akHb4sLlpov9"
      },
      "source": [
        "# %% convert to torch tensors\n",
        "# if tensors have only one dimension, an artificial dimension is created with unsqueeze (e.g. [10]->[10,1], so 1D->2D)\n",
        "Y = torch.from_numpy(y)\n",
        "Y = Y.float()\n",
        "if len(Y.shape)==1:\n",
        "    Y = Y.unsqueeze(dim=1)\n",
        "\n",
        "X = torch.from_numpy(x)\n",
        "X = X.float()\n",
        "if len(X.shape)==1:\n",
        "    X = X.unsqueeze(dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_jswetZpowA"
      },
      "source": [
        "# %% Split dataset in training, validation and test tensors\n",
        "# TODO**\n",
        "prop_train = \n",
        "prop_val = \n",
        "prop_test = \n",
        "# TODO**\n",
        "\n",
        "sample_num = {'all': X.shape[0], \n",
        "              'train': round(prop_train*X.shape[0]),\n",
        "              'val': round(prop_val*X.shape[0]),\n",
        "              'test': round(prop_test*X.shape[0])}\n",
        "\n",
        "# idx shuffle\n",
        "idx = np.random.choice(sample_num['all'], sample_num['all'], replace=False)\n",
        "# assign idx to each sample\n",
        "sample_idx = {'all': idx[:], \n",
        "              'train': idx[0:sample_num['train']],\n",
        "              'val': idx[sample_num['train']:sample_num['train']+sample_num['val']],\n",
        "              'test': idx[sample_num['train']+sample_num['val']:]}\n",
        "\n",
        "# Create train data\n",
        "X_train = X[sample_idx['train']]\n",
        "Y_train = Y[sample_idx['train']]\n",
        "\n",
        "# Create validation data\n",
        "X_val = X[sample_idx['val']]\n",
        "Y_val = Y[sample_idx['val']]\n",
        "\n",
        "# Create test data\n",
        "X_test = X[sample_idx['test']]\n",
        "Y_test = Y[sample_idx['test']]\n",
        "\n",
        "\n",
        "# %% Show data point\n",
        "print('Input of first ten train Sample:', X_train[0:10])\n",
        "print('Target of first ten train Sample:', Y_train[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBIUKIz5powE"
      },
      "source": [
        "#%% class of neural network 'RegressNet'\n",
        "# set up layer and architecture of network in constructor __init__\n",
        "# define operations on layer in forward pass method\n",
        "class RegressNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        super(RegressNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(inputSize, 128)\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.fc3 = nn.Linear(32, outputSize)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # max pooling over (2, 2) window\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNkvuIWrpowJ"
      },
      "source": [
        "#%% Specify network hyperparameter and create instance of RegressNet\n",
        "# TODO**        \n",
        "inputDim =\n",
        "outputDim =\n",
        "\n",
        "# Create instance of RegressNet\n",
        "net = RegressNet(inputDim, outputDim)\n",
        "# TODO**\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej8iLTTQpowN"
      },
      "source": [
        "#%% Send tensors and networks to GPU (if you have one which supports cuda) for faster computations\n",
        "X_train, Y_train = X_train.to(device), Y_train.to(device)\n",
        "X_val, Y_val = X_val.to(device), Y_val.to(device)\n",
        "X_test, Y_test = X_test.to(device), Y_test.to(device)\n",
        "\n",
        "# The network itself must also be sent to the GPU. Either you write net = RegressNet() and then later net.to(device) or directly net = RegressNet().to(device)\n",
        "# The latter option may have the advantage that the instance net is created directly on the GPU, whereas in variant 1 it must first be sent to the GPU.\n",
        "if device_num>1:\n",
        "    print(\"Let's use\", device_num, \"GPU's\")\n",
        "    net = nn.DataParallel(net)\n",
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEB7solvpowR"
      },
      "source": [
        "#%% Specify hyperparameter\n",
        "# hyperparemter: num_epoch, num_lr, loss_func, optimizer\n",
        "# how many epochs do we want to train?\n",
        "# TODO** \n",
        "num_epoch = \n",
        "learn_rate = \n",
        "# TODO**\n",
        "# Loss and optimizer\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learn_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGpL1sQ_powV"
      },
      "source": [
        "#%% Loss before training\n",
        "# Compute loss of test data before training the network (with random weights)\n",
        "Y_pred_train_before = net(X_train)\n",
        "loss_train_before = loss_func(Y_pred_train_before, Y_train)\n",
        "Y_pred_val_before = net(X_val)\n",
        "loss_val_before = loss_func(Y_pred_val_before, Y_val)\n",
        "Y_pred_test_before = net(X_test)\n",
        "loss_test_before = loss_func(Y_pred_test_before, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6Uo7R0xpowZ"
      },
      "source": [
        "#%% Training\n",
        "plt.figure() # monitor loss curve during training\n",
        "# for loop over epochs\n",
        "for epoch in range(num_epoch):\n",
        "    # classical forward pass -> predict new output from train data\n",
        "    Y_pred_train = net(X_train)\n",
        "    # compute loss    \n",
        "    loss_train = loss_func(Y_pred_train, Y_train)\n",
        "    \n",
        "    # Compute gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Calling .backward() mutiple times accumulates the gradient (by addition) for each parameter. This is why you should call optimizer.zero_grad() after each .step() call\n",
        "    # Note that following the first .backward call, a second call is only possible after you have performed another forward pass.\n",
        "    loss_train.backward()\n",
        "    # perform a parameter update based on the current gradient (stored in .grad attribute of a parameter)\n",
        "    optimizer.step()\n",
        "    \n",
        "    # TODO**\n",
        "    # forward pass for validation\n",
        "    Y_pred_val = \n",
        "    loss_val = \n",
        "    # TODO**\n",
        "    \n",
        "    # plot train and val loss\n",
        "    plt.scatter(epoch, loss_train.data.item(), color='b', s=10, marker='o')    \n",
        "    plt.scatter(epoch, loss_val.data.item(), color='r', s=10, marker='o')\n",
        "    \n",
        "    # print message with actual losses\n",
        "    print('Train Epoch: {}/{} ({:.0f}%)\\ttrain_Loss: {:.6f}\\tval_Loss: {:.6f}'.format(\n",
        "    epoch+1, num_epoch, epoch/num_epoch*100, loss_train.item(), loss_val.item()))\n",
        "\n",
        "\n",
        "# show training and validation loss    \n",
        "plt.legend(['train-loss','val-loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.savefig('/content/drive/My Drive/bda_lab/ex2/results/who_loss.png')\n",
        "#plt.show()\n",
        "\n",
        "print('Train loss before training was:', loss_train_before.item())\n",
        "print('Train loss after training is:', loss_train.item())\n",
        "print('Val loss before training was:', loss_val_before.item())\n",
        "print('Val loss after training is:', loss_val.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytU3EgRpowe"
      },
      "source": [
        "#%% Pred vs. Ref Figure Train/Val set\n",
        "# plot the prediction against the reference for the train/val points\n",
        "# if the prediction equals the reference the dots will appear at the 'perfect model' line\n",
        "plt.figure()\n",
        "plt.title('pred vs. ref: train/val points')\n",
        "plt.scatter(Y_train.cpu().numpy(), Y_pred_train.cpu().detach().numpy(), color='b', s=5, marker='o')\n",
        "plt.scatter(Y_val.cpu().numpy(), Y_pred_val.cpu().detach().numpy(), color='r', s=5, marker='o')\n",
        "plt.scatter(Y_val.cpu().numpy(), Y_pred_val_before.cpu().detach().numpy(), color='m', s=5, marker='^')\n",
        "plt.plot((0,1),(0,1), color='k')\n",
        "plt.xlabel('reference')\n",
        "plt.ylabel('prediction')\n",
        "plt.legend(['perfect model', 'train-sample after tr','val-sample after tr', 'val-sample before tr'])\n",
        "plt.xlim((0,1))\n",
        "plt.ylim((0,1))\n",
        "plt.savefig('/content/drive/My Drive/bda_lab/ex2/results/who_pred_vs_ref_val.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKwwQdL7powg"
      },
      "source": [
        "#%% Test results\n",
        "# TODO**\n",
        "# forward pass \n",
        "# Y_pred_test_oh is on the GPU, because net and X_test are on the GPU, but we want it on the CPU from now on.\n",
        "Y_pred_test = \n",
        "loss_test = \n",
        "# TODO**\n",
        "print('Test loss before training was:', loss_test_before.item())\n",
        "print('Test loss after training is:', loss_test.item())\n",
        "\n",
        "# Plot mean abs difference between prediction and reference\n",
        "print('Mean abs difference:', np.mean(abs(Y_pred_test.cpu().detach().numpy()-Y_test.cpu().numpy()), axis=0)*scale_y, 'years')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoADQD3ipowk"
      },
      "source": [
        "#%% Pred vs. Ref Figure Test set\n",
        "# plot the prediction against the reference for the test points\n",
        "# if the prediction equals the reference the dots will appear at the 'perfect model' line\n",
        "plt.figure()\n",
        "plt.title('pred vs. ref: test points')\n",
        "plt.scatter(Y_test.cpu().numpy(), Y_pred_test.cpu().detach().numpy(), color='g', s=5, marker='o')\n",
        "plt.scatter(Y_test.cpu().numpy(), Y_pred_test_before.cpu().detach().numpy(), color='m', s=5, marker='^')\n",
        "plt.plot((0,1),(0,1), color='k')\n",
        "plt.xlabel('reference')\n",
        "plt.ylabel('prediction')\n",
        "plt.legend(['perfect model','test-sample after tr', 'test-sample before tr'])\n",
        "plt.xlim((0,1))\n",
        "plt.ylim((0,1))\n",
        "plt.savefig('/content/drive/My Drive/bda_lab/ex2/results/who_pred_vs_ref_test.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}