{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FRbft71zpovU"
   },
   "outputs": [],
   "source": [
    "# Exercise_2\n",
    "# main simple regression WHO data\n",
    "# TODO**\n",
    "# Names of group members: Fabian Bloch and Christopher Mahn\n",
    "# Date: April 24th, 2023\n",
    "# TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V55Tqr8_pove"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path of this exercise: /hdd/repository/hcu-ma-gmt-big-data-analysis/data/ex2\n"
     ]
    }
   ],
   "source": [
    "#%% Load modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import csv\n",
    "\n",
    "# torch modules\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# plot module\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# module for interoperable file-operations\n",
    "import os\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "print(f'Path of this exercise: {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ApFO2bmNpovl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: NVIDIA GeForce RTX 3080 Ti\n",
      "Device: cuda -- Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# %% CUDA for PyTorch\n",
    "# Right at the beginning: check if a cuda compatible GPU is available in your computer. \n",
    "# If so, set device = cuda:0 which means that later all calculations will be performed on the graphics card. \n",
    "# If no GPU is available, the calculations will run on the CPU, which is also absolutely sufficient for the examples in these exercises.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# cudnn.benchmark = True\n",
    "if(use_cuda):\n",
    "    print(f'Found: {torch.cuda.get_device_name()}')\n",
    "if device.type == 'cpu':\n",
    "    device_num = 0\n",
    "    print('No GPU available.')\n",
    "else:\n",
    "    device_num = torch.cuda.device_count()\n",
    "    print('Device:', device, '-- Number of devices:', device_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tEq8B2lJqeZ_"
   },
   "outputs": [],
   "source": [
    "# Mounting Google Drive locally \n",
    "# from google.colab import drive\n",
    "#drive.mount(\"/content/drive\", force_remount=True)\n",
    "# drive.mount('/content/drive')\n",
    "# you can also choose one of the other options to load data\n",
    "# therefore see https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qDuUbHGwpovs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the dataset: (1649, 22)\n"
     ]
    }
   ],
   "source": [
    "# %% read data\n",
    "# path to WHO data\n",
    "data_path = os.path.join(path, 'data/life-expectancy-who/Life Expectancy Data.csv')\n",
    "\n",
    "# read csv sheet with pandas\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# drop each row where is nan data\n",
    "df = df.dropna()\n",
    "# drop each row where is not at least 21 not nan data\n",
    "# df = df.dropna(thresh=21) # there are many nan in row 4 12 14, which can cause errors\n",
    "\n",
    "# get numpy out of pandas dataframe\n",
    "data = df.values\n",
    "# data=df.to_numpy()\n",
    "\n",
    "# get column names to see, which columns we have to extract as x and y\n",
    "column_names = np.array(df.columns[:], dtype=np.str_)\n",
    "\n",
    "# TODO**\n",
    "print(f'Dimension of the dataset: {np.shape(data)}')\n",
    "# TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QxMKROTGpovx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (1649,)\n",
      "y shape: (1649,)\n",
      "x_1 = 19.1 = 19.1\n",
      "y_1 = 65.0 = 65.0\n"
     ]
    }
   ],
   "source": [
    "# %% split in X and Y\n",
    "# extract any feature you want as X \n",
    "# extract target values as Y\n",
    "# TODO**\n",
    "x = np.array(data[:,10], dtype=np.float32)\n",
    "y = np.array(data[:,3], dtype=np.float32)\n",
    "# TODO**\n",
    "print('x shape:', x.shape)\n",
    "print('y shape:', y.shape)\n",
    "print(f'x_1 = {x[0]:.1f} = 19.1')\n",
    "print(f'y_1 = {y[0]:.1f} = 65.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ArJIaoXdpov3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale_x: 77.1\n",
      "Scale_y: 89.0\n"
     ]
    }
   ],
   "source": [
    "# %% normalize X and Y between (0,1). If multiple features in X are selected, each feature is normalized individually\n",
    "scale_x = np.max(x, axis=0)\n",
    "scale_y = np.max(y, axis=0)\n",
    "x = x/scale_x\n",
    "y = y/scale_y\n",
    "print('Scale_x:',scale_x)\n",
    "print('Scale_y:',scale_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "akHb4sLlpov9"
   },
   "outputs": [],
   "source": [
    "# %% convert to torch tensors\n",
    "# if tensors have only one dimension, an artificial dimension is created with unsqueeze (e.g. [10]->[10,1], so 1D->2D)\n",
    "Y = torch.from_numpy(y)\n",
    "Y = Y.float()\n",
    "if len(Y.shape)==1:\n",
    "    Y = Y.unsqueeze(dim=1)\n",
    "\n",
    "X = torch.from_numpy(x)\n",
    "X = X.float()\n",
    "if len(X.shape)==1:\n",
    "    X = X.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "h_jswetZpowA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input of first ten train Sample: tensor([[0.5901],\n",
      "        [0.0856],\n",
      "        [0.3256],\n",
      "        [0.2438],\n",
      "        [0.6757],\n",
      "        [0.2127],\n",
      "        [0.5460],\n",
      "        [0.4125],\n",
      "        [0.9403],\n",
      "        [0.7588]])\n",
      "Target of first ten train Sample: tensor([[0.7652],\n",
      "        [0.8921],\n",
      "        [0.8753],\n",
      "        [0.6663],\n",
      "        [0.8483],\n",
      "        [0.7180],\n",
      "        [0.7573],\n",
      "        [0.8303],\n",
      "        [0.8157],\n",
      "        [0.8180]])\n"
     ]
    }
   ],
   "source": [
    "# %% Split dataset in training, validation and test tensors\n",
    "# TODO**\n",
    "prop_train = 0.5\n",
    "prop_val = 0.25\n",
    "prop_test = 0.25\n",
    "# TODO**\n",
    "\n",
    "sample_num = {'all': X.shape[0], \n",
    "              'train': round(prop_train*X.shape[0]),\n",
    "              'val': round(prop_val*X.shape[0]),\n",
    "              'test': round(prop_test*X.shape[0])}\n",
    "\n",
    "# idx shuffle\n",
    "idx = np.random.choice(sample_num['all'], sample_num['all'], replace=False)\n",
    "# assign idx to each sample\n",
    "sample_idx = {'all': idx[:], \n",
    "              'train': idx[0:sample_num['train']],\n",
    "              'val': idx[sample_num['train']:sample_num['train']+sample_num['val']],\n",
    "              'test': idx[sample_num['train']+sample_num['val']:]}\n",
    "\n",
    "# Create train data\n",
    "X_train = X[sample_idx['train']]\n",
    "Y_train = Y[sample_idx['train']]\n",
    "\n",
    "# Create validation data\n",
    "X_val = X[sample_idx['val']]\n",
    "Y_val = Y[sample_idx['val']]\n",
    "\n",
    "# Create test data\n",
    "X_test = X[sample_idx['test']]\n",
    "Y_test = Y[sample_idx['test']]\n",
    "\n",
    "\n",
    "# %% Show data point\n",
    "print('Input of first ten train Sample:', X_train[0:10])\n",
    "print('Target of first ten train Sample:', Y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MBIUKIz5powE"
   },
   "outputs": [],
   "source": [
    "#%% class of neural network 'RegressNet'\n",
    "# set up layer and architecture of network in constructor __init__\n",
    "# define operations on layer in forward pass method\n",
    "class RegressNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(RegressNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputSize, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, outputSize)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # max pooling over (2, 2) window\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HNkvuIWrpowJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressNet(\n",
      "  (fc1): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#%% Specify network hyperparameter and create instance of RegressNet\n",
    "# TODO**        \n",
    "inputDim = 1\n",
    "outputDim = 1\n",
    "\n",
    "# Create instance of RegressNet\n",
    "net = RegressNet(inputDim, outputDim)\n",
    "# TODO**\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ej8iLTTQpowN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RegressNet(\n",
       "  (fc1): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.version.cuda)\n",
    "\n",
    "#%% Send tensors and networks to GPU (if you have one which supports cuda) for faster computations\n",
    "X_train, Y_train = X_train.to(device), Y_train.to(device)\n",
    "X_val, Y_val = X_val.to(device), Y_val.to(device)\n",
    "X_test, Y_test = X_test.to(device), Y_test.to(device)\n",
    "\n",
    "# The network itself must also be sent to the GPU. Either you write net = RegressNet() and then later net.to(device) or directly net = RegressNet().to(device)\n",
    "# The latter option may have the advantage that the instance net is created directly on the GPU, whereas in variant 1 it must first be sent to the GPU.\n",
    "if device_num>1:\n",
    "    print(\"Let's use\", device_num, \"GPU's\")\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wEB7solvpowR"
   },
   "outputs": [],
   "source": [
    "#%% Specify hyperparameter\n",
    "# hyperparemter: num_epoch, num_lr, loss_func, optimizer\n",
    "# how many epochs do we want to train?\n",
    "# TODO** \n",
    "num_epoch = 10000\n",
    "learn_rate = 1e-3\n",
    "# TODO**\n",
    "# Loss and optimizer\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fGpL1sQ_powV"
   },
   "outputs": [],
   "source": [
    "#%% Loss before training\n",
    "# Compute loss of test data before training the network (with random weights)\n",
    "Y_pred_train_before = net(X_train)\n",
    "loss_train_before = loss_func(Y_pred_train_before, Y_train)\n",
    "Y_pred_val_before = net(X_val)\n",
    "loss_val_before = loss_func(Y_pred_val_before, Y_val)\n",
    "Y_pred_test_before = net(X_test)\n",
    "loss_test_before = loss_func(Y_pred_test_before, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "s6Uo7R0xpowZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/10000 (0%)\ttrain_Loss: 0.886519\tval_Loss: 0.761588\n",
      "Train Epoch: 2/10000 (0%)\ttrain_Loss: 0.764846\tval_Loss: 0.650591\n",
      "Train Epoch: 3/10000 (0%)\ttrain_Loss: 0.654211\tval_Loss: 0.555164\n",
      "Train Epoch: 4/10000 (0%)\ttrain_Loss: 0.558727\tval_Loss: 0.476749\n",
      "Train Epoch: 5/10000 (0%)\ttrain_Loss: 0.480123\tval_Loss: 0.411897\n",
      "Train Epoch: 6/10000 (0%)\ttrain_Loss: 0.415249\tval_Loss: 0.355852\n",
      "Train Epoch: 7/10000 (0%)\ttrain_Loss: 0.359199\tval_Loss: 0.307406\n",
      "Train Epoch: 8/10000 (0%)\ttrain_Loss: 0.310665\tval_Loss: 0.263363\n",
      "Train Epoch: 9/10000 (0%)\ttrain_Loss: 0.266414\tval_Loss: 0.222888\n",
      "Train Epoch: 10/10000 (0%)\ttrain_Loss: 0.225815\tval_Loss: 0.186277\n",
      "Train Epoch: 11/10000 (0%)\ttrain_Loss: 0.189017\tval_Loss: 0.153628\n",
      "Train Epoch: 12/10000 (0%)\ttrain_Loss: 0.156250\tval_Loss: 0.124820\n",
      "Train Epoch: 13/10000 (0%)\ttrain_Loss: 0.127315\tval_Loss: 0.099748\n",
      "Train Epoch: 14/10000 (0%)\ttrain_Loss: 0.102099\tval_Loss: 0.078592\n",
      "Train Epoch: 15/10000 (0%)\ttrain_Loss: 0.080740\tval_Loss: 0.060648\n",
      "Train Epoch: 16/10000 (0%)\ttrain_Loss: 0.062532\tval_Loss: 0.045497\n",
      "Train Epoch: 17/10000 (0%)\ttrain_Loss: 0.047101\tval_Loss: 0.033070\n",
      "Train Epoch: 18/10000 (0%)\ttrain_Loss: 0.034385\tval_Loss: 0.023274\n",
      "Train Epoch: 19/10000 (0%)\ttrain_Loss: 0.024293\tval_Loss: 0.015967\n",
      "Train Epoch: 20/10000 (0%)\ttrain_Loss: 0.016687\tval_Loss: 0.010964\n",
      "Train Epoch: 21/10000 (0%)\ttrain_Loss: 0.011383\tval_Loss: 0.008030\n",
      "Train Epoch: 22/10000 (0%)\ttrain_Loss: 0.008153\tval_Loss: 0.006888\n",
      "Train Epoch: 23/10000 (0%)\ttrain_Loss: 0.006723\tval_Loss: 0.007222\n",
      "Train Epoch: 24/10000 (0%)\ttrain_Loss: 0.006783\tval_Loss: 0.008691\n",
      "Train Epoch: 25/10000 (0%)\ttrain_Loss: 0.007993\tval_Loss: 0.010940\n",
      "Train Epoch: 26/10000 (0%)\ttrain_Loss: 0.010006\tval_Loss: 0.013621\n",
      "Train Epoch: 27/10000 (0%)\ttrain_Loss: 0.012476\tval_Loss: 0.016410\n",
      "Train Epoch: 28/10000 (0%)\ttrain_Loss: 0.015084\tval_Loss: 0.019028\n",
      "Train Epoch: 29/10000 (0%)\ttrain_Loss: 0.017552\tval_Loss: 0.021251\n",
      "Train Epoch: 30/10000 (0%)\ttrain_Loss: 0.019660\tval_Loss: 0.022924\n",
      "Train Epoch: 31/10000 (0%)\ttrain_Loss: 0.021251\tval_Loss: 0.023959\n",
      "Train Epoch: 32/10000 (0%)\ttrain_Loss: 0.022236\tval_Loss: 0.024331\n",
      "Train Epoch: 33/10000 (0%)\ttrain_Loss: 0.022590\tval_Loss: 0.024071\n",
      "Train Epoch: 34/10000 (0%)\ttrain_Loss: 0.022342\tval_Loss: 0.023254\n",
      "Train Epoch: 35/10000 (0%)\ttrain_Loss: 0.021562\tval_Loss: 0.021985\n",
      "Train Epoch: 36/10000 (0%)\ttrain_Loss: 0.020353\tval_Loss: 0.020383\n",
      "Train Epoch: 37/10000 (0%)\ttrain_Loss: 0.018830\tval_Loss: 0.018574\n",
      "Train Epoch: 38/10000 (0%)\ttrain_Loss: 0.017115\tval_Loss: 0.016679\n",
      "Train Epoch: 39/10000 (0%)\ttrain_Loss: 0.015325\tval_Loss: 0.014803\n",
      "Train Epoch: 40/10000 (0%)\ttrain_Loss: 0.013563\tval_Loss: 0.013037\n",
      "Train Epoch: 41/10000 (0%)\ttrain_Loss: 0.011915\tval_Loss: 0.011446\n",
      "Train Epoch: 42/10000 (0%)\ttrain_Loss: 0.010445\tval_Loss: 0.010078\n",
      "Train Epoch: 43/10000 (0%)\ttrain_Loss: 0.009196\tval_Loss: 0.008957\n",
      "Train Epoch: 44/10000 (0%)\ttrain_Loss: 0.008191\tval_Loss: 0.008088\n",
      "Train Epoch: 45/10000 (0%)\ttrain_Loss: 0.007434\tval_Loss: 0.007461\n",
      "Train Epoch: 46/10000 (0%)\ttrain_Loss: 0.006913\tval_Loss: 0.007055\n",
      "Train Epoch: 47/10000 (0%)\ttrain_Loss: 0.006605\tval_Loss: 0.006837\n",
      "Train Epoch: 48/10000 (0%)\ttrain_Loss: 0.006476\tval_Loss: 0.006771\n",
      "Train Epoch: 49/10000 (0%)\ttrain_Loss: 0.006491\tval_Loss: 0.006819\n",
      "Train Epoch: 50/10000 (0%)\ttrain_Loss: 0.006610\tval_Loss: 0.006944\n",
      "Train Epoch: 51/10000 (0%)\ttrain_Loss: 0.006796\tval_Loss: 0.007111\n",
      "Train Epoch: 52/10000 (1%)\ttrain_Loss: 0.007014\tval_Loss: 0.007290\n",
      "Train Epoch: 53/10000 (1%)\ttrain_Loss: 0.007235\tval_Loss: 0.007458\n",
      "Train Epoch: 54/10000 (1%)\ttrain_Loss: 0.007436\tval_Loss: 0.007597\n",
      "Train Epoch: 55/10000 (1%)\ttrain_Loss: 0.007597\tval_Loss: 0.007695\n",
      "Train Epoch: 56/10000 (1%)\ttrain_Loss: 0.007710\tval_Loss: 0.007746\n",
      "Train Epoch: 57/10000 (1%)\ttrain_Loss: 0.007767\tval_Loss: 0.007749\n",
      "Train Epoch: 58/10000 (1%)\ttrain_Loss: 0.007770\tval_Loss: 0.007709\n",
      "Train Epoch: 59/10000 (1%)\ttrain_Loss: 0.007722\tval_Loss: 0.007631\n",
      "Train Epoch: 60/10000 (1%)\ttrain_Loss: 0.007630\tval_Loss: 0.007525\n",
      "Train Epoch: 61/10000 (1%)\ttrain_Loss: 0.007504\tval_Loss: 0.007401\n",
      "Train Epoch: 62/10000 (1%)\ttrain_Loss: 0.007356\tval_Loss: 0.007269\n",
      "Train Epoch: 63/10000 (1%)\ttrain_Loss: 0.007195\tval_Loss: 0.007138\n",
      "Train Epoch: 64/10000 (1%)\ttrain_Loss: 0.007033\tval_Loss: 0.007017\n",
      "Train Epoch: 65/10000 (1%)\ttrain_Loss: 0.006878\tval_Loss: 0.006913\n",
      "Train Epoch: 66/10000 (1%)\ttrain_Loss: 0.006739\tval_Loss: 0.006830\n",
      "Train Epoch: 67/10000 (1%)\ttrain_Loss: 0.006619\tval_Loss: 0.006770\n",
      "Train Epoch: 68/10000 (1%)\ttrain_Loss: 0.006523\tval_Loss: 0.006735\n",
      "Train Epoch: 69/10000 (1%)\ttrain_Loss: 0.006452\tval_Loss: 0.006722\n",
      "Train Epoch: 70/10000 (1%)\ttrain_Loss: 0.006404\tval_Loss: 0.006729\n",
      "Train Epoch: 71/10000 (1%)\ttrain_Loss: 0.006378\tval_Loss: 0.006752\n",
      "Train Epoch: 72/10000 (1%)\ttrain_Loss: 0.006370\tval_Loss: 0.006785\n",
      "Train Epoch: 73/10000 (1%)\ttrain_Loss: 0.006376\tval_Loss: 0.006826\n",
      "Train Epoch: 74/10000 (1%)\ttrain_Loss: 0.006391\tval_Loss: 0.006867\n",
      "Train Epoch: 75/10000 (1%)\ttrain_Loss: 0.006412\tval_Loss: 0.006907\n",
      "Train Epoch: 76/10000 (1%)\ttrain_Loss: 0.006433\tval_Loss: 0.006940\n",
      "Train Epoch: 77/10000 (1%)\ttrain_Loss: 0.006452\tval_Loss: 0.006965\n",
      "Train Epoch: 78/10000 (1%)\ttrain_Loss: 0.006466\tval_Loss: 0.006981\n",
      "Train Epoch: 79/10000 (1%)\ttrain_Loss: 0.006475\tval_Loss: 0.006986\n",
      "Train Epoch: 80/10000 (1%)\ttrain_Loss: 0.006476\tval_Loss: 0.006981\n",
      "Train Epoch: 81/10000 (1%)\ttrain_Loss: 0.006470\tval_Loss: 0.006968\n",
      "Train Epoch: 82/10000 (1%)\ttrain_Loss: 0.006458\tval_Loss: 0.006947\n",
      "Train Epoch: 83/10000 (1%)\ttrain_Loss: 0.006442\tval_Loss: 0.006920\n",
      "Train Epoch: 84/10000 (1%)\ttrain_Loss: 0.006422\tval_Loss: 0.006891\n",
      "Train Epoch: 85/10000 (1%)\ttrain_Loss: 0.006400\tval_Loss: 0.006859\n",
      "Train Epoch: 86/10000 (1%)\ttrain_Loss: 0.006377\tval_Loss: 0.006828\n",
      "Train Epoch: 87/10000 (1%)\ttrain_Loss: 0.006356\tval_Loss: 0.006798\n",
      "Train Epoch: 88/10000 (1%)\ttrain_Loss: 0.006337\tval_Loss: 0.006771\n",
      "Train Epoch: 89/10000 (1%)\ttrain_Loss: 0.006321\tval_Loss: 0.006746\n",
      "Train Epoch: 90/10000 (1%)\ttrain_Loss: 0.006307\tval_Loss: 0.006726\n",
      "Train Epoch: 91/10000 (1%)\ttrain_Loss: 0.006297\tval_Loss: 0.006709\n",
      "Train Epoch: 92/10000 (1%)\ttrain_Loss: 0.006290\tval_Loss: 0.006695\n",
      "Train Epoch: 93/10000 (1%)\ttrain_Loss: 0.006285\tval_Loss: 0.006684\n",
      "Train Epoch: 94/10000 (1%)\ttrain_Loss: 0.006281\tval_Loss: 0.006675\n",
      "Train Epoch: 95/10000 (1%)\ttrain_Loss: 0.006280\tval_Loss: 0.006669\n",
      "Train Epoch: 96/10000 (1%)\ttrain_Loss: 0.006278\tval_Loss: 0.006664\n",
      "Train Epoch: 97/10000 (1%)\ttrain_Loss: 0.006277\tval_Loss: 0.006660\n",
      "Train Epoch: 98/10000 (1%)\ttrain_Loss: 0.006276\tval_Loss: 0.006656\n",
      "Train Epoch: 99/10000 (1%)\ttrain_Loss: 0.006273\tval_Loss: 0.006653\n",
      "Train Epoch: 100/10000 (1%)\ttrain_Loss: 0.006270\tval_Loss: 0.006651\n",
      "Train Epoch: 101/10000 (1%)\ttrain_Loss: 0.006266\tval_Loss: 0.006649\n",
      "Train Epoch: 102/10000 (1%)\ttrain_Loss: 0.006261\tval_Loss: 0.006647\n",
      "Train Epoch: 103/10000 (1%)\ttrain_Loss: 0.006256\tval_Loss: 0.006645\n",
      "Train Epoch: 104/10000 (1%)\ttrain_Loss: 0.006249\tval_Loss: 0.006643\n",
      "Train Epoch: 105/10000 (1%)\ttrain_Loss: 0.006243\tval_Loss: 0.006643\n",
      "Train Epoch: 106/10000 (1%)\ttrain_Loss: 0.006236\tval_Loss: 0.006642\n",
      "Train Epoch: 107/10000 (1%)\ttrain_Loss: 0.006229\tval_Loss: 0.006643\n",
      "Train Epoch: 108/10000 (1%)\ttrain_Loss: 0.006223\tval_Loss: 0.006644\n",
      "Train Epoch: 109/10000 (1%)\ttrain_Loss: 0.006217\tval_Loss: 0.006645\n",
      "Train Epoch: 110/10000 (1%)\ttrain_Loss: 0.006211\tval_Loss: 0.006647\n",
      "Train Epoch: 111/10000 (1%)\ttrain_Loss: 0.006206\tval_Loss: 0.006649\n",
      "Train Epoch: 112/10000 (1%)\ttrain_Loss: 0.006201\tval_Loss: 0.006651\n",
      "Train Epoch: 113/10000 (1%)\ttrain_Loss: 0.006197\tval_Loss: 0.006653\n",
      "Train Epoch: 114/10000 (1%)\ttrain_Loss: 0.006193\tval_Loss: 0.006654\n",
      "Train Epoch: 115/10000 (1%)\ttrain_Loss: 0.006189\tval_Loss: 0.006655\n",
      "Train Epoch: 116/10000 (1%)\ttrain_Loss: 0.006185\tval_Loss: 0.006656\n",
      "Train Epoch: 117/10000 (1%)\ttrain_Loss: 0.006181\tval_Loss: 0.006656\n",
      "Train Epoch: 118/10000 (1%)\ttrain_Loss: 0.006177\tval_Loss: 0.006656\n",
      "Train Epoch: 119/10000 (1%)\ttrain_Loss: 0.006173\tval_Loss: 0.006654\n",
      "Train Epoch: 120/10000 (1%)\ttrain_Loss: 0.006169\tval_Loss: 0.006653\n",
      "Train Epoch: 121/10000 (1%)\ttrain_Loss: 0.006165\tval_Loss: 0.006650\n",
      "Train Epoch: 122/10000 (1%)\ttrain_Loss: 0.006160\tval_Loss: 0.006648\n",
      "Train Epoch: 123/10000 (1%)\ttrain_Loss: 0.006156\tval_Loss: 0.006644\n",
      "Train Epoch: 124/10000 (1%)\ttrain_Loss: 0.006152\tval_Loss: 0.006641\n",
      "Train Epoch: 125/10000 (1%)\ttrain_Loss: 0.006147\tval_Loss: 0.006638\n",
      "Train Epoch: 126/10000 (1%)\ttrain_Loss: 0.006143\tval_Loss: 0.006634\n",
      "Train Epoch: 127/10000 (1%)\ttrain_Loss: 0.006138\tval_Loss: 0.006630\n",
      "Train Epoch: 128/10000 (1%)\ttrain_Loss: 0.006134\tval_Loss: 0.006627\n",
      "Train Epoch: 129/10000 (1%)\ttrain_Loss: 0.006130\tval_Loss: 0.006623\n",
      "Train Epoch: 130/10000 (1%)\ttrain_Loss: 0.006126\tval_Loss: 0.006620\n",
      "Train Epoch: 131/10000 (1%)\ttrain_Loss: 0.006123\tval_Loss: 0.006616\n",
      "Train Epoch: 132/10000 (1%)\ttrain_Loss: 0.006119\tval_Loss: 0.006613\n",
      "Train Epoch: 133/10000 (1%)\ttrain_Loss: 0.006115\tval_Loss: 0.006610\n",
      "Train Epoch: 134/10000 (1%)\ttrain_Loss: 0.006112\tval_Loss: 0.006608\n",
      "Train Epoch: 135/10000 (1%)\ttrain_Loss: 0.006109\tval_Loss: 0.006605\n",
      "Train Epoch: 136/10000 (1%)\ttrain_Loss: 0.006105\tval_Loss: 0.006603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 137/10000 (1%)\ttrain_Loss: 0.006102\tval_Loss: 0.006601\n",
      "Train Epoch: 138/10000 (1%)\ttrain_Loss: 0.006099\tval_Loss: 0.006599\n",
      "Train Epoch: 139/10000 (1%)\ttrain_Loss: 0.006096\tval_Loss: 0.006597\n",
      "Train Epoch: 140/10000 (1%)\ttrain_Loss: 0.006093\tval_Loss: 0.006595\n",
      "Train Epoch: 141/10000 (1%)\ttrain_Loss: 0.006090\tval_Loss: 0.006594\n",
      "Train Epoch: 142/10000 (1%)\ttrain_Loss: 0.006086\tval_Loss: 0.006592\n",
      "Train Epoch: 143/10000 (1%)\ttrain_Loss: 0.006083\tval_Loss: 0.006591\n",
      "Train Epoch: 144/10000 (1%)\ttrain_Loss: 0.006080\tval_Loss: 0.006590\n",
      "Train Epoch: 145/10000 (1%)\ttrain_Loss: 0.006077\tval_Loss: 0.006589\n",
      "Train Epoch: 146/10000 (1%)\ttrain_Loss: 0.006074\tval_Loss: 0.006588\n",
      "Train Epoch: 147/10000 (1%)\ttrain_Loss: 0.006071\tval_Loss: 0.006587\n",
      "Train Epoch: 148/10000 (1%)\ttrain_Loss: 0.006068\tval_Loss: 0.006586\n",
      "Train Epoch: 149/10000 (1%)\ttrain_Loss: 0.006065\tval_Loss: 0.006584\n",
      "Train Epoch: 150/10000 (1%)\ttrain_Loss: 0.006063\tval_Loss: 0.006583\n",
      "Train Epoch: 151/10000 (2%)\ttrain_Loss: 0.006060\tval_Loss: 0.006582\n",
      "Train Epoch: 152/10000 (2%)\ttrain_Loss: 0.006057\tval_Loss: 0.006580\n",
      "Train Epoch: 153/10000 (2%)\ttrain_Loss: 0.006054\tval_Loss: 0.006579\n",
      "Train Epoch: 154/10000 (2%)\ttrain_Loss: 0.006051\tval_Loss: 0.006577\n",
      "Train Epoch: 155/10000 (2%)\ttrain_Loss: 0.006048\tval_Loss: 0.006576\n",
      "Train Epoch: 156/10000 (2%)\ttrain_Loss: 0.006045\tval_Loss: 0.006574\n",
      "Train Epoch: 157/10000 (2%)\ttrain_Loss: 0.006042\tval_Loss: 0.006572\n",
      "Train Epoch: 158/10000 (2%)\ttrain_Loss: 0.006039\tval_Loss: 0.006570\n",
      "Train Epoch: 159/10000 (2%)\ttrain_Loss: 0.006036\tval_Loss: 0.006568\n",
      "Train Epoch: 160/10000 (2%)\ttrain_Loss: 0.006033\tval_Loss: 0.006566\n",
      "Train Epoch: 161/10000 (2%)\ttrain_Loss: 0.006030\tval_Loss: 0.006564\n",
      "Train Epoch: 162/10000 (2%)\ttrain_Loss: 0.006027\tval_Loss: 0.006562\n",
      "Train Epoch: 163/10000 (2%)\ttrain_Loss: 0.006025\tval_Loss: 0.006559\n",
      "Train Epoch: 164/10000 (2%)\ttrain_Loss: 0.006022\tval_Loss: 0.006557\n",
      "Train Epoch: 165/10000 (2%)\ttrain_Loss: 0.006019\tval_Loss: 0.006555\n",
      "Train Epoch: 166/10000 (2%)\ttrain_Loss: 0.006016\tval_Loss: 0.006553\n",
      "Train Epoch: 167/10000 (2%)\ttrain_Loss: 0.006013\tval_Loss: 0.006551\n",
      "Train Epoch: 168/10000 (2%)\ttrain_Loss: 0.006010\tval_Loss: 0.006549\n",
      "Train Epoch: 169/10000 (2%)\ttrain_Loss: 0.006007\tval_Loss: 0.006547\n",
      "Train Epoch: 170/10000 (2%)\ttrain_Loss: 0.006004\tval_Loss: 0.006545\n",
      "Train Epoch: 171/10000 (2%)\ttrain_Loss: 0.006001\tval_Loss: 0.006543\n",
      "Train Epoch: 172/10000 (2%)\ttrain_Loss: 0.005999\tval_Loss: 0.006541\n",
      "Train Epoch: 173/10000 (2%)\ttrain_Loss: 0.005996\tval_Loss: 0.006539\n",
      "Train Epoch: 174/10000 (2%)\ttrain_Loss: 0.005993\tval_Loss: 0.006537\n",
      "Train Epoch: 175/10000 (2%)\ttrain_Loss: 0.005990\tval_Loss: 0.006535\n",
      "Train Epoch: 176/10000 (2%)\ttrain_Loss: 0.005987\tval_Loss: 0.006533\n",
      "Train Epoch: 177/10000 (2%)\ttrain_Loss: 0.005984\tval_Loss: 0.006531\n",
      "Train Epoch: 178/10000 (2%)\ttrain_Loss: 0.005981\tval_Loss: 0.006530\n",
      "Train Epoch: 179/10000 (2%)\ttrain_Loss: 0.005978\tval_Loss: 0.006528\n",
      "Train Epoch: 180/10000 (2%)\ttrain_Loss: 0.005976\tval_Loss: 0.006526\n",
      "Train Epoch: 181/10000 (2%)\ttrain_Loss: 0.005973\tval_Loss: 0.006524\n",
      "Train Epoch: 182/10000 (2%)\ttrain_Loss: 0.005970\tval_Loss: 0.006523\n",
      "Train Epoch: 183/10000 (2%)\ttrain_Loss: 0.005967\tval_Loss: 0.006521\n",
      "Train Epoch: 184/10000 (2%)\ttrain_Loss: 0.005964\tval_Loss: 0.006519\n",
      "Train Epoch: 185/10000 (2%)\ttrain_Loss: 0.005961\tval_Loss: 0.006517\n",
      "Train Epoch: 186/10000 (2%)\ttrain_Loss: 0.005959\tval_Loss: 0.006516\n",
      "Train Epoch: 187/10000 (2%)\ttrain_Loss: 0.005956\tval_Loss: 0.006514\n",
      "Train Epoch: 188/10000 (2%)\ttrain_Loss: 0.005953\tval_Loss: 0.006512\n",
      "Train Epoch: 189/10000 (2%)\ttrain_Loss: 0.005950\tval_Loss: 0.006510\n",
      "Train Epoch: 190/10000 (2%)\ttrain_Loss: 0.005947\tval_Loss: 0.006509\n",
      "Train Epoch: 191/10000 (2%)\ttrain_Loss: 0.005945\tval_Loss: 0.006507\n",
      "Train Epoch: 192/10000 (2%)\ttrain_Loss: 0.005942\tval_Loss: 0.006505\n",
      "Train Epoch: 193/10000 (2%)\ttrain_Loss: 0.005939\tval_Loss: 0.006503\n",
      "Train Epoch: 194/10000 (2%)\ttrain_Loss: 0.005936\tval_Loss: 0.006501\n",
      "Train Epoch: 195/10000 (2%)\ttrain_Loss: 0.005934\tval_Loss: 0.006499\n",
      "Train Epoch: 196/10000 (2%)\ttrain_Loss: 0.005931\tval_Loss: 0.006498\n",
      "Train Epoch: 197/10000 (2%)\ttrain_Loss: 0.005928\tval_Loss: 0.006496\n",
      "Train Epoch: 198/10000 (2%)\ttrain_Loss: 0.005925\tval_Loss: 0.006494\n",
      "Train Epoch: 199/10000 (2%)\ttrain_Loss: 0.005923\tval_Loss: 0.006492\n",
      "Train Epoch: 200/10000 (2%)\ttrain_Loss: 0.005920\tval_Loss: 0.006490\n",
      "Train Epoch: 201/10000 (2%)\ttrain_Loss: 0.005917\tval_Loss: 0.006489\n",
      "Train Epoch: 202/10000 (2%)\ttrain_Loss: 0.005914\tval_Loss: 0.006487\n",
      "Train Epoch: 203/10000 (2%)\ttrain_Loss: 0.005912\tval_Loss: 0.006485\n",
      "Train Epoch: 204/10000 (2%)\ttrain_Loss: 0.005909\tval_Loss: 0.006483\n",
      "Train Epoch: 205/10000 (2%)\ttrain_Loss: 0.005906\tval_Loss: 0.006482\n",
      "Train Epoch: 206/10000 (2%)\ttrain_Loss: 0.005904\tval_Loss: 0.006480\n",
      "Train Epoch: 207/10000 (2%)\ttrain_Loss: 0.005901\tval_Loss: 0.006478\n",
      "Train Epoch: 208/10000 (2%)\ttrain_Loss: 0.005898\tval_Loss: 0.006477\n",
      "Train Epoch: 209/10000 (2%)\ttrain_Loss: 0.005895\tval_Loss: 0.006475\n",
      "Train Epoch: 210/10000 (2%)\ttrain_Loss: 0.005893\tval_Loss: 0.006473\n",
      "Train Epoch: 211/10000 (2%)\ttrain_Loss: 0.005890\tval_Loss: 0.006472\n",
      "Train Epoch: 212/10000 (2%)\ttrain_Loss: 0.005887\tval_Loss: 0.006470\n",
      "Train Epoch: 213/10000 (2%)\ttrain_Loss: 0.005885\tval_Loss: 0.006468\n",
      "Train Epoch: 214/10000 (2%)\ttrain_Loss: 0.005882\tval_Loss: 0.006466\n",
      "Train Epoch: 215/10000 (2%)\ttrain_Loss: 0.005880\tval_Loss: 0.006465\n",
      "Train Epoch: 216/10000 (2%)\ttrain_Loss: 0.005877\tval_Loss: 0.006463\n",
      "Train Epoch: 217/10000 (2%)\ttrain_Loss: 0.005874\tval_Loss: 0.006461\n",
      "Train Epoch: 218/10000 (2%)\ttrain_Loss: 0.005872\tval_Loss: 0.006460\n",
      "Train Epoch: 219/10000 (2%)\ttrain_Loss: 0.005869\tval_Loss: 0.006458\n",
      "Train Epoch: 220/10000 (2%)\ttrain_Loss: 0.005866\tval_Loss: 0.006457\n",
      "Train Epoch: 221/10000 (2%)\ttrain_Loss: 0.005864\tval_Loss: 0.006455\n",
      "Train Epoch: 222/10000 (2%)\ttrain_Loss: 0.005861\tval_Loss: 0.006453\n",
      "Train Epoch: 223/10000 (2%)\ttrain_Loss: 0.005859\tval_Loss: 0.006452\n",
      "Train Epoch: 224/10000 (2%)\ttrain_Loss: 0.005856\tval_Loss: 0.006450\n",
      "Train Epoch: 225/10000 (2%)\ttrain_Loss: 0.005854\tval_Loss: 0.006449\n",
      "Train Epoch: 226/10000 (2%)\ttrain_Loss: 0.005851\tval_Loss: 0.006447\n",
      "Train Epoch: 227/10000 (2%)\ttrain_Loss: 0.005849\tval_Loss: 0.006445\n",
      "Train Epoch: 228/10000 (2%)\ttrain_Loss: 0.005846\tval_Loss: 0.006444\n",
      "Train Epoch: 229/10000 (2%)\ttrain_Loss: 0.005843\tval_Loss: 0.006442\n",
      "Train Epoch: 230/10000 (2%)\ttrain_Loss: 0.005841\tval_Loss: 0.006441\n",
      "Train Epoch: 231/10000 (2%)\ttrain_Loss: 0.005838\tval_Loss: 0.006439\n",
      "Train Epoch: 232/10000 (2%)\ttrain_Loss: 0.005836\tval_Loss: 0.006438\n",
      "Train Epoch: 233/10000 (2%)\ttrain_Loss: 0.005834\tval_Loss: 0.006436\n",
      "Train Epoch: 234/10000 (2%)\ttrain_Loss: 0.005831\tval_Loss: 0.006435\n",
      "Train Epoch: 235/10000 (2%)\ttrain_Loss: 0.005829\tval_Loss: 0.006433\n",
      "Train Epoch: 236/10000 (2%)\ttrain_Loss: 0.005826\tval_Loss: 0.006432\n",
      "Train Epoch: 237/10000 (2%)\ttrain_Loss: 0.005824\tval_Loss: 0.006430\n",
      "Train Epoch: 238/10000 (2%)\ttrain_Loss: 0.005821\tval_Loss: 0.006429\n",
      "Train Epoch: 239/10000 (2%)\ttrain_Loss: 0.005819\tval_Loss: 0.006427\n",
      "Train Epoch: 240/10000 (2%)\ttrain_Loss: 0.005816\tval_Loss: 0.006426\n",
      "Train Epoch: 241/10000 (2%)\ttrain_Loss: 0.005814\tval_Loss: 0.006424\n",
      "Train Epoch: 242/10000 (2%)\ttrain_Loss: 0.005812\tval_Loss: 0.006423\n",
      "Train Epoch: 243/10000 (2%)\ttrain_Loss: 0.005809\tval_Loss: 0.006421\n",
      "Train Epoch: 244/10000 (2%)\ttrain_Loss: 0.005807\tval_Loss: 0.006420\n",
      "Train Epoch: 245/10000 (2%)\ttrain_Loss: 0.005804\tval_Loss: 0.006418\n",
      "Train Epoch: 246/10000 (2%)\ttrain_Loss: 0.005802\tval_Loss: 0.006417\n",
      "Train Epoch: 247/10000 (2%)\ttrain_Loss: 0.005800\tval_Loss: 0.006416\n",
      "Train Epoch: 248/10000 (2%)\ttrain_Loss: 0.005797\tval_Loss: 0.006414\n",
      "Train Epoch: 249/10000 (2%)\ttrain_Loss: 0.005795\tval_Loss: 0.006413\n",
      "Train Epoch: 250/10000 (2%)\ttrain_Loss: 0.005793\tval_Loss: 0.006411\n",
      "Train Epoch: 251/10000 (2%)\ttrain_Loss: 0.005790\tval_Loss: 0.006410\n",
      "Train Epoch: 252/10000 (3%)\ttrain_Loss: 0.005788\tval_Loss: 0.006409\n",
      "Train Epoch: 253/10000 (3%)\ttrain_Loss: 0.005786\tval_Loss: 0.006407\n",
      "Train Epoch: 254/10000 (3%)\ttrain_Loss: 0.005783\tval_Loss: 0.006406\n",
      "Train Epoch: 255/10000 (3%)\ttrain_Loss: 0.005781\tval_Loss: 0.006405\n",
      "Train Epoch: 256/10000 (3%)\ttrain_Loss: 0.005779\tval_Loss: 0.006403\n",
      "Train Epoch: 257/10000 (3%)\ttrain_Loss: 0.005777\tval_Loss: 0.006402\n",
      "Train Epoch: 258/10000 (3%)\ttrain_Loss: 0.005774\tval_Loss: 0.006401\n",
      "Train Epoch: 259/10000 (3%)\ttrain_Loss: 0.005772\tval_Loss: 0.006399\n",
      "Train Epoch: 260/10000 (3%)\ttrain_Loss: 0.005770\tval_Loss: 0.006398\n",
      "Train Epoch: 261/10000 (3%)\ttrain_Loss: 0.005768\tval_Loss: 0.006397\n",
      "Train Epoch: 262/10000 (3%)\ttrain_Loss: 0.005766\tval_Loss: 0.006396\n",
      "Train Epoch: 263/10000 (3%)\ttrain_Loss: 0.005763\tval_Loss: 0.006394\n",
      "Train Epoch: 264/10000 (3%)\ttrain_Loss: 0.005761\tval_Loss: 0.006393\n",
      "Train Epoch: 265/10000 (3%)\ttrain_Loss: 0.005759\tval_Loss: 0.006392\n",
      "Train Epoch: 266/10000 (3%)\ttrain_Loss: 0.005757\tval_Loss: 0.006391\n",
      "Train Epoch: 267/10000 (3%)\ttrain_Loss: 0.005755\tval_Loss: 0.006389\n",
      "Train Epoch: 268/10000 (3%)\ttrain_Loss: 0.005753\tval_Loss: 0.006388\n",
      "Train Epoch: 269/10000 (3%)\ttrain_Loss: 0.005750\tval_Loss: 0.006387\n",
      "Train Epoch: 270/10000 (3%)\ttrain_Loss: 0.005748\tval_Loss: 0.006386\n",
      "Train Epoch: 271/10000 (3%)\ttrain_Loss: 0.005746\tval_Loss: 0.006384\n",
      "Train Epoch: 272/10000 (3%)\ttrain_Loss: 0.005744\tval_Loss: 0.006383\n",
      "Train Epoch: 273/10000 (3%)\ttrain_Loss: 0.005742\tval_Loss: 0.006382\n",
      "Train Epoch: 274/10000 (3%)\ttrain_Loss: 0.005740\tval_Loss: 0.006381\n",
      "Train Epoch: 275/10000 (3%)\ttrain_Loss: 0.005738\tval_Loss: 0.006380\n",
      "Train Epoch: 276/10000 (3%)\ttrain_Loss: 0.005736\tval_Loss: 0.006378\n",
      "Train Epoch: 277/10000 (3%)\ttrain_Loss: 0.005734\tval_Loss: 0.006377\n",
      "Train Epoch: 278/10000 (3%)\ttrain_Loss: 0.005732\tval_Loss: 0.006376\n",
      "Train Epoch: 279/10000 (3%)\ttrain_Loss: 0.005730\tval_Loss: 0.006375\n",
      "Train Epoch: 280/10000 (3%)\ttrain_Loss: 0.005728\tval_Loss: 0.006374\n",
      "Train Epoch: 281/10000 (3%)\ttrain_Loss: 0.005725\tval_Loss: 0.006373\n",
      "Train Epoch: 282/10000 (3%)\ttrain_Loss: 0.005723\tval_Loss: 0.006372\n",
      "Train Epoch: 283/10000 (3%)\ttrain_Loss: 0.005721\tval_Loss: 0.006370\n",
      "Train Epoch: 284/10000 (3%)\ttrain_Loss: 0.005719\tval_Loss: 0.006369\n",
      "Train Epoch: 285/10000 (3%)\ttrain_Loss: 0.005717\tval_Loss: 0.006368\n",
      "Train Epoch: 286/10000 (3%)\ttrain_Loss: 0.005715\tval_Loss: 0.006367\n",
      "Train Epoch: 287/10000 (3%)\ttrain_Loss: 0.005714\tval_Loss: 0.006366\n",
      "Train Epoch: 288/10000 (3%)\ttrain_Loss: 0.005712\tval_Loss: 0.006365\n",
      "Train Epoch: 289/10000 (3%)\ttrain_Loss: 0.005710\tval_Loss: 0.006364\n",
      "Train Epoch: 290/10000 (3%)\ttrain_Loss: 0.005708\tval_Loss: 0.006363\n",
      "Train Epoch: 291/10000 (3%)\ttrain_Loss: 0.005706\tval_Loss: 0.006361\n",
      "Train Epoch: 292/10000 (3%)\ttrain_Loss: 0.005704\tval_Loss: 0.006360\n",
      "Train Epoch: 293/10000 (3%)\ttrain_Loss: 0.005702\tval_Loss: 0.006359\n",
      "Train Epoch: 294/10000 (3%)\ttrain_Loss: 0.005700\tval_Loss: 0.006358\n",
      "Train Epoch: 295/10000 (3%)\ttrain_Loss: 0.005698\tval_Loss: 0.006357\n",
      "Train Epoch: 296/10000 (3%)\ttrain_Loss: 0.005696\tval_Loss: 0.006356\n",
      "Train Epoch: 297/10000 (3%)\ttrain_Loss: 0.005694\tval_Loss: 0.006355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 298/10000 (3%)\ttrain_Loss: 0.005692\tval_Loss: 0.006354\n",
      "Train Epoch: 299/10000 (3%)\ttrain_Loss: 0.005690\tval_Loss: 0.006353\n",
      "Train Epoch: 300/10000 (3%)\ttrain_Loss: 0.005688\tval_Loss: 0.006352\n",
      "Train Epoch: 301/10000 (3%)\ttrain_Loss: 0.005687\tval_Loss: 0.006351\n",
      "Train Epoch: 302/10000 (3%)\ttrain_Loss: 0.005685\tval_Loss: 0.006350\n",
      "Train Epoch: 303/10000 (3%)\ttrain_Loss: 0.005683\tval_Loss: 0.006349\n",
      "Train Epoch: 304/10000 (3%)\ttrain_Loss: 0.005681\tval_Loss: 0.006348\n",
      "Train Epoch: 305/10000 (3%)\ttrain_Loss: 0.005679\tval_Loss: 0.006347\n",
      "Train Epoch: 306/10000 (3%)\ttrain_Loss: 0.005677\tval_Loss: 0.006347\n",
      "Train Epoch: 307/10000 (3%)\ttrain_Loss: 0.005676\tval_Loss: 0.006346\n",
      "Train Epoch: 308/10000 (3%)\ttrain_Loss: 0.005674\tval_Loss: 0.006345\n",
      "Train Epoch: 309/10000 (3%)\ttrain_Loss: 0.005672\tval_Loss: 0.006344\n",
      "Train Epoch: 310/10000 (3%)\ttrain_Loss: 0.005670\tval_Loss: 0.006343\n",
      "Train Epoch: 311/10000 (3%)\ttrain_Loss: 0.005668\tval_Loss: 0.006342\n",
      "Train Epoch: 312/10000 (3%)\ttrain_Loss: 0.005667\tval_Loss: 0.006342\n",
      "Train Epoch: 313/10000 (3%)\ttrain_Loss: 0.005665\tval_Loss: 0.006341\n",
      "Train Epoch: 314/10000 (3%)\ttrain_Loss: 0.005663\tval_Loss: 0.006340\n",
      "Train Epoch: 315/10000 (3%)\ttrain_Loss: 0.005662\tval_Loss: 0.006340\n",
      "Train Epoch: 316/10000 (3%)\ttrain_Loss: 0.005660\tval_Loss: 0.006339\n",
      "Train Epoch: 317/10000 (3%)\ttrain_Loss: 0.005658\tval_Loss: 0.006338\n",
      "Train Epoch: 318/10000 (3%)\ttrain_Loss: 0.005656\tval_Loss: 0.006338\n",
      "Train Epoch: 319/10000 (3%)\ttrain_Loss: 0.005655\tval_Loss: 0.006337\n",
      "Train Epoch: 320/10000 (3%)\ttrain_Loss: 0.005653\tval_Loss: 0.006337\n",
      "Train Epoch: 321/10000 (3%)\ttrain_Loss: 0.005651\tval_Loss: 0.006336\n",
      "Train Epoch: 322/10000 (3%)\ttrain_Loss: 0.005650\tval_Loss: 0.006335\n",
      "Train Epoch: 323/10000 (3%)\ttrain_Loss: 0.005648\tval_Loss: 0.006334\n",
      "Train Epoch: 324/10000 (3%)\ttrain_Loss: 0.005646\tval_Loss: 0.006334\n",
      "Train Epoch: 325/10000 (3%)\ttrain_Loss: 0.005645\tval_Loss: 0.006333\n",
      "Train Epoch: 326/10000 (3%)\ttrain_Loss: 0.005643\tval_Loss: 0.006332\n",
      "Train Epoch: 327/10000 (3%)\ttrain_Loss: 0.005642\tval_Loss: 0.006331\n",
      "Train Epoch: 328/10000 (3%)\ttrain_Loss: 0.005640\tval_Loss: 0.006331\n",
      "Train Epoch: 329/10000 (3%)\ttrain_Loss: 0.005638\tval_Loss: 0.006330\n",
      "Train Epoch: 330/10000 (3%)\ttrain_Loss: 0.005637\tval_Loss: 0.006329\n",
      "Train Epoch: 331/10000 (3%)\ttrain_Loss: 0.005635\tval_Loss: 0.006329\n",
      "Train Epoch: 332/10000 (3%)\ttrain_Loss: 0.005634\tval_Loss: 0.006328\n",
      "Train Epoch: 333/10000 (3%)\ttrain_Loss: 0.005632\tval_Loss: 0.006328\n",
      "Train Epoch: 334/10000 (3%)\ttrain_Loss: 0.005631\tval_Loss: 0.006327\n",
      "Train Epoch: 335/10000 (3%)\ttrain_Loss: 0.005629\tval_Loss: 0.006326\n",
      "Train Epoch: 336/10000 (3%)\ttrain_Loss: 0.005628\tval_Loss: 0.006326\n",
      "Train Epoch: 337/10000 (3%)\ttrain_Loss: 0.005626\tval_Loss: 0.006325\n",
      "Train Epoch: 338/10000 (3%)\ttrain_Loss: 0.005625\tval_Loss: 0.006325\n",
      "Train Epoch: 339/10000 (3%)\ttrain_Loss: 0.005623\tval_Loss: 0.006324\n",
      "Train Epoch: 340/10000 (3%)\ttrain_Loss: 0.005622\tval_Loss: 0.006324\n",
      "Train Epoch: 341/10000 (3%)\ttrain_Loss: 0.005620\tval_Loss: 0.006323\n",
      "Train Epoch: 342/10000 (3%)\ttrain_Loss: 0.005619\tval_Loss: 0.006322\n",
      "Train Epoch: 343/10000 (3%)\ttrain_Loss: 0.005617\tval_Loss: 0.006322\n",
      "Train Epoch: 344/10000 (3%)\ttrain_Loss: 0.005616\tval_Loss: 0.006321\n",
      "Train Epoch: 345/10000 (3%)\ttrain_Loss: 0.005614\tval_Loss: 0.006321\n",
      "Train Epoch: 346/10000 (3%)\ttrain_Loss: 0.005613\tval_Loss: 0.006320\n",
      "Train Epoch: 347/10000 (3%)\ttrain_Loss: 0.005611\tval_Loss: 0.006320\n",
      "Train Epoch: 348/10000 (3%)\ttrain_Loss: 0.005610\tval_Loss: 0.006319\n",
      "Train Epoch: 349/10000 (3%)\ttrain_Loss: 0.005609\tval_Loss: 0.006318\n",
      "Train Epoch: 350/10000 (3%)\ttrain_Loss: 0.005607\tval_Loss: 0.006318\n",
      "Train Epoch: 351/10000 (4%)\ttrain_Loss: 0.005606\tval_Loss: 0.006317\n",
      "Train Epoch: 352/10000 (4%)\ttrain_Loss: 0.005605\tval_Loss: 0.006317\n",
      "Train Epoch: 353/10000 (4%)\ttrain_Loss: 0.005603\tval_Loss: 0.006316\n",
      "Train Epoch: 354/10000 (4%)\ttrain_Loss: 0.005602\tval_Loss: 0.006316\n",
      "Train Epoch: 355/10000 (4%)\ttrain_Loss: 0.005600\tval_Loss: 0.006315\n",
      "Train Epoch: 356/10000 (4%)\ttrain_Loss: 0.005599\tval_Loss: 0.006315\n",
      "Train Epoch: 357/10000 (4%)\ttrain_Loss: 0.005598\tval_Loss: 0.006314\n",
      "Train Epoch: 358/10000 (4%)\ttrain_Loss: 0.005596\tval_Loss: 0.006314\n",
      "Train Epoch: 359/10000 (4%)\ttrain_Loss: 0.005595\tval_Loss: 0.006313\n",
      "Train Epoch: 360/10000 (4%)\ttrain_Loss: 0.005594\tval_Loss: 0.006313\n",
      "Train Epoch: 361/10000 (4%)\ttrain_Loss: 0.005593\tval_Loss: 0.006312\n",
      "Train Epoch: 362/10000 (4%)\ttrain_Loss: 0.005591\tval_Loss: 0.006311\n",
      "Train Epoch: 363/10000 (4%)\ttrain_Loss: 0.005590\tval_Loss: 0.006311\n",
      "Train Epoch: 364/10000 (4%)\ttrain_Loss: 0.005589\tval_Loss: 0.006310\n",
      "Train Epoch: 365/10000 (4%)\ttrain_Loss: 0.005588\tval_Loss: 0.006310\n",
      "Train Epoch: 366/10000 (4%)\ttrain_Loss: 0.005586\tval_Loss: 0.006309\n",
      "Train Epoch: 367/10000 (4%)\ttrain_Loss: 0.005585\tval_Loss: 0.006309\n",
      "Train Epoch: 368/10000 (4%)\ttrain_Loss: 0.005584\tval_Loss: 0.006308\n",
      "Train Epoch: 369/10000 (4%)\ttrain_Loss: 0.005583\tval_Loss: 0.006308\n",
      "Train Epoch: 370/10000 (4%)\ttrain_Loss: 0.005581\tval_Loss: 0.006307\n",
      "Train Epoch: 371/10000 (4%)\ttrain_Loss: 0.005580\tval_Loss: 0.006307\n",
      "Train Epoch: 372/10000 (4%)\ttrain_Loss: 0.005579\tval_Loss: 0.006306\n",
      "Train Epoch: 373/10000 (4%)\ttrain_Loss: 0.005578\tval_Loss: 0.006305\n",
      "Train Epoch: 374/10000 (4%)\ttrain_Loss: 0.005577\tval_Loss: 0.006305\n",
      "Train Epoch: 375/10000 (4%)\ttrain_Loss: 0.005575\tval_Loss: 0.006304\n",
      "Train Epoch: 376/10000 (4%)\ttrain_Loss: 0.005574\tval_Loss: 0.006304\n",
      "Train Epoch: 377/10000 (4%)\ttrain_Loss: 0.005573\tval_Loss: 0.006303\n",
      "Train Epoch: 378/10000 (4%)\ttrain_Loss: 0.005572\tval_Loss: 0.006303\n",
      "Train Epoch: 379/10000 (4%)\ttrain_Loss: 0.005571\tval_Loss: 0.006302\n",
      "Train Epoch: 380/10000 (4%)\ttrain_Loss: 0.005570\tval_Loss: 0.006302\n",
      "Train Epoch: 381/10000 (4%)\ttrain_Loss: 0.005569\tval_Loss: 0.006301\n",
      "Train Epoch: 382/10000 (4%)\ttrain_Loss: 0.005567\tval_Loss: 0.006300\n",
      "Train Epoch: 383/10000 (4%)\ttrain_Loss: 0.005566\tval_Loss: 0.006300\n",
      "Train Epoch: 384/10000 (4%)\ttrain_Loss: 0.005565\tval_Loss: 0.006299\n",
      "Train Epoch: 385/10000 (4%)\ttrain_Loss: 0.005564\tval_Loss: 0.006299\n",
      "Train Epoch: 386/10000 (4%)\ttrain_Loss: 0.005563\tval_Loss: 0.006298\n",
      "Train Epoch: 387/10000 (4%)\ttrain_Loss: 0.005562\tval_Loss: 0.006298\n",
      "Train Epoch: 388/10000 (4%)\ttrain_Loss: 0.005561\tval_Loss: 0.006297\n",
      "Train Epoch: 389/10000 (4%)\ttrain_Loss: 0.005560\tval_Loss: 0.006297\n",
      "Train Epoch: 390/10000 (4%)\ttrain_Loss: 0.005559\tval_Loss: 0.006296\n",
      "Train Epoch: 391/10000 (4%)\ttrain_Loss: 0.005558\tval_Loss: 0.006296\n",
      "Train Epoch: 392/10000 (4%)\ttrain_Loss: 0.005557\tval_Loss: 0.006296\n",
      "Train Epoch: 393/10000 (4%)\ttrain_Loss: 0.005555\tval_Loss: 0.006295\n",
      "Train Epoch: 394/10000 (4%)\ttrain_Loss: 0.005554\tval_Loss: 0.006295\n",
      "Train Epoch: 395/10000 (4%)\ttrain_Loss: 0.005553\tval_Loss: 0.006294\n",
      "Train Epoch: 396/10000 (4%)\ttrain_Loss: 0.005552\tval_Loss: 0.006294\n",
      "Train Epoch: 397/10000 (4%)\ttrain_Loss: 0.005551\tval_Loss: 0.006294\n",
      "Train Epoch: 398/10000 (4%)\ttrain_Loss: 0.005550\tval_Loss: 0.006293\n",
      "Train Epoch: 399/10000 (4%)\ttrain_Loss: 0.005549\tval_Loss: 0.006293\n",
      "Train Epoch: 400/10000 (4%)\ttrain_Loss: 0.005548\tval_Loss: 0.006292\n",
      "Train Epoch: 401/10000 (4%)\ttrain_Loss: 0.005547\tval_Loss: 0.006292\n",
      "Train Epoch: 402/10000 (4%)\ttrain_Loss: 0.005546\tval_Loss: 0.006291\n",
      "Train Epoch: 403/10000 (4%)\ttrain_Loss: 0.005545\tval_Loss: 0.006291\n",
      "Train Epoch: 404/10000 (4%)\ttrain_Loss: 0.005544\tval_Loss: 0.006291\n",
      "Train Epoch: 405/10000 (4%)\ttrain_Loss: 0.005543\tval_Loss: 0.006290\n",
      "Train Epoch: 406/10000 (4%)\ttrain_Loss: 0.005542\tval_Loss: 0.006290\n",
      "Train Epoch: 407/10000 (4%)\ttrain_Loss: 0.005541\tval_Loss: 0.006290\n",
      "Train Epoch: 408/10000 (4%)\ttrain_Loss: 0.005540\tval_Loss: 0.006289\n",
      "Train Epoch: 409/10000 (4%)\ttrain_Loss: 0.005540\tval_Loss: 0.006289\n",
      "Train Epoch: 410/10000 (4%)\ttrain_Loss: 0.005539\tval_Loss: 0.006289\n",
      "Train Epoch: 411/10000 (4%)\ttrain_Loss: 0.005538\tval_Loss: 0.006289\n",
      "Train Epoch: 412/10000 (4%)\ttrain_Loss: 0.005537\tval_Loss: 0.006288\n",
      "Train Epoch: 413/10000 (4%)\ttrain_Loss: 0.005536\tval_Loss: 0.006288\n",
      "Train Epoch: 414/10000 (4%)\ttrain_Loss: 0.005535\tval_Loss: 0.006288\n",
      "Train Epoch: 415/10000 (4%)\ttrain_Loss: 0.005534\tval_Loss: 0.006288\n",
      "Train Epoch: 416/10000 (4%)\ttrain_Loss: 0.005533\tval_Loss: 0.006287\n",
      "Train Epoch: 417/10000 (4%)\ttrain_Loss: 0.005532\tval_Loss: 0.006287\n",
      "Train Epoch: 418/10000 (4%)\ttrain_Loss: 0.005531\tval_Loss: 0.006287\n",
      "Train Epoch: 419/10000 (4%)\ttrain_Loss: 0.005530\tval_Loss: 0.006287\n",
      "Train Epoch: 420/10000 (4%)\ttrain_Loss: 0.005530\tval_Loss: 0.006286\n",
      "Train Epoch: 421/10000 (4%)\ttrain_Loss: 0.005529\tval_Loss: 0.006286\n",
      "Train Epoch: 422/10000 (4%)\ttrain_Loss: 0.005528\tval_Loss: 0.006286\n",
      "Train Epoch: 423/10000 (4%)\ttrain_Loss: 0.005527\tval_Loss: 0.006285\n",
      "Train Epoch: 424/10000 (4%)\ttrain_Loss: 0.005526\tval_Loss: 0.006285\n",
      "Train Epoch: 425/10000 (4%)\ttrain_Loss: 0.005525\tval_Loss: 0.006285\n",
      "Train Epoch: 426/10000 (4%)\ttrain_Loss: 0.005524\tval_Loss: 0.006284\n",
      "Train Epoch: 427/10000 (4%)\ttrain_Loss: 0.005524\tval_Loss: 0.006284\n",
      "Train Epoch: 428/10000 (4%)\ttrain_Loss: 0.005523\tval_Loss: 0.006284\n",
      "Train Epoch: 429/10000 (4%)\ttrain_Loss: 0.005522\tval_Loss: 0.006283\n",
      "Train Epoch: 430/10000 (4%)\ttrain_Loss: 0.005521\tval_Loss: 0.006283\n",
      "Train Epoch: 431/10000 (4%)\ttrain_Loss: 0.005520\tval_Loss: 0.006282\n",
      "Train Epoch: 432/10000 (4%)\ttrain_Loss: 0.005519\tval_Loss: 0.006282\n",
      "Train Epoch: 433/10000 (4%)\ttrain_Loss: 0.005519\tval_Loss: 0.006282\n",
      "Train Epoch: 434/10000 (4%)\ttrain_Loss: 0.005518\tval_Loss: 0.006281\n",
      "Train Epoch: 435/10000 (4%)\ttrain_Loss: 0.005517\tval_Loss: 0.006281\n",
      "Train Epoch: 436/10000 (4%)\ttrain_Loss: 0.005516\tval_Loss: 0.006281\n",
      "Train Epoch: 437/10000 (4%)\ttrain_Loss: 0.005515\tval_Loss: 0.006280\n",
      "Train Epoch: 438/10000 (4%)\ttrain_Loss: 0.005515\tval_Loss: 0.006280\n",
      "Train Epoch: 439/10000 (4%)\ttrain_Loss: 0.005514\tval_Loss: 0.006280\n",
      "Train Epoch: 440/10000 (4%)\ttrain_Loss: 0.005513\tval_Loss: 0.006279\n",
      "Train Epoch: 441/10000 (4%)\ttrain_Loss: 0.005512\tval_Loss: 0.006279\n",
      "Train Epoch: 442/10000 (4%)\ttrain_Loss: 0.005511\tval_Loss: 0.006278\n",
      "Train Epoch: 443/10000 (4%)\ttrain_Loss: 0.005511\tval_Loss: 0.006278\n",
      "Train Epoch: 444/10000 (4%)\ttrain_Loss: 0.005510\tval_Loss: 0.006278\n",
      "Train Epoch: 445/10000 (4%)\ttrain_Loss: 0.005509\tval_Loss: 0.006277\n",
      "Train Epoch: 446/10000 (4%)\ttrain_Loss: 0.005508\tval_Loss: 0.006277\n",
      "Train Epoch: 447/10000 (4%)\ttrain_Loss: 0.005508\tval_Loss: 0.006277\n",
      "Train Epoch: 448/10000 (4%)\ttrain_Loss: 0.005507\tval_Loss: 0.006276\n",
      "Train Epoch: 449/10000 (4%)\ttrain_Loss: 0.005506\tval_Loss: 0.006276\n",
      "Train Epoch: 450/10000 (4%)\ttrain_Loss: 0.005505\tval_Loss: 0.006276\n",
      "Train Epoch: 451/10000 (4%)\ttrain_Loss: 0.005505\tval_Loss: 0.006276\n",
      "Train Epoch: 452/10000 (5%)\ttrain_Loss: 0.005504\tval_Loss: 0.006275\n",
      "Train Epoch: 453/10000 (5%)\ttrain_Loss: 0.005503\tval_Loss: 0.006275\n",
      "Train Epoch: 454/10000 (5%)\ttrain_Loss: 0.005503\tval_Loss: 0.006275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 455/10000 (5%)\ttrain_Loss: 0.005502\tval_Loss: 0.006275\n",
      "Train Epoch: 456/10000 (5%)\ttrain_Loss: 0.005501\tval_Loss: 0.006275\n",
      "Train Epoch: 457/10000 (5%)\ttrain_Loss: 0.005500\tval_Loss: 0.006274\n",
      "Train Epoch: 458/10000 (5%)\ttrain_Loss: 0.005500\tval_Loss: 0.006274\n",
      "Train Epoch: 459/10000 (5%)\ttrain_Loss: 0.005499\tval_Loss: 0.006274\n",
      "Train Epoch: 460/10000 (5%)\ttrain_Loss: 0.005498\tval_Loss: 0.006274\n",
      "Train Epoch: 461/10000 (5%)\ttrain_Loss: 0.005498\tval_Loss: 0.006273\n",
      "Train Epoch: 462/10000 (5%)\ttrain_Loss: 0.005497\tval_Loss: 0.006273\n",
      "Train Epoch: 463/10000 (5%)\ttrain_Loss: 0.005496\tval_Loss: 0.006273\n",
      "Train Epoch: 464/10000 (5%)\ttrain_Loss: 0.005496\tval_Loss: 0.006273\n",
      "Train Epoch: 465/10000 (5%)\ttrain_Loss: 0.005495\tval_Loss: 0.006272\n",
      "Train Epoch: 466/10000 (5%)\ttrain_Loss: 0.005494\tval_Loss: 0.006272\n",
      "Train Epoch: 467/10000 (5%)\ttrain_Loss: 0.005494\tval_Loss: 0.006272\n",
      "Train Epoch: 468/10000 (5%)\ttrain_Loss: 0.005493\tval_Loss: 0.006271\n",
      "Train Epoch: 469/10000 (5%)\ttrain_Loss: 0.005492\tval_Loss: 0.006271\n",
      "Train Epoch: 470/10000 (5%)\ttrain_Loss: 0.005492\tval_Loss: 0.006271\n",
      "Train Epoch: 471/10000 (5%)\ttrain_Loss: 0.005491\tval_Loss: 0.006270\n",
      "Train Epoch: 472/10000 (5%)\ttrain_Loss: 0.005490\tval_Loss: 0.006270\n",
      "Train Epoch: 473/10000 (5%)\ttrain_Loss: 0.005490\tval_Loss: 0.006270\n",
      "Train Epoch: 474/10000 (5%)\ttrain_Loss: 0.005489\tval_Loss: 0.006270\n",
      "Train Epoch: 475/10000 (5%)\ttrain_Loss: 0.005488\tval_Loss: 0.006269\n",
      "Train Epoch: 476/10000 (5%)\ttrain_Loss: 0.005488\tval_Loss: 0.006269\n",
      "Train Epoch: 477/10000 (5%)\ttrain_Loss: 0.005487\tval_Loss: 0.006269\n",
      "Train Epoch: 478/10000 (5%)\ttrain_Loss: 0.005487\tval_Loss: 0.006268\n",
      "Train Epoch: 479/10000 (5%)\ttrain_Loss: 0.005486\tval_Loss: 0.006268\n",
      "Train Epoch: 480/10000 (5%)\ttrain_Loss: 0.005485\tval_Loss: 0.006268\n",
      "Train Epoch: 481/10000 (5%)\ttrain_Loss: 0.005485\tval_Loss: 0.006267\n",
      "Train Epoch: 482/10000 (5%)\ttrain_Loss: 0.005484\tval_Loss: 0.006267\n",
      "Train Epoch: 483/10000 (5%)\ttrain_Loss: 0.005483\tval_Loss: 0.006266\n",
      "Train Epoch: 484/10000 (5%)\ttrain_Loss: 0.005483\tval_Loss: 0.006266\n",
      "Train Epoch: 485/10000 (5%)\ttrain_Loss: 0.005482\tval_Loss: 0.006266\n",
      "Train Epoch: 486/10000 (5%)\ttrain_Loss: 0.005482\tval_Loss: 0.006265\n",
      "Train Epoch: 487/10000 (5%)\ttrain_Loss: 0.005481\tval_Loss: 0.006265\n",
      "Train Epoch: 488/10000 (5%)\ttrain_Loss: 0.005480\tval_Loss: 0.006265\n",
      "Train Epoch: 489/10000 (5%)\ttrain_Loss: 0.005480\tval_Loss: 0.006264\n",
      "Train Epoch: 490/10000 (5%)\ttrain_Loss: 0.005479\tval_Loss: 0.006264\n",
      "Train Epoch: 491/10000 (5%)\ttrain_Loss: 0.005479\tval_Loss: 0.006263\n",
      "Train Epoch: 492/10000 (5%)\ttrain_Loss: 0.005478\tval_Loss: 0.006263\n",
      "Train Epoch: 493/10000 (5%)\ttrain_Loss: 0.005477\tval_Loss: 0.006263\n",
      "Train Epoch: 494/10000 (5%)\ttrain_Loss: 0.005477\tval_Loss: 0.006262\n",
      "Train Epoch: 495/10000 (5%)\ttrain_Loss: 0.005476\tval_Loss: 0.006262\n",
      "Train Epoch: 496/10000 (5%)\ttrain_Loss: 0.005476\tval_Loss: 0.006262\n",
      "Train Epoch: 497/10000 (5%)\ttrain_Loss: 0.005475\tval_Loss: 0.006261\n",
      "Train Epoch: 498/10000 (5%)\ttrain_Loss: 0.005475\tval_Loss: 0.006261\n",
      "Train Epoch: 499/10000 (5%)\ttrain_Loss: 0.005474\tval_Loss: 0.006261\n",
      "Train Epoch: 500/10000 (5%)\ttrain_Loss: 0.005473\tval_Loss: 0.006261\n",
      "Train Epoch: 501/10000 (5%)\ttrain_Loss: 0.005473\tval_Loss: 0.006260\n",
      "Train Epoch: 502/10000 (5%)\ttrain_Loss: 0.005472\tval_Loss: 0.006260\n",
      "Train Epoch: 503/10000 (5%)\ttrain_Loss: 0.005472\tval_Loss: 0.006260\n",
      "Train Epoch: 504/10000 (5%)\ttrain_Loss: 0.005471\tval_Loss: 0.006260\n",
      "Train Epoch: 505/10000 (5%)\ttrain_Loss: 0.005471\tval_Loss: 0.006259\n",
      "Train Epoch: 506/10000 (5%)\ttrain_Loss: 0.005470\tval_Loss: 0.006259\n",
      "Train Epoch: 507/10000 (5%)\ttrain_Loss: 0.005470\tval_Loss: 0.006259\n",
      "Train Epoch: 508/10000 (5%)\ttrain_Loss: 0.005469\tval_Loss: 0.006259\n",
      "Train Epoch: 509/10000 (5%)\ttrain_Loss: 0.005469\tval_Loss: 0.006259\n",
      "Train Epoch: 510/10000 (5%)\ttrain_Loss: 0.005468\tval_Loss: 0.006259\n",
      "Train Epoch: 511/10000 (5%)\ttrain_Loss: 0.005467\tval_Loss: 0.006258\n",
      "Train Epoch: 512/10000 (5%)\ttrain_Loss: 0.005467\tval_Loss: 0.006258\n",
      "Train Epoch: 513/10000 (5%)\ttrain_Loss: 0.005466\tval_Loss: 0.006258\n",
      "Train Epoch: 514/10000 (5%)\ttrain_Loss: 0.005466\tval_Loss: 0.006257\n",
      "Train Epoch: 515/10000 (5%)\ttrain_Loss: 0.005465\tval_Loss: 0.006257\n",
      "Train Epoch: 516/10000 (5%)\ttrain_Loss: 0.005465\tval_Loss: 0.006257\n",
      "Train Epoch: 517/10000 (5%)\ttrain_Loss: 0.005464\tval_Loss: 0.006256\n",
      "Train Epoch: 518/10000 (5%)\ttrain_Loss: 0.005464\tval_Loss: 0.006256\n",
      "Train Epoch: 519/10000 (5%)\ttrain_Loss: 0.005463\tval_Loss: 0.006256\n",
      "Train Epoch: 520/10000 (5%)\ttrain_Loss: 0.005463\tval_Loss: 0.006256\n",
      "Train Epoch: 521/10000 (5%)\ttrain_Loss: 0.005462\tval_Loss: 0.006256\n",
      "Train Epoch: 522/10000 (5%)\ttrain_Loss: 0.005462\tval_Loss: 0.006255\n",
      "Train Epoch: 523/10000 (5%)\ttrain_Loss: 0.005461\tval_Loss: 0.006255\n",
      "Train Epoch: 524/10000 (5%)\ttrain_Loss: 0.005461\tval_Loss: 0.006255\n",
      "Train Epoch: 525/10000 (5%)\ttrain_Loss: 0.005460\tval_Loss: 0.006254\n",
      "Train Epoch: 526/10000 (5%)\ttrain_Loss: 0.005460\tval_Loss: 0.006254\n",
      "Train Epoch: 527/10000 (5%)\ttrain_Loss: 0.005459\tval_Loss: 0.006254\n",
      "Train Epoch: 528/10000 (5%)\ttrain_Loss: 0.005459\tval_Loss: 0.006254\n",
      "Train Epoch: 529/10000 (5%)\ttrain_Loss: 0.005458\tval_Loss: 0.006253\n",
      "Train Epoch: 530/10000 (5%)\ttrain_Loss: 0.005458\tval_Loss: 0.006253\n",
      "Train Epoch: 531/10000 (5%)\ttrain_Loss: 0.005457\tval_Loss: 0.006253\n",
      "Train Epoch: 532/10000 (5%)\ttrain_Loss: 0.005457\tval_Loss: 0.006252\n",
      "Train Epoch: 533/10000 (5%)\ttrain_Loss: 0.005456\tval_Loss: 0.006252\n",
      "Train Epoch: 534/10000 (5%)\ttrain_Loss: 0.005456\tval_Loss: 0.006252\n",
      "Train Epoch: 535/10000 (5%)\ttrain_Loss: 0.005455\tval_Loss: 0.006251\n",
      "Train Epoch: 536/10000 (5%)\ttrain_Loss: 0.005455\tval_Loss: 0.006251\n",
      "Train Epoch: 537/10000 (5%)\ttrain_Loss: 0.005454\tval_Loss: 0.006251\n",
      "Train Epoch: 538/10000 (5%)\ttrain_Loss: 0.005454\tval_Loss: 0.006250\n",
      "Train Epoch: 539/10000 (5%)\ttrain_Loss: 0.005453\tval_Loss: 0.006250\n",
      "Train Epoch: 540/10000 (5%)\ttrain_Loss: 0.005453\tval_Loss: 0.006250\n",
      "Train Epoch: 541/10000 (5%)\ttrain_Loss: 0.005453\tval_Loss: 0.006249\n",
      "Train Epoch: 542/10000 (5%)\ttrain_Loss: 0.005452\tval_Loss: 0.006249\n",
      "Train Epoch: 543/10000 (5%)\ttrain_Loss: 0.005452\tval_Loss: 0.006249\n",
      "Train Epoch: 544/10000 (5%)\ttrain_Loss: 0.005451\tval_Loss: 0.006248\n",
      "Train Epoch: 545/10000 (5%)\ttrain_Loss: 0.005451\tval_Loss: 0.006248\n",
      "Train Epoch: 546/10000 (5%)\ttrain_Loss: 0.005450\tval_Loss: 0.006248\n",
      "Train Epoch: 547/10000 (5%)\ttrain_Loss: 0.005450\tval_Loss: 0.006247\n",
      "Train Epoch: 548/10000 (5%)\ttrain_Loss: 0.005449\tval_Loss: 0.006247\n",
      "Train Epoch: 549/10000 (5%)\ttrain_Loss: 0.005449\tval_Loss: 0.006246\n",
      "Train Epoch: 550/10000 (5%)\ttrain_Loss: 0.005448\tval_Loss: 0.006246\n",
      "Train Epoch: 551/10000 (6%)\ttrain_Loss: 0.005448\tval_Loss: 0.006246\n",
      "Train Epoch: 552/10000 (6%)\ttrain_Loss: 0.005447\tval_Loss: 0.006245\n",
      "Train Epoch: 553/10000 (6%)\ttrain_Loss: 0.005447\tval_Loss: 0.006245\n",
      "Train Epoch: 554/10000 (6%)\ttrain_Loss: 0.005447\tval_Loss: 0.006245\n",
      "Train Epoch: 555/10000 (6%)\ttrain_Loss: 0.005446\tval_Loss: 0.006244\n",
      "Train Epoch: 556/10000 (6%)\ttrain_Loss: 0.005446\tval_Loss: 0.006244\n",
      "Train Epoch: 557/10000 (6%)\ttrain_Loss: 0.005445\tval_Loss: 0.006244\n",
      "Train Epoch: 558/10000 (6%)\ttrain_Loss: 0.005445\tval_Loss: 0.006243\n",
      "Train Epoch: 559/10000 (6%)\ttrain_Loss: 0.005444\tval_Loss: 0.006243\n",
      "Train Epoch: 560/10000 (6%)\ttrain_Loss: 0.005444\tval_Loss: 0.006243\n",
      "Train Epoch: 561/10000 (6%)\ttrain_Loss: 0.005443\tval_Loss: 0.006242\n",
      "Train Epoch: 562/10000 (6%)\ttrain_Loss: 0.005443\tval_Loss: 0.006242\n",
      "Train Epoch: 563/10000 (6%)\ttrain_Loss: 0.005442\tval_Loss: 0.006242\n",
      "Train Epoch: 564/10000 (6%)\ttrain_Loss: 0.005442\tval_Loss: 0.006241\n",
      "Train Epoch: 565/10000 (6%)\ttrain_Loss: 0.005442\tval_Loss: 0.006241\n",
      "Train Epoch: 566/10000 (6%)\ttrain_Loss: 0.005441\tval_Loss: 0.006241\n",
      "Train Epoch: 567/10000 (6%)\ttrain_Loss: 0.005441\tval_Loss: 0.006240\n",
      "Train Epoch: 568/10000 (6%)\ttrain_Loss: 0.005440\tval_Loss: 0.006240\n",
      "Train Epoch: 569/10000 (6%)\ttrain_Loss: 0.005440\tval_Loss: 0.006239\n",
      "Train Epoch: 570/10000 (6%)\ttrain_Loss: 0.005439\tval_Loss: 0.006239\n",
      "Train Epoch: 571/10000 (6%)\ttrain_Loss: 0.005439\tval_Loss: 0.006239\n",
      "Train Epoch: 572/10000 (6%)\ttrain_Loss: 0.005439\tval_Loss: 0.006238\n",
      "Train Epoch: 573/10000 (6%)\ttrain_Loss: 0.005438\tval_Loss: 0.006238\n",
      "Train Epoch: 574/10000 (6%)\ttrain_Loss: 0.005438\tval_Loss: 0.006237\n",
      "Train Epoch: 575/10000 (6%)\ttrain_Loss: 0.005437\tval_Loss: 0.006237\n",
      "Train Epoch: 576/10000 (6%)\ttrain_Loss: 0.005437\tval_Loss: 0.006237\n",
      "Train Epoch: 577/10000 (6%)\ttrain_Loss: 0.005436\tval_Loss: 0.006236\n",
      "Train Epoch: 578/10000 (6%)\ttrain_Loss: 0.005436\tval_Loss: 0.006236\n",
      "Train Epoch: 579/10000 (6%)\ttrain_Loss: 0.005436\tval_Loss: 0.006236\n",
      "Train Epoch: 580/10000 (6%)\ttrain_Loss: 0.005435\tval_Loss: 0.006235\n",
      "Train Epoch: 581/10000 (6%)\ttrain_Loss: 0.005435\tval_Loss: 0.006235\n",
      "Train Epoch: 582/10000 (6%)\ttrain_Loss: 0.005434\tval_Loss: 0.006234\n",
      "Train Epoch: 583/10000 (6%)\ttrain_Loss: 0.005434\tval_Loss: 0.006234\n",
      "Train Epoch: 584/10000 (6%)\ttrain_Loss: 0.005433\tval_Loss: 0.006234\n",
      "Train Epoch: 585/10000 (6%)\ttrain_Loss: 0.005433\tval_Loss: 0.006234\n",
      "Train Epoch: 586/10000 (6%)\ttrain_Loss: 0.005433\tval_Loss: 0.006233\n",
      "Train Epoch: 587/10000 (6%)\ttrain_Loss: 0.005432\tval_Loss: 0.006233\n",
      "Train Epoch: 588/10000 (6%)\ttrain_Loss: 0.005432\tval_Loss: 0.006233\n",
      "Train Epoch: 589/10000 (6%)\ttrain_Loss: 0.005431\tval_Loss: 0.006232\n",
      "Train Epoch: 590/10000 (6%)\ttrain_Loss: 0.005431\tval_Loss: 0.006232\n",
      "Train Epoch: 591/10000 (6%)\ttrain_Loss: 0.005430\tval_Loss: 0.006232\n",
      "Train Epoch: 592/10000 (6%)\ttrain_Loss: 0.005430\tval_Loss: 0.006231\n",
      "Train Epoch: 593/10000 (6%)\ttrain_Loss: 0.005430\tval_Loss: 0.006231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 594/10000 (6%)\ttrain_Loss: 0.005429\tval_Loss: 0.006231\n",
      "Train Epoch: 595/10000 (6%)\ttrain_Loss: 0.005429\tval_Loss: 0.006230\n",
      "Train Epoch: 596/10000 (6%)\ttrain_Loss: 0.005428\tval_Loss: 0.006230\n",
      "Train Epoch: 597/10000 (6%)\ttrain_Loss: 0.005428\tval_Loss: 0.006230\n",
      "Train Epoch: 598/10000 (6%)\ttrain_Loss: 0.005428\tval_Loss: 0.006229\n",
      "Train Epoch: 599/10000 (6%)\ttrain_Loss: 0.005427\tval_Loss: 0.006229\n",
      "Train Epoch: 600/10000 (6%)\ttrain_Loss: 0.005427\tval_Loss: 0.006228\n",
      "Train Epoch: 601/10000 (6%)\ttrain_Loss: 0.005426\tval_Loss: 0.006228\n",
      "Train Epoch: 602/10000 (6%)\ttrain_Loss: 0.005426\tval_Loss: 0.006228\n",
      "Train Epoch: 603/10000 (6%)\ttrain_Loss: 0.005426\tval_Loss: 0.006227\n",
      "Train Epoch: 604/10000 (6%)\ttrain_Loss: 0.005425\tval_Loss: 0.006227\n",
      "Train Epoch: 605/10000 (6%)\ttrain_Loss: 0.005425\tval_Loss: 0.006226\n",
      "Train Epoch: 606/10000 (6%)\ttrain_Loss: 0.005424\tval_Loss: 0.006226\n",
      "Train Epoch: 607/10000 (6%)\ttrain_Loss: 0.005424\tval_Loss: 0.006226\n",
      "Train Epoch: 608/10000 (6%)\ttrain_Loss: 0.005424\tval_Loss: 0.006225\n",
      "Train Epoch: 609/10000 (6%)\ttrain_Loss: 0.005423\tval_Loss: 0.006225\n",
      "Train Epoch: 610/10000 (6%)\ttrain_Loss: 0.005423\tval_Loss: 0.006225\n",
      "Train Epoch: 611/10000 (6%)\ttrain_Loss: 0.005422\tval_Loss: 0.006224\n",
      "Train Epoch: 612/10000 (6%)\ttrain_Loss: 0.005422\tval_Loss: 0.006224\n",
      "Train Epoch: 613/10000 (6%)\ttrain_Loss: 0.005422\tval_Loss: 0.006223\n",
      "Train Epoch: 614/10000 (6%)\ttrain_Loss: 0.005421\tval_Loss: 0.006223\n",
      "Train Epoch: 615/10000 (6%)\ttrain_Loss: 0.005421\tval_Loss: 0.006223\n",
      "Train Epoch: 616/10000 (6%)\ttrain_Loss: 0.005421\tval_Loss: 0.006222\n",
      "Train Epoch: 617/10000 (6%)\ttrain_Loss: 0.005420\tval_Loss: 0.006222\n",
      "Train Epoch: 618/10000 (6%)\ttrain_Loss: 0.005420\tval_Loss: 0.006221\n",
      "Train Epoch: 619/10000 (6%)\ttrain_Loss: 0.005419\tval_Loss: 0.006221\n",
      "Train Epoch: 620/10000 (6%)\ttrain_Loss: 0.005419\tval_Loss: 0.006221\n",
      "Train Epoch: 621/10000 (6%)\ttrain_Loss: 0.005419\tval_Loss: 0.006220\n",
      "Train Epoch: 622/10000 (6%)\ttrain_Loss: 0.005418\tval_Loss: 0.006220\n",
      "Train Epoch: 623/10000 (6%)\ttrain_Loss: 0.005418\tval_Loss: 0.006219\n",
      "Train Epoch: 624/10000 (6%)\ttrain_Loss: 0.005417\tval_Loss: 0.006219\n",
      "Train Epoch: 625/10000 (6%)\ttrain_Loss: 0.005417\tval_Loss: 0.006219\n",
      "Train Epoch: 626/10000 (6%)\ttrain_Loss: 0.005417\tval_Loss: 0.006218\n",
      "Train Epoch: 627/10000 (6%)\ttrain_Loss: 0.005416\tval_Loss: 0.006218\n",
      "Train Epoch: 628/10000 (6%)\ttrain_Loss: 0.005416\tval_Loss: 0.006217\n",
      "Train Epoch: 629/10000 (6%)\ttrain_Loss: 0.005416\tval_Loss: 0.006217\n",
      "Train Epoch: 630/10000 (6%)\ttrain_Loss: 0.005415\tval_Loss: 0.006217\n",
      "Train Epoch: 631/10000 (6%)\ttrain_Loss: 0.005415\tval_Loss: 0.006216\n",
      "Train Epoch: 632/10000 (6%)\ttrain_Loss: 0.005414\tval_Loss: 0.006216\n",
      "Train Epoch: 633/10000 (6%)\ttrain_Loss: 0.005414\tval_Loss: 0.006215\n",
      "Train Epoch: 634/10000 (6%)\ttrain_Loss: 0.005414\tval_Loss: 0.006215\n",
      "Train Epoch: 635/10000 (6%)\ttrain_Loss: 0.005413\tval_Loss: 0.006215\n",
      "Train Epoch: 636/10000 (6%)\ttrain_Loss: 0.005413\tval_Loss: 0.006214\n",
      "Train Epoch: 637/10000 (6%)\ttrain_Loss: 0.005413\tval_Loss: 0.006214\n",
      "Train Epoch: 638/10000 (6%)\ttrain_Loss: 0.005412\tval_Loss: 0.006213\n",
      "Train Epoch: 639/10000 (6%)\ttrain_Loss: 0.005412\tval_Loss: 0.006213\n",
      "Train Epoch: 640/10000 (6%)\ttrain_Loss: 0.005411\tval_Loss: 0.006213\n",
      "Train Epoch: 641/10000 (6%)\ttrain_Loss: 0.005411\tval_Loss: 0.006212\n",
      "Train Epoch: 642/10000 (6%)\ttrain_Loss: 0.005411\tval_Loss: 0.006212\n",
      "Train Epoch: 643/10000 (6%)\ttrain_Loss: 0.005410\tval_Loss: 0.006212\n",
      "Train Epoch: 644/10000 (6%)\ttrain_Loss: 0.005410\tval_Loss: 0.006211\n",
      "Train Epoch: 645/10000 (6%)\ttrain_Loss: 0.005410\tval_Loss: 0.006211\n",
      "Train Epoch: 646/10000 (6%)\ttrain_Loss: 0.005409\tval_Loss: 0.006210\n",
      "Train Epoch: 647/10000 (6%)\ttrain_Loss: 0.005409\tval_Loss: 0.006210\n",
      "Train Epoch: 648/10000 (6%)\ttrain_Loss: 0.005408\tval_Loss: 0.006210\n",
      "Train Epoch: 649/10000 (6%)\ttrain_Loss: 0.005408\tval_Loss: 0.006209\n",
      "Train Epoch: 650/10000 (6%)\ttrain_Loss: 0.005408\tval_Loss: 0.006209\n",
      "Train Epoch: 651/10000 (6%)\ttrain_Loss: 0.005407\tval_Loss: 0.006209\n",
      "Train Epoch: 652/10000 (7%)\ttrain_Loss: 0.005407\tval_Loss: 0.006208\n",
      "Train Epoch: 653/10000 (7%)\ttrain_Loss: 0.005407\tval_Loss: 0.006208\n",
      "Train Epoch: 654/10000 (7%)\ttrain_Loss: 0.005406\tval_Loss: 0.006208\n",
      "Train Epoch: 655/10000 (7%)\ttrain_Loss: 0.005406\tval_Loss: 0.006207\n",
      "Train Epoch: 656/10000 (7%)\ttrain_Loss: 0.005406\tval_Loss: 0.006207\n",
      "Train Epoch: 657/10000 (7%)\ttrain_Loss: 0.005405\tval_Loss: 0.006207\n",
      "Train Epoch: 658/10000 (7%)\ttrain_Loss: 0.005405\tval_Loss: 0.006206\n",
      "Train Epoch: 659/10000 (7%)\ttrain_Loss: 0.005404\tval_Loss: 0.006206\n",
      "Train Epoch: 660/10000 (7%)\ttrain_Loss: 0.005404\tval_Loss: 0.006205\n",
      "Train Epoch: 661/10000 (7%)\ttrain_Loss: 0.005404\tval_Loss: 0.006205\n",
      "Train Epoch: 662/10000 (7%)\ttrain_Loss: 0.005403\tval_Loss: 0.006205\n",
      "Train Epoch: 663/10000 (7%)\ttrain_Loss: 0.005403\tval_Loss: 0.006204\n",
      "Train Epoch: 664/10000 (7%)\ttrain_Loss: 0.005403\tval_Loss: 0.006204\n",
      "Train Epoch: 665/10000 (7%)\ttrain_Loss: 0.005402\tval_Loss: 0.006204\n",
      "Train Epoch: 666/10000 (7%)\ttrain_Loss: 0.005402\tval_Loss: 0.006203\n",
      "Train Epoch: 667/10000 (7%)\ttrain_Loss: 0.005402\tval_Loss: 0.006203\n",
      "Train Epoch: 668/10000 (7%)\ttrain_Loss: 0.005401\tval_Loss: 0.006203\n",
      "Train Epoch: 669/10000 (7%)\ttrain_Loss: 0.005401\tval_Loss: 0.006203\n",
      "Train Epoch: 670/10000 (7%)\ttrain_Loss: 0.005401\tval_Loss: 0.006202\n",
      "Train Epoch: 671/10000 (7%)\ttrain_Loss: 0.005400\tval_Loss: 0.006202\n",
      "Train Epoch: 672/10000 (7%)\ttrain_Loss: 0.005400\tval_Loss: 0.006202\n",
      "Train Epoch: 673/10000 (7%)\ttrain_Loss: 0.005399\tval_Loss: 0.006201\n",
      "Train Epoch: 674/10000 (7%)\ttrain_Loss: 0.005399\tval_Loss: 0.006201\n",
      "Train Epoch: 675/10000 (7%)\ttrain_Loss: 0.005399\tval_Loss: 0.006200\n",
      "Train Epoch: 676/10000 (7%)\ttrain_Loss: 0.005398\tval_Loss: 0.006200\n",
      "Train Epoch: 677/10000 (7%)\ttrain_Loss: 0.005398\tval_Loss: 0.006200\n",
      "Train Epoch: 678/10000 (7%)\ttrain_Loss: 0.005398\tval_Loss: 0.006199\n",
      "Train Epoch: 679/10000 (7%)\ttrain_Loss: 0.005397\tval_Loss: 0.006199\n",
      "Train Epoch: 680/10000 (7%)\ttrain_Loss: 0.005397\tval_Loss: 0.006199\n",
      "Train Epoch: 681/10000 (7%)\ttrain_Loss: 0.005397\tval_Loss: 0.006198\n",
      "Train Epoch: 682/10000 (7%)\ttrain_Loss: 0.005396\tval_Loss: 0.006198\n",
      "Train Epoch: 683/10000 (7%)\ttrain_Loss: 0.005396\tval_Loss: 0.006197\n",
      "Train Epoch: 684/10000 (7%)\ttrain_Loss: 0.005396\tval_Loss: 0.006197\n",
      "Train Epoch: 685/10000 (7%)\ttrain_Loss: 0.005395\tval_Loss: 0.006197\n",
      "Train Epoch: 686/10000 (7%)\ttrain_Loss: 0.005395\tval_Loss: 0.006196\n",
      "Train Epoch: 687/10000 (7%)\ttrain_Loss: 0.005395\tval_Loss: 0.006196\n",
      "Train Epoch: 688/10000 (7%)\ttrain_Loss: 0.005394\tval_Loss: 0.006196\n",
      "Train Epoch: 689/10000 (7%)\ttrain_Loss: 0.005394\tval_Loss: 0.006195\n",
      "Train Epoch: 690/10000 (7%)\ttrain_Loss: 0.005394\tval_Loss: 0.006195\n",
      "Train Epoch: 691/10000 (7%)\ttrain_Loss: 0.005393\tval_Loss: 0.006194\n",
      "Train Epoch: 692/10000 (7%)\ttrain_Loss: 0.005393\tval_Loss: 0.006194\n",
      "Train Epoch: 693/10000 (7%)\ttrain_Loss: 0.005393\tval_Loss: 0.006194\n",
      "Train Epoch: 694/10000 (7%)\ttrain_Loss: 0.005392\tval_Loss: 0.006193\n",
      "Train Epoch: 695/10000 (7%)\ttrain_Loss: 0.005392\tval_Loss: 0.006193\n",
      "Train Epoch: 696/10000 (7%)\ttrain_Loss: 0.005392\tval_Loss: 0.006193\n",
      "Train Epoch: 697/10000 (7%)\ttrain_Loss: 0.005391\tval_Loss: 0.006192\n",
      "Train Epoch: 698/10000 (7%)\ttrain_Loss: 0.005391\tval_Loss: 0.006192\n",
      "Train Epoch: 699/10000 (7%)\ttrain_Loss: 0.005391\tval_Loss: 0.006191\n",
      "Train Epoch: 700/10000 (7%)\ttrain_Loss: 0.005390\tval_Loss: 0.006191\n",
      "Train Epoch: 701/10000 (7%)\ttrain_Loss: 0.005390\tval_Loss: 0.006190\n",
      "Train Epoch: 702/10000 (7%)\ttrain_Loss: 0.005390\tval_Loss: 0.006190\n",
      "Train Epoch: 703/10000 (7%)\ttrain_Loss: 0.005389\tval_Loss: 0.006189\n",
      "Train Epoch: 704/10000 (7%)\ttrain_Loss: 0.005389\tval_Loss: 0.006189\n",
      "Train Epoch: 705/10000 (7%)\ttrain_Loss: 0.005389\tval_Loss: 0.006189\n",
      "Train Epoch: 706/10000 (7%)\ttrain_Loss: 0.005388\tval_Loss: 0.006188\n",
      "Train Epoch: 707/10000 (7%)\ttrain_Loss: 0.005388\tval_Loss: 0.006188\n",
      "Train Epoch: 708/10000 (7%)\ttrain_Loss: 0.005388\tval_Loss: 0.006188\n",
      "Train Epoch: 709/10000 (7%)\ttrain_Loss: 0.005387\tval_Loss: 0.006187\n",
      "Train Epoch: 710/10000 (7%)\ttrain_Loss: 0.005387\tval_Loss: 0.006187\n",
      "Train Epoch: 711/10000 (7%)\ttrain_Loss: 0.005387\tval_Loss: 0.006186\n",
      "Train Epoch: 712/10000 (7%)\ttrain_Loss: 0.005386\tval_Loss: 0.006186\n",
      "Train Epoch: 713/10000 (7%)\ttrain_Loss: 0.005386\tval_Loss: 0.006186\n",
      "Train Epoch: 714/10000 (7%)\ttrain_Loss: 0.005386\tval_Loss: 0.006186\n",
      "Train Epoch: 715/10000 (7%)\ttrain_Loss: 0.005386\tval_Loss: 0.006185\n",
      "Train Epoch: 716/10000 (7%)\ttrain_Loss: 0.005385\tval_Loss: 0.006185\n",
      "Train Epoch: 717/10000 (7%)\ttrain_Loss: 0.005385\tval_Loss: 0.006185\n",
      "Train Epoch: 718/10000 (7%)\ttrain_Loss: 0.005385\tval_Loss: 0.006184\n",
      "Train Epoch: 719/10000 (7%)\ttrain_Loss: 0.005384\tval_Loss: 0.006184\n",
      "Train Epoch: 720/10000 (7%)\ttrain_Loss: 0.005384\tval_Loss: 0.006184\n",
      "Train Epoch: 721/10000 (7%)\ttrain_Loss: 0.005384\tval_Loss: 0.006183\n",
      "Train Epoch: 722/10000 (7%)\ttrain_Loss: 0.005383\tval_Loss: 0.006183\n",
      "Train Epoch: 723/10000 (7%)\ttrain_Loss: 0.005383\tval_Loss: 0.006182\n",
      "Train Epoch: 724/10000 (7%)\ttrain_Loss: 0.005383\tval_Loss: 0.006182\n",
      "Train Epoch: 725/10000 (7%)\ttrain_Loss: 0.005382\tval_Loss: 0.006182\n",
      "Train Epoch: 726/10000 (7%)\ttrain_Loss: 0.005382\tval_Loss: 0.006181\n",
      "Train Epoch: 727/10000 (7%)\ttrain_Loss: 0.005382\tval_Loss: 0.006181\n",
      "Train Epoch: 728/10000 (7%)\ttrain_Loss: 0.005382\tval_Loss: 0.006180\n",
      "Train Epoch: 729/10000 (7%)\ttrain_Loss: 0.005381\tval_Loss: 0.006180\n",
      "Train Epoch: 730/10000 (7%)\ttrain_Loss: 0.005381\tval_Loss: 0.006180\n",
      "Train Epoch: 731/10000 (7%)\ttrain_Loss: 0.005381\tval_Loss: 0.006179\n",
      "Train Epoch: 732/10000 (7%)\ttrain_Loss: 0.005380\tval_Loss: 0.006179\n",
      "Train Epoch: 733/10000 (7%)\ttrain_Loss: 0.005380\tval_Loss: 0.006178\n",
      "Train Epoch: 734/10000 (7%)\ttrain_Loss: 0.005380\tval_Loss: 0.006178\n",
      "Train Epoch: 735/10000 (7%)\ttrain_Loss: 0.005379\tval_Loss: 0.006178\n",
      "Train Epoch: 736/10000 (7%)\ttrain_Loss: 0.005379\tval_Loss: 0.006177\n",
      "Train Epoch: 737/10000 (7%)\ttrain_Loss: 0.005379\tval_Loss: 0.006177\n",
      "Train Epoch: 738/10000 (7%)\ttrain_Loss: 0.005378\tval_Loss: 0.006176\n",
      "Train Epoch: 739/10000 (7%)\ttrain_Loss: 0.005378\tval_Loss: 0.006176\n",
      "Train Epoch: 740/10000 (7%)\ttrain_Loss: 0.005378\tval_Loss: 0.006176\n",
      "Train Epoch: 741/10000 (7%)\ttrain_Loss: 0.005378\tval_Loss: 0.006175\n",
      "Train Epoch: 742/10000 (7%)\ttrain_Loss: 0.005377\tval_Loss: 0.006175\n",
      "Train Epoch: 743/10000 (7%)\ttrain_Loss: 0.005377\tval_Loss: 0.006175\n",
      "Train Epoch: 744/10000 (7%)\ttrain_Loss: 0.005377\tval_Loss: 0.006174\n",
      "Train Epoch: 745/10000 (7%)\ttrain_Loss: 0.005376\tval_Loss: 0.006174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 746/10000 (7%)\ttrain_Loss: 0.005376\tval_Loss: 0.006173\n",
      "Train Epoch: 747/10000 (7%)\ttrain_Loss: 0.005376\tval_Loss: 0.006173\n",
      "Train Epoch: 748/10000 (7%)\ttrain_Loss: 0.005375\tval_Loss: 0.006173\n",
      "Train Epoch: 749/10000 (7%)\ttrain_Loss: 0.005375\tval_Loss: 0.006172\n",
      "Train Epoch: 750/10000 (7%)\ttrain_Loss: 0.005375\tval_Loss: 0.006172\n",
      "Train Epoch: 751/10000 (8%)\ttrain_Loss: 0.005375\tval_Loss: 0.006171\n",
      "Train Epoch: 752/10000 (8%)\ttrain_Loss: 0.005374\tval_Loss: 0.006171\n",
      "Train Epoch: 753/10000 (8%)\ttrain_Loss: 0.005374\tval_Loss: 0.006171\n",
      "Train Epoch: 754/10000 (8%)\ttrain_Loss: 0.005374\tval_Loss: 0.006170\n",
      "Train Epoch: 755/10000 (8%)\ttrain_Loss: 0.005373\tval_Loss: 0.006170\n",
      "Train Epoch: 756/10000 (8%)\ttrain_Loss: 0.005373\tval_Loss: 0.006169\n",
      "Train Epoch: 757/10000 (8%)\ttrain_Loss: 0.005373\tval_Loss: 0.006169\n",
      "Train Epoch: 758/10000 (8%)\ttrain_Loss: 0.005372\tval_Loss: 0.006169\n",
      "Train Epoch: 759/10000 (8%)\ttrain_Loss: 0.005372\tval_Loss: 0.006168\n",
      "Train Epoch: 760/10000 (8%)\ttrain_Loss: 0.005372\tval_Loss: 0.006168\n",
      "Train Epoch: 761/10000 (8%)\ttrain_Loss: 0.005371\tval_Loss: 0.006168\n",
      "Train Epoch: 762/10000 (8%)\ttrain_Loss: 0.005371\tval_Loss: 0.006167\n",
      "Train Epoch: 763/10000 (8%)\ttrain_Loss: 0.005371\tval_Loss: 0.006167\n",
      "Train Epoch: 764/10000 (8%)\ttrain_Loss: 0.005371\tval_Loss: 0.006166\n",
      "Train Epoch: 765/10000 (8%)\ttrain_Loss: 0.005370\tval_Loss: 0.006166\n",
      "Train Epoch: 766/10000 (8%)\ttrain_Loss: 0.005370\tval_Loss: 0.006166\n",
      "Train Epoch: 767/10000 (8%)\ttrain_Loss: 0.005370\tval_Loss: 0.006165\n",
      "Train Epoch: 768/10000 (8%)\ttrain_Loss: 0.005369\tval_Loss: 0.006165\n",
      "Train Epoch: 769/10000 (8%)\ttrain_Loss: 0.005369\tval_Loss: 0.006164\n",
      "Train Epoch: 770/10000 (8%)\ttrain_Loss: 0.005369\tval_Loss: 0.006164\n",
      "Train Epoch: 771/10000 (8%)\ttrain_Loss: 0.005368\tval_Loss: 0.006164\n",
      "Train Epoch: 772/10000 (8%)\ttrain_Loss: 0.005368\tval_Loss: 0.006163\n",
      "Train Epoch: 773/10000 (8%)\ttrain_Loss: 0.005368\tval_Loss: 0.006163\n",
      "Train Epoch: 774/10000 (8%)\ttrain_Loss: 0.005368\tval_Loss: 0.006162\n",
      "Train Epoch: 775/10000 (8%)\ttrain_Loss: 0.005367\tval_Loss: 0.006162\n",
      "Train Epoch: 776/10000 (8%)\ttrain_Loss: 0.005367\tval_Loss: 0.006162\n",
      "Train Epoch: 777/10000 (8%)\ttrain_Loss: 0.005367\tval_Loss: 0.006161\n",
      "Train Epoch: 778/10000 (8%)\ttrain_Loss: 0.005366\tval_Loss: 0.006161\n",
      "Train Epoch: 779/10000 (8%)\ttrain_Loss: 0.005366\tval_Loss: 0.006160\n",
      "Train Epoch: 780/10000 (8%)\ttrain_Loss: 0.005366\tval_Loss: 0.006160\n",
      "Train Epoch: 781/10000 (8%)\ttrain_Loss: 0.005365\tval_Loss: 0.006160\n",
      "Train Epoch: 782/10000 (8%)\ttrain_Loss: 0.005365\tval_Loss: 0.006159\n",
      "Train Epoch: 783/10000 (8%)\ttrain_Loss: 0.005365\tval_Loss: 0.006159\n",
      "Train Epoch: 784/10000 (8%)\ttrain_Loss: 0.005364\tval_Loss: 0.006158\n",
      "Train Epoch: 785/10000 (8%)\ttrain_Loss: 0.005364\tval_Loss: 0.006158\n",
      "Train Epoch: 786/10000 (8%)\ttrain_Loss: 0.005364\tval_Loss: 0.006158\n",
      "Train Epoch: 787/10000 (8%)\ttrain_Loss: 0.005364\tval_Loss: 0.006157\n",
      "Train Epoch: 788/10000 (8%)\ttrain_Loss: 0.005363\tval_Loss: 0.006157\n",
      "Train Epoch: 789/10000 (8%)\ttrain_Loss: 0.005363\tval_Loss: 0.006157\n",
      "Train Epoch: 790/10000 (8%)\ttrain_Loss: 0.005363\tval_Loss: 0.006156\n",
      "Train Epoch: 791/10000 (8%)\ttrain_Loss: 0.005362\tval_Loss: 0.006156\n",
      "Train Epoch: 792/10000 (8%)\ttrain_Loss: 0.005362\tval_Loss: 0.006155\n",
      "Train Epoch: 793/10000 (8%)\ttrain_Loss: 0.005362\tval_Loss: 0.006155\n",
      "Train Epoch: 794/10000 (8%)\ttrain_Loss: 0.005361\tval_Loss: 0.006155\n",
      "Train Epoch: 795/10000 (8%)\ttrain_Loss: 0.005361\tval_Loss: 0.006154\n",
      "Train Epoch: 796/10000 (8%)\ttrain_Loss: 0.005361\tval_Loss: 0.006154\n",
      "Train Epoch: 797/10000 (8%)\ttrain_Loss: 0.005361\tval_Loss: 0.006153\n",
      "Train Epoch: 798/10000 (8%)\ttrain_Loss: 0.005360\tval_Loss: 0.006153\n",
      "Train Epoch: 799/10000 (8%)\ttrain_Loss: 0.005360\tval_Loss: 0.006153\n",
      "Train Epoch: 800/10000 (8%)\ttrain_Loss: 0.005360\tval_Loss: 0.006152\n",
      "Train Epoch: 801/10000 (8%)\ttrain_Loss: 0.005359\tval_Loss: 0.006152\n",
      "Train Epoch: 802/10000 (8%)\ttrain_Loss: 0.005359\tval_Loss: 0.006151\n",
      "Train Epoch: 803/10000 (8%)\ttrain_Loss: 0.005359\tval_Loss: 0.006151\n",
      "Train Epoch: 804/10000 (8%)\ttrain_Loss: 0.005358\tval_Loss: 0.006151\n",
      "Train Epoch: 805/10000 (8%)\ttrain_Loss: 0.005358\tval_Loss: 0.006150\n",
      "Train Epoch: 806/10000 (8%)\ttrain_Loss: 0.005358\tval_Loss: 0.006150\n",
      "Train Epoch: 807/10000 (8%)\ttrain_Loss: 0.005358\tval_Loss: 0.006149\n",
      "Train Epoch: 808/10000 (8%)\ttrain_Loss: 0.005357\tval_Loss: 0.006149\n",
      "Train Epoch: 809/10000 (8%)\ttrain_Loss: 0.005357\tval_Loss: 0.006148\n",
      "Train Epoch: 810/10000 (8%)\ttrain_Loss: 0.005357\tval_Loss: 0.006148\n",
      "Train Epoch: 811/10000 (8%)\ttrain_Loss: 0.005356\tval_Loss: 0.006148\n",
      "Train Epoch: 812/10000 (8%)\ttrain_Loss: 0.005356\tval_Loss: 0.006147\n",
      "Train Epoch: 813/10000 (8%)\ttrain_Loss: 0.005356\tval_Loss: 0.006147\n",
      "Train Epoch: 814/10000 (8%)\ttrain_Loss: 0.005355\tval_Loss: 0.006146\n",
      "Train Epoch: 815/10000 (8%)\ttrain_Loss: 0.005355\tval_Loss: 0.006146\n",
      "Train Epoch: 816/10000 (8%)\ttrain_Loss: 0.005355\tval_Loss: 0.006146\n",
      "Train Epoch: 817/10000 (8%)\ttrain_Loss: 0.005355\tval_Loss: 0.006145\n",
      "Train Epoch: 818/10000 (8%)\ttrain_Loss: 0.005354\tval_Loss: 0.006145\n",
      "Train Epoch: 819/10000 (8%)\ttrain_Loss: 0.005354\tval_Loss: 0.006145\n",
      "Train Epoch: 820/10000 (8%)\ttrain_Loss: 0.005354\tval_Loss: 0.006144\n",
      "Train Epoch: 821/10000 (8%)\ttrain_Loss: 0.005353\tval_Loss: 0.006144\n",
      "Train Epoch: 822/10000 (8%)\ttrain_Loss: 0.005353\tval_Loss: 0.006144\n",
      "Train Epoch: 823/10000 (8%)\ttrain_Loss: 0.005353\tval_Loss: 0.006144\n",
      "Train Epoch: 824/10000 (8%)\ttrain_Loss: 0.005352\tval_Loss: 0.006143\n",
      "Train Epoch: 825/10000 (8%)\ttrain_Loss: 0.005352\tval_Loss: 0.006143\n",
      "Train Epoch: 826/10000 (8%)\ttrain_Loss: 0.005352\tval_Loss: 0.006143\n",
      "Train Epoch: 827/10000 (8%)\ttrain_Loss: 0.005352\tval_Loss: 0.006142\n",
      "Train Epoch: 828/10000 (8%)\ttrain_Loss: 0.005351\tval_Loss: 0.006142\n",
      "Train Epoch: 829/10000 (8%)\ttrain_Loss: 0.005351\tval_Loss: 0.006142\n",
      "Train Epoch: 830/10000 (8%)\ttrain_Loss: 0.005351\tval_Loss: 0.006141\n",
      "Train Epoch: 831/10000 (8%)\ttrain_Loss: 0.005350\tval_Loss: 0.006141\n",
      "Train Epoch: 832/10000 (8%)\ttrain_Loss: 0.005350\tval_Loss: 0.006140\n",
      "Train Epoch: 833/10000 (8%)\ttrain_Loss: 0.005350\tval_Loss: 0.006140\n",
      "Train Epoch: 834/10000 (8%)\ttrain_Loss: 0.005350\tval_Loss: 0.006140\n",
      "Train Epoch: 835/10000 (8%)\ttrain_Loss: 0.005349\tval_Loss: 0.006139\n",
      "Train Epoch: 836/10000 (8%)\ttrain_Loss: 0.005349\tval_Loss: 0.006139\n",
      "Train Epoch: 837/10000 (8%)\ttrain_Loss: 0.005349\tval_Loss: 0.006138\n",
      "Train Epoch: 838/10000 (8%)\ttrain_Loss: 0.005348\tval_Loss: 0.006138\n",
      "Train Epoch: 839/10000 (8%)\ttrain_Loss: 0.005348\tval_Loss: 0.006138\n",
      "Train Epoch: 840/10000 (8%)\ttrain_Loss: 0.005348\tval_Loss: 0.006137\n",
      "Train Epoch: 841/10000 (8%)\ttrain_Loss: 0.005348\tval_Loss: 0.006137\n",
      "Train Epoch: 842/10000 (8%)\ttrain_Loss: 0.005347\tval_Loss: 0.006137\n",
      "Train Epoch: 843/10000 (8%)\ttrain_Loss: 0.005347\tval_Loss: 0.006136\n",
      "Train Epoch: 844/10000 (8%)\ttrain_Loss: 0.005347\tval_Loss: 0.006136\n",
      "Train Epoch: 845/10000 (8%)\ttrain_Loss: 0.005346\tval_Loss: 0.006135\n",
      "Train Epoch: 846/10000 (8%)\ttrain_Loss: 0.005346\tval_Loss: 0.006135\n",
      "Train Epoch: 847/10000 (8%)\ttrain_Loss: 0.005346\tval_Loss: 0.006135\n",
      "Train Epoch: 848/10000 (8%)\ttrain_Loss: 0.005346\tval_Loss: 0.006134\n",
      "Train Epoch: 849/10000 (8%)\ttrain_Loss: 0.005345\tval_Loss: 0.006134\n",
      "Train Epoch: 850/10000 (8%)\ttrain_Loss: 0.005345\tval_Loss: 0.006133\n",
      "Train Epoch: 851/10000 (8%)\ttrain_Loss: 0.005345\tval_Loss: 0.006133\n",
      "Train Epoch: 852/10000 (9%)\ttrain_Loss: 0.005345\tval_Loss: 0.006133\n",
      "Train Epoch: 853/10000 (9%)\ttrain_Loss: 0.005344\tval_Loss: 0.006132\n",
      "Train Epoch: 854/10000 (9%)\ttrain_Loss: 0.005344\tval_Loss: 0.006132\n",
      "Train Epoch: 855/10000 (9%)\ttrain_Loss: 0.005344\tval_Loss: 0.006131\n",
      "Train Epoch: 856/10000 (9%)\ttrain_Loss: 0.005343\tval_Loss: 0.006131\n",
      "Train Epoch: 857/10000 (9%)\ttrain_Loss: 0.005343\tval_Loss: 0.006130\n",
      "Train Epoch: 858/10000 (9%)\ttrain_Loss: 0.005343\tval_Loss: 0.006130\n",
      "Train Epoch: 859/10000 (9%)\ttrain_Loss: 0.005343\tval_Loss: 0.006130\n",
      "Train Epoch: 860/10000 (9%)\ttrain_Loss: 0.005342\tval_Loss: 0.006129\n",
      "Train Epoch: 861/10000 (9%)\ttrain_Loss: 0.005342\tval_Loss: 0.006129\n",
      "Train Epoch: 862/10000 (9%)\ttrain_Loss: 0.005342\tval_Loss: 0.006129\n",
      "Train Epoch: 863/10000 (9%)\ttrain_Loss: 0.005342\tval_Loss: 0.006128\n",
      "Train Epoch: 864/10000 (9%)\ttrain_Loss: 0.005341\tval_Loss: 0.006128\n",
      "Train Epoch: 865/10000 (9%)\ttrain_Loss: 0.005341\tval_Loss: 0.006128\n",
      "Train Epoch: 866/10000 (9%)\ttrain_Loss: 0.005341\tval_Loss: 0.006127\n",
      "Train Epoch: 867/10000 (9%)\ttrain_Loss: 0.005340\tval_Loss: 0.006127\n",
      "Train Epoch: 868/10000 (9%)\ttrain_Loss: 0.005340\tval_Loss: 0.006126\n",
      "Train Epoch: 869/10000 (9%)\ttrain_Loss: 0.005340\tval_Loss: 0.006126\n",
      "Train Epoch: 870/10000 (9%)\ttrain_Loss: 0.005340\tval_Loss: 0.006126\n",
      "Train Epoch: 871/10000 (9%)\ttrain_Loss: 0.005339\tval_Loss: 0.006125\n",
      "Train Epoch: 872/10000 (9%)\ttrain_Loss: 0.005339\tval_Loss: 0.006125\n",
      "Train Epoch: 873/10000 (9%)\ttrain_Loss: 0.005339\tval_Loss: 0.006125\n",
      "Train Epoch: 874/10000 (9%)\ttrain_Loss: 0.005339\tval_Loss: 0.006124\n",
      "Train Epoch: 875/10000 (9%)\ttrain_Loss: 0.005338\tval_Loss: 0.006124\n",
      "Train Epoch: 876/10000 (9%)\ttrain_Loss: 0.005338\tval_Loss: 0.006124\n",
      "Train Epoch: 877/10000 (9%)\ttrain_Loss: 0.005338\tval_Loss: 0.006123\n",
      "Train Epoch: 878/10000 (9%)\ttrain_Loss: 0.005337\tval_Loss: 0.006123\n",
      "Train Epoch: 879/10000 (9%)\ttrain_Loss: 0.005337\tval_Loss: 0.006122\n",
      "Train Epoch: 880/10000 (9%)\ttrain_Loss: 0.005337\tval_Loss: 0.006122\n",
      "Train Epoch: 881/10000 (9%)\ttrain_Loss: 0.005337\tval_Loss: 0.006122\n",
      "Train Epoch: 882/10000 (9%)\ttrain_Loss: 0.005336\tval_Loss: 0.006121\n",
      "Train Epoch: 883/10000 (9%)\ttrain_Loss: 0.005336\tval_Loss: 0.006121\n",
      "Train Epoch: 884/10000 (9%)\ttrain_Loss: 0.005336\tval_Loss: 0.006120\n",
      "Train Epoch: 885/10000 (9%)\ttrain_Loss: 0.005336\tval_Loss: 0.006120\n",
      "Train Epoch: 886/10000 (9%)\ttrain_Loss: 0.005335\tval_Loss: 0.006120\n",
      "Train Epoch: 887/10000 (9%)\ttrain_Loss: 0.005335\tval_Loss: 0.006119\n",
      "Train Epoch: 888/10000 (9%)\ttrain_Loss: 0.005335\tval_Loss: 0.006119\n",
      "Train Epoch: 889/10000 (9%)\ttrain_Loss: 0.005335\tval_Loss: 0.006119\n",
      "Train Epoch: 890/10000 (9%)\ttrain_Loss: 0.005334\tval_Loss: 0.006118\n",
      "Train Epoch: 891/10000 (9%)\ttrain_Loss: 0.005334\tval_Loss: 0.006118\n",
      "Train Epoch: 892/10000 (9%)\ttrain_Loss: 0.005334\tval_Loss: 0.006118\n",
      "Train Epoch: 893/10000 (9%)\ttrain_Loss: 0.005333\tval_Loss: 0.006117\n",
      "Train Epoch: 894/10000 (9%)\ttrain_Loss: 0.005333\tval_Loss: 0.006117\n",
      "Train Epoch: 895/10000 (9%)\ttrain_Loss: 0.005333\tval_Loss: 0.006117\n",
      "Train Epoch: 896/10000 (9%)\ttrain_Loss: 0.005333\tval_Loss: 0.006116\n",
      "Train Epoch: 897/10000 (9%)\ttrain_Loss: 0.005332\tval_Loss: 0.006116\n",
      "Train Epoch: 898/10000 (9%)\ttrain_Loss: 0.005332\tval_Loss: 0.006115\n",
      "Train Epoch: 899/10000 (9%)\ttrain_Loss: 0.005332\tval_Loss: 0.006115\n",
      "Train Epoch: 900/10000 (9%)\ttrain_Loss: 0.005332\tval_Loss: 0.006114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 901/10000 (9%)\ttrain_Loss: 0.005331\tval_Loss: 0.006114\n",
      "Train Epoch: 902/10000 (9%)\ttrain_Loss: 0.005331\tval_Loss: 0.006114\n",
      "Train Epoch: 903/10000 (9%)\ttrain_Loss: 0.005331\tval_Loss: 0.006113\n",
      "Train Epoch: 904/10000 (9%)\ttrain_Loss: 0.005331\tval_Loss: 0.006113\n",
      "Train Epoch: 905/10000 (9%)\ttrain_Loss: 0.005330\tval_Loss: 0.006112\n",
      "Train Epoch: 906/10000 (9%)\ttrain_Loss: 0.005330\tval_Loss: 0.006112\n",
      "Train Epoch: 907/10000 (9%)\ttrain_Loss: 0.005330\tval_Loss: 0.006112\n",
      "Train Epoch: 908/10000 (9%)\ttrain_Loss: 0.005330\tval_Loss: 0.006111\n",
      "Train Epoch: 909/10000 (9%)\ttrain_Loss: 0.005329\tval_Loss: 0.006111\n",
      "Train Epoch: 910/10000 (9%)\ttrain_Loss: 0.005329\tval_Loss: 0.006110\n",
      "Train Epoch: 911/10000 (9%)\ttrain_Loss: 0.005329\tval_Loss: 0.006110\n",
      "Train Epoch: 912/10000 (9%)\ttrain_Loss: 0.005329\tval_Loss: 0.006110\n",
      "Train Epoch: 913/10000 (9%)\ttrain_Loss: 0.005328\tval_Loss: 0.006109\n",
      "Train Epoch: 914/10000 (9%)\ttrain_Loss: 0.005328\tval_Loss: 0.006109\n",
      "Train Epoch: 915/10000 (9%)\ttrain_Loss: 0.005328\tval_Loss: 0.006109\n",
      "Train Epoch: 916/10000 (9%)\ttrain_Loss: 0.005327\tval_Loss: 0.006108\n",
      "Train Epoch: 917/10000 (9%)\ttrain_Loss: 0.005327\tval_Loss: 0.006108\n",
      "Train Epoch: 918/10000 (9%)\ttrain_Loss: 0.005327\tval_Loss: 0.006107\n",
      "Train Epoch: 919/10000 (9%)\ttrain_Loss: 0.005327\tval_Loss: 0.006107\n",
      "Train Epoch: 920/10000 (9%)\ttrain_Loss: 0.005326\tval_Loss: 0.006107\n",
      "Train Epoch: 921/10000 (9%)\ttrain_Loss: 0.005326\tval_Loss: 0.006106\n",
      "Train Epoch: 922/10000 (9%)\ttrain_Loss: 0.005326\tval_Loss: 0.006106\n",
      "Train Epoch: 923/10000 (9%)\ttrain_Loss: 0.005326\tval_Loss: 0.006106\n",
      "Train Epoch: 924/10000 (9%)\ttrain_Loss: 0.005325\tval_Loss: 0.006105\n",
      "Train Epoch: 925/10000 (9%)\ttrain_Loss: 0.005325\tval_Loss: 0.006105\n",
      "Train Epoch: 926/10000 (9%)\ttrain_Loss: 0.005325\tval_Loss: 0.006105\n",
      "Train Epoch: 927/10000 (9%)\ttrain_Loss: 0.005325\tval_Loss: 0.006104\n",
      "Train Epoch: 928/10000 (9%)\ttrain_Loss: 0.005324\tval_Loss: 0.006104\n",
      "Train Epoch: 929/10000 (9%)\ttrain_Loss: 0.005324\tval_Loss: 0.006104\n",
      "Train Epoch: 930/10000 (9%)\ttrain_Loss: 0.005324\tval_Loss: 0.006103\n",
      "Train Epoch: 931/10000 (9%)\ttrain_Loss: 0.005324\tval_Loss: 0.006103\n",
      "Train Epoch: 932/10000 (9%)\ttrain_Loss: 0.005323\tval_Loss: 0.006103\n",
      "Train Epoch: 933/10000 (9%)\ttrain_Loss: 0.005323\tval_Loss: 0.006103\n",
      "Train Epoch: 934/10000 (9%)\ttrain_Loss: 0.005323\tval_Loss: 0.006102\n",
      "Train Epoch: 935/10000 (9%)\ttrain_Loss: 0.005323\tval_Loss: 0.006102\n",
      "Train Epoch: 936/10000 (9%)\ttrain_Loss: 0.005322\tval_Loss: 0.006102\n",
      "Train Epoch: 937/10000 (9%)\ttrain_Loss: 0.005322\tval_Loss: 0.006101\n",
      "Train Epoch: 938/10000 (9%)\ttrain_Loss: 0.005322\tval_Loss: 0.006101\n",
      "Train Epoch: 939/10000 (9%)\ttrain_Loss: 0.005322\tval_Loss: 0.006100\n",
      "Train Epoch: 940/10000 (9%)\ttrain_Loss: 0.005321\tval_Loss: 0.006100\n",
      "Train Epoch: 941/10000 (9%)\ttrain_Loss: 0.005321\tval_Loss: 0.006099\n",
      "Train Epoch: 942/10000 (9%)\ttrain_Loss: 0.005321\tval_Loss: 0.006099\n",
      "Train Epoch: 943/10000 (9%)\ttrain_Loss: 0.005321\tval_Loss: 0.006098\n",
      "Train Epoch: 944/10000 (9%)\ttrain_Loss: 0.005320\tval_Loss: 0.006098\n",
      "Train Epoch: 945/10000 (9%)\ttrain_Loss: 0.005320\tval_Loss: 0.006097\n",
      "Train Epoch: 946/10000 (9%)\ttrain_Loss: 0.005320\tval_Loss: 0.006097\n",
      "Train Epoch: 947/10000 (9%)\ttrain_Loss: 0.005320\tval_Loss: 0.006097\n",
      "Train Epoch: 948/10000 (9%)\ttrain_Loss: 0.005319\tval_Loss: 0.006096\n",
      "Train Epoch: 949/10000 (9%)\ttrain_Loss: 0.005319\tval_Loss: 0.006096\n",
      "Train Epoch: 950/10000 (9%)\ttrain_Loss: 0.005319\tval_Loss: 0.006095\n",
      "Train Epoch: 951/10000 (10%)\ttrain_Loss: 0.005319\tval_Loss: 0.006095\n",
      "Train Epoch: 952/10000 (10%)\ttrain_Loss: 0.005318\tval_Loss: 0.006095\n",
      "Train Epoch: 953/10000 (10%)\ttrain_Loss: 0.005318\tval_Loss: 0.006094\n",
      "Train Epoch: 954/10000 (10%)\ttrain_Loss: 0.005318\tval_Loss: 0.006094\n",
      "Train Epoch: 955/10000 (10%)\ttrain_Loss: 0.005318\tval_Loss: 0.006094\n",
      "Train Epoch: 956/10000 (10%)\ttrain_Loss: 0.005318\tval_Loss: 0.006094\n",
      "Train Epoch: 957/10000 (10%)\ttrain_Loss: 0.005317\tval_Loss: 0.006093\n",
      "Train Epoch: 958/10000 (10%)\ttrain_Loss: 0.005317\tval_Loss: 0.006093\n",
      "Train Epoch: 959/10000 (10%)\ttrain_Loss: 0.005317\tval_Loss: 0.006093\n",
      "Train Epoch: 960/10000 (10%)\ttrain_Loss: 0.005317\tval_Loss: 0.006093\n",
      "Train Epoch: 961/10000 (10%)\ttrain_Loss: 0.005316\tval_Loss: 0.006092\n",
      "Train Epoch: 962/10000 (10%)\ttrain_Loss: 0.005316\tval_Loss: 0.006092\n",
      "Train Epoch: 963/10000 (10%)\ttrain_Loss: 0.005316\tval_Loss: 0.006092\n",
      "Train Epoch: 964/10000 (10%)\ttrain_Loss: 0.005316\tval_Loss: 0.006091\n",
      "Train Epoch: 965/10000 (10%)\ttrain_Loss: 0.005315\tval_Loss: 0.006091\n",
      "Train Epoch: 966/10000 (10%)\ttrain_Loss: 0.005315\tval_Loss: 0.006090\n",
      "Train Epoch: 967/10000 (10%)\ttrain_Loss: 0.005315\tval_Loss: 0.006090\n",
      "Train Epoch: 968/10000 (10%)\ttrain_Loss: 0.005315\tval_Loss: 0.006090\n",
      "Train Epoch: 969/10000 (10%)\ttrain_Loss: 0.005314\tval_Loss: 0.006089\n",
      "Train Epoch: 970/10000 (10%)\ttrain_Loss: 0.005314\tval_Loss: 0.006089\n",
      "Train Epoch: 971/10000 (10%)\ttrain_Loss: 0.005314\tval_Loss: 0.006089\n",
      "Train Epoch: 972/10000 (10%)\ttrain_Loss: 0.005314\tval_Loss: 0.006088\n",
      "Train Epoch: 973/10000 (10%)\ttrain_Loss: 0.005314\tval_Loss: 0.006088\n",
      "Train Epoch: 974/10000 (10%)\ttrain_Loss: 0.005313\tval_Loss: 0.006088\n",
      "Train Epoch: 975/10000 (10%)\ttrain_Loss: 0.005313\tval_Loss: 0.006088\n",
      "Train Epoch: 976/10000 (10%)\ttrain_Loss: 0.005313\tval_Loss: 0.006087\n",
      "Train Epoch: 977/10000 (10%)\ttrain_Loss: 0.005313\tval_Loss: 0.006087\n",
      "Train Epoch: 978/10000 (10%)\ttrain_Loss: 0.005312\tval_Loss: 0.006087\n",
      "Train Epoch: 979/10000 (10%)\ttrain_Loss: 0.005312\tval_Loss: 0.006086\n",
      "Train Epoch: 980/10000 (10%)\ttrain_Loss: 0.005312\tval_Loss: 0.006086\n",
      "Train Epoch: 981/10000 (10%)\ttrain_Loss: 0.005312\tval_Loss: 0.006086\n",
      "Train Epoch: 982/10000 (10%)\ttrain_Loss: 0.005312\tval_Loss: 0.006085\n",
      "Train Epoch: 983/10000 (10%)\ttrain_Loss: 0.005311\tval_Loss: 0.006085\n",
      "Train Epoch: 984/10000 (10%)\ttrain_Loss: 0.005311\tval_Loss: 0.006085\n",
      "Train Epoch: 985/10000 (10%)\ttrain_Loss: 0.005311\tval_Loss: 0.006084\n",
      "Train Epoch: 986/10000 (10%)\ttrain_Loss: 0.005311\tval_Loss: 0.006084\n",
      "Train Epoch: 987/10000 (10%)\ttrain_Loss: 0.005310\tval_Loss: 0.006084\n",
      "Train Epoch: 988/10000 (10%)\ttrain_Loss: 0.005310\tval_Loss: 0.006083\n",
      "Train Epoch: 989/10000 (10%)\ttrain_Loss: 0.005310\tval_Loss: 0.006083\n",
      "Train Epoch: 990/10000 (10%)\ttrain_Loss: 0.005310\tval_Loss: 0.006083\n",
      "Train Epoch: 991/10000 (10%)\ttrain_Loss: 0.005310\tval_Loss: 0.006082\n",
      "Train Epoch: 992/10000 (10%)\ttrain_Loss: 0.005309\tval_Loss: 0.006082\n",
      "Train Epoch: 993/10000 (10%)\ttrain_Loss: 0.005309\tval_Loss: 0.006082\n",
      "Train Epoch: 994/10000 (10%)\ttrain_Loss: 0.005309\tval_Loss: 0.006081\n",
      "Train Epoch: 995/10000 (10%)\ttrain_Loss: 0.005309\tval_Loss: 0.006081\n",
      "Train Epoch: 996/10000 (10%)\ttrain_Loss: 0.005309\tval_Loss: 0.006081\n",
      "Train Epoch: 997/10000 (10%)\ttrain_Loss: 0.005308\tval_Loss: 0.006081\n",
      "Train Epoch: 998/10000 (10%)\ttrain_Loss: 0.005308\tval_Loss: 0.006080\n",
      "Train Epoch: 999/10000 (10%)\ttrain_Loss: 0.005308\tval_Loss: 0.006080\n",
      "Train Epoch: 1000/10000 (10%)\ttrain_Loss: 0.005308\tval_Loss: 0.006080\n",
      "Train Epoch: 1001/10000 (10%)\ttrain_Loss: 0.005307\tval_Loss: 0.006079\n",
      "Train Epoch: 1002/10000 (10%)\ttrain_Loss: 0.005307\tval_Loss: 0.006079\n",
      "Train Epoch: 1003/10000 (10%)\ttrain_Loss: 0.005307\tval_Loss: 0.006079\n",
      "Train Epoch: 1004/10000 (10%)\ttrain_Loss: 0.005307\tval_Loss: 0.006078\n",
      "Train Epoch: 1005/10000 (10%)\ttrain_Loss: 0.005307\tval_Loss: 0.006078\n",
      "Train Epoch: 1006/10000 (10%)\ttrain_Loss: 0.005306\tval_Loss: 0.006078\n",
      "Train Epoch: 1007/10000 (10%)\ttrain_Loss: 0.005306\tval_Loss: 0.006077\n",
      "Train Epoch: 1008/10000 (10%)\ttrain_Loss: 0.005306\tval_Loss: 0.006077\n",
      "Train Epoch: 1009/10000 (10%)\ttrain_Loss: 0.005306\tval_Loss: 0.006077\n",
      "Train Epoch: 1010/10000 (10%)\ttrain_Loss: 0.005306\tval_Loss: 0.006076\n",
      "Train Epoch: 1011/10000 (10%)\ttrain_Loss: 0.005305\tval_Loss: 0.006076\n",
      "Train Epoch: 1012/10000 (10%)\ttrain_Loss: 0.005305\tval_Loss: 0.006076\n",
      "Train Epoch: 1013/10000 (10%)\ttrain_Loss: 0.005305\tval_Loss: 0.006075\n",
      "Train Epoch: 1014/10000 (10%)\ttrain_Loss: 0.005305\tval_Loss: 0.006075\n",
      "Train Epoch: 1015/10000 (10%)\ttrain_Loss: 0.005305\tval_Loss: 0.006075\n",
      "Train Epoch: 1016/10000 (10%)\ttrain_Loss: 0.005304\tval_Loss: 0.006074\n",
      "Train Epoch: 1017/10000 (10%)\ttrain_Loss: 0.005304\tval_Loss: 0.006074\n",
      "Train Epoch: 1018/10000 (10%)\ttrain_Loss: 0.005304\tval_Loss: 0.006074\n",
      "Train Epoch: 1019/10000 (10%)\ttrain_Loss: 0.005304\tval_Loss: 0.006074\n",
      "Train Epoch: 1020/10000 (10%)\ttrain_Loss: 0.005304\tval_Loss: 0.006073\n",
      "Train Epoch: 1021/10000 (10%)\ttrain_Loss: 0.005303\tval_Loss: 0.006073\n",
      "Train Epoch: 1022/10000 (10%)\ttrain_Loss: 0.005303\tval_Loss: 0.006073\n",
      "Train Epoch: 1023/10000 (10%)\ttrain_Loss: 0.005303\tval_Loss: 0.006073\n",
      "Train Epoch: 1024/10000 (10%)\ttrain_Loss: 0.005303\tval_Loss: 0.006073\n",
      "Train Epoch: 1025/10000 (10%)\ttrain_Loss: 0.005303\tval_Loss: 0.006072\n",
      "Train Epoch: 1026/10000 (10%)\ttrain_Loss: 0.005302\tval_Loss: 0.006072\n",
      "Train Epoch: 1027/10000 (10%)\ttrain_Loss: 0.005302\tval_Loss: 0.006072\n",
      "Train Epoch: 1028/10000 (10%)\ttrain_Loss: 0.005302\tval_Loss: 0.006071\n",
      "Train Epoch: 1029/10000 (10%)\ttrain_Loss: 0.005302\tval_Loss: 0.006071\n",
      "Train Epoch: 1030/10000 (10%)\ttrain_Loss: 0.005302\tval_Loss: 0.006070\n",
      "Train Epoch: 1031/10000 (10%)\ttrain_Loss: 0.005302\tval_Loss: 0.006070\n",
      "Train Epoch: 1032/10000 (10%)\ttrain_Loss: 0.005301\tval_Loss: 0.006070\n",
      "Train Epoch: 1033/10000 (10%)\ttrain_Loss: 0.005301\tval_Loss: 0.006070\n",
      "Train Epoch: 1034/10000 (10%)\ttrain_Loss: 0.005301\tval_Loss: 0.006070\n",
      "Train Epoch: 1035/10000 (10%)\ttrain_Loss: 0.005301\tval_Loss: 0.006069\n",
      "Train Epoch: 1036/10000 (10%)\ttrain_Loss: 0.005301\tval_Loss: 0.006069\n",
      "Train Epoch: 1037/10000 (10%)\ttrain_Loss: 0.005300\tval_Loss: 0.006069\n",
      "Train Epoch: 1038/10000 (10%)\ttrain_Loss: 0.005300\tval_Loss: 0.006068\n",
      "Train Epoch: 1039/10000 (10%)\ttrain_Loss: 0.005300\tval_Loss: 0.006068\n",
      "Train Epoch: 1040/10000 (10%)\ttrain_Loss: 0.005300\tval_Loss: 0.006067\n",
      "Train Epoch: 1041/10000 (10%)\ttrain_Loss: 0.005300\tval_Loss: 0.006067\n",
      "Train Epoch: 1042/10000 (10%)\ttrain_Loss: 0.005299\tval_Loss: 0.006067\n",
      "Train Epoch: 1043/10000 (10%)\ttrain_Loss: 0.005299\tval_Loss: 0.006067\n",
      "Train Epoch: 1044/10000 (10%)\ttrain_Loss: 0.005299\tval_Loss: 0.006066\n",
      "Train Epoch: 1045/10000 (10%)\ttrain_Loss: 0.005299\tval_Loss: 0.006066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1046/10000 (10%)\ttrain_Loss: 0.005299\tval_Loss: 0.006066\n",
      "Train Epoch: 1047/10000 (10%)\ttrain_Loss: 0.005298\tval_Loss: 0.006065\n",
      "Train Epoch: 1048/10000 (10%)\ttrain_Loss: 0.005298\tval_Loss: 0.006065\n",
      "Train Epoch: 1049/10000 (10%)\ttrain_Loss: 0.005298\tval_Loss: 0.006064\n",
      "Train Epoch: 1050/10000 (10%)\ttrain_Loss: 0.005298\tval_Loss: 0.006064\n",
      "Train Epoch: 1051/10000 (10%)\ttrain_Loss: 0.005298\tval_Loss: 0.006063\n",
      "Train Epoch: 1052/10000 (11%)\ttrain_Loss: 0.005298\tval_Loss: 0.006063\n",
      "Train Epoch: 1053/10000 (11%)\ttrain_Loss: 0.005297\tval_Loss: 0.006063\n",
      "Train Epoch: 1054/10000 (11%)\ttrain_Loss: 0.005297\tval_Loss: 0.006062\n",
      "Train Epoch: 1055/10000 (11%)\ttrain_Loss: 0.005297\tval_Loss: 0.006061\n",
      "Train Epoch: 1056/10000 (11%)\ttrain_Loss: 0.005297\tval_Loss: 0.006061\n",
      "Train Epoch: 1057/10000 (11%)\ttrain_Loss: 0.005297\tval_Loss: 0.006061\n",
      "Train Epoch: 1058/10000 (11%)\ttrain_Loss: 0.005296\tval_Loss: 0.006060\n",
      "Train Epoch: 1059/10000 (11%)\ttrain_Loss: 0.005296\tval_Loss: 0.006060\n",
      "Train Epoch: 1060/10000 (11%)\ttrain_Loss: 0.005296\tval_Loss: 0.006059\n",
      "Train Epoch: 1061/10000 (11%)\ttrain_Loss: 0.005296\tval_Loss: 0.006059\n",
      "Train Epoch: 1062/10000 (11%)\ttrain_Loss: 0.005296\tval_Loss: 0.006058\n",
      "Train Epoch: 1063/10000 (11%)\ttrain_Loss: 0.005295\tval_Loss: 0.006058\n",
      "Train Epoch: 1064/10000 (11%)\ttrain_Loss: 0.005295\tval_Loss: 0.006057\n",
      "Train Epoch: 1065/10000 (11%)\ttrain_Loss: 0.005295\tval_Loss: 0.006057\n",
      "Train Epoch: 1066/10000 (11%)\ttrain_Loss: 0.005295\tval_Loss: 0.006057\n",
      "Train Epoch: 1067/10000 (11%)\ttrain_Loss: 0.005295\tval_Loss: 0.006057\n",
      "Train Epoch: 1068/10000 (11%)\ttrain_Loss: 0.005294\tval_Loss: 0.006057\n",
      "Train Epoch: 1069/10000 (11%)\ttrain_Loss: 0.005294\tval_Loss: 0.006057\n",
      "Train Epoch: 1070/10000 (11%)\ttrain_Loss: 0.005294\tval_Loss: 0.006057\n",
      "Train Epoch: 1071/10000 (11%)\ttrain_Loss: 0.005294\tval_Loss: 0.006057\n",
      "Train Epoch: 1072/10000 (11%)\ttrain_Loss: 0.005294\tval_Loss: 0.006057\n",
      "Train Epoch: 1073/10000 (11%)\ttrain_Loss: 0.005293\tval_Loss: 0.006057\n",
      "Train Epoch: 1074/10000 (11%)\ttrain_Loss: 0.005293\tval_Loss: 0.006057\n",
      "Train Epoch: 1075/10000 (11%)\ttrain_Loss: 0.005293\tval_Loss: 0.006057\n",
      "Train Epoch: 1076/10000 (11%)\ttrain_Loss: 0.005293\tval_Loss: 0.006057\n",
      "Train Epoch: 1077/10000 (11%)\ttrain_Loss: 0.005293\tval_Loss: 0.006057\n",
      "Train Epoch: 1078/10000 (11%)\ttrain_Loss: 0.005292\tval_Loss: 0.006057\n",
      "Train Epoch: 1079/10000 (11%)\ttrain_Loss: 0.005292\tval_Loss: 0.006057\n",
      "Train Epoch: 1080/10000 (11%)\ttrain_Loss: 0.005292\tval_Loss: 0.006056\n",
      "Train Epoch: 1081/10000 (11%)\ttrain_Loss: 0.005292\tval_Loss: 0.006056\n",
      "Train Epoch: 1082/10000 (11%)\ttrain_Loss: 0.005292\tval_Loss: 0.006056\n",
      "Train Epoch: 1083/10000 (11%)\ttrain_Loss: 0.005291\tval_Loss: 0.006055\n",
      "Train Epoch: 1084/10000 (11%)\ttrain_Loss: 0.005291\tval_Loss: 0.006055\n",
      "Train Epoch: 1085/10000 (11%)\ttrain_Loss: 0.005291\tval_Loss: 0.006054\n",
      "Train Epoch: 1086/10000 (11%)\ttrain_Loss: 0.005291\tval_Loss: 0.006054\n",
      "Train Epoch: 1087/10000 (11%)\ttrain_Loss: 0.005291\tval_Loss: 0.006054\n",
      "Train Epoch: 1088/10000 (11%)\ttrain_Loss: 0.005291\tval_Loss: 0.006053\n",
      "Train Epoch: 1089/10000 (11%)\ttrain_Loss: 0.005290\tval_Loss: 0.006053\n",
      "Train Epoch: 1090/10000 (11%)\ttrain_Loss: 0.005290\tval_Loss: 0.006053\n",
      "Train Epoch: 1091/10000 (11%)\ttrain_Loss: 0.005290\tval_Loss: 0.006052\n",
      "Train Epoch: 1092/10000 (11%)\ttrain_Loss: 0.005290\tval_Loss: 0.006052\n",
      "Train Epoch: 1093/10000 (11%)\ttrain_Loss: 0.005290\tval_Loss: 0.006052\n",
      "Train Epoch: 1094/10000 (11%)\ttrain_Loss: 0.005289\tval_Loss: 0.006051\n",
      "Train Epoch: 1095/10000 (11%)\ttrain_Loss: 0.005289\tval_Loss: 0.006051\n",
      "Train Epoch: 1096/10000 (11%)\ttrain_Loss: 0.005289\tval_Loss: 0.006051\n",
      "Train Epoch: 1097/10000 (11%)\ttrain_Loss: 0.005289\tval_Loss: 0.006051\n",
      "Train Epoch: 1098/10000 (11%)\ttrain_Loss: 0.005289\tval_Loss: 0.006050\n",
      "Train Epoch: 1099/10000 (11%)\ttrain_Loss: 0.005289\tval_Loss: 0.006050\n",
      "Train Epoch: 1100/10000 (11%)\ttrain_Loss: 0.005288\tval_Loss: 0.006050\n",
      "Train Epoch: 1101/10000 (11%)\ttrain_Loss: 0.005288\tval_Loss: 0.006049\n",
      "Train Epoch: 1102/10000 (11%)\ttrain_Loss: 0.005288\tval_Loss: 0.006049\n",
      "Train Epoch: 1103/10000 (11%)\ttrain_Loss: 0.005288\tval_Loss: 0.006049\n",
      "Train Epoch: 1104/10000 (11%)\ttrain_Loss: 0.005288\tval_Loss: 0.006049\n",
      "Train Epoch: 1105/10000 (11%)\ttrain_Loss: 0.005287\tval_Loss: 0.006049\n",
      "Train Epoch: 1106/10000 (11%)\ttrain_Loss: 0.005287\tval_Loss: 0.006048\n",
      "Train Epoch: 1107/10000 (11%)\ttrain_Loss: 0.005287\tval_Loss: 0.006048\n",
      "Train Epoch: 1108/10000 (11%)\ttrain_Loss: 0.005287\tval_Loss: 0.006048\n",
      "Train Epoch: 1109/10000 (11%)\ttrain_Loss: 0.005287\tval_Loss: 0.006047\n",
      "Train Epoch: 1110/10000 (11%)\ttrain_Loss: 0.005287\tval_Loss: 0.006047\n",
      "Train Epoch: 1111/10000 (11%)\ttrain_Loss: 0.005286\tval_Loss: 0.006047\n",
      "Train Epoch: 1112/10000 (11%)\ttrain_Loss: 0.005286\tval_Loss: 0.006046\n",
      "Train Epoch: 1113/10000 (11%)\ttrain_Loss: 0.005286\tval_Loss: 0.006046\n",
      "Train Epoch: 1114/10000 (11%)\ttrain_Loss: 0.005286\tval_Loss: 0.006046\n",
      "Train Epoch: 1115/10000 (11%)\ttrain_Loss: 0.005286\tval_Loss: 0.006046\n",
      "Train Epoch: 1116/10000 (11%)\ttrain_Loss: 0.005285\tval_Loss: 0.006046\n",
      "Train Epoch: 1117/10000 (11%)\ttrain_Loss: 0.005285\tval_Loss: 0.006045\n",
      "Train Epoch: 1118/10000 (11%)\ttrain_Loss: 0.005285\tval_Loss: 0.006045\n",
      "Train Epoch: 1119/10000 (11%)\ttrain_Loss: 0.005285\tval_Loss: 0.006044\n",
      "Train Epoch: 1120/10000 (11%)\ttrain_Loss: 0.005285\tval_Loss: 0.006044\n",
      "Train Epoch: 1121/10000 (11%)\ttrain_Loss: 0.005285\tval_Loss: 0.006044\n",
      "Train Epoch: 1122/10000 (11%)\ttrain_Loss: 0.005284\tval_Loss: 0.006044\n",
      "Train Epoch: 1123/10000 (11%)\ttrain_Loss: 0.005284\tval_Loss: 0.006043\n",
      "Train Epoch: 1124/10000 (11%)\ttrain_Loss: 0.005284\tval_Loss: 0.006043\n",
      "Train Epoch: 1125/10000 (11%)\ttrain_Loss: 0.005284\tval_Loss: 0.006043\n",
      "Train Epoch: 1126/10000 (11%)\ttrain_Loss: 0.005284\tval_Loss: 0.006042\n",
      "Train Epoch: 1127/10000 (11%)\ttrain_Loss: 0.005284\tval_Loss: 0.006042\n",
      "Train Epoch: 1128/10000 (11%)\ttrain_Loss: 0.005283\tval_Loss: 0.006042\n",
      "Train Epoch: 1129/10000 (11%)\ttrain_Loss: 0.005283\tval_Loss: 0.006041\n",
      "Train Epoch: 1130/10000 (11%)\ttrain_Loss: 0.005283\tval_Loss: 0.006041\n",
      "Train Epoch: 1131/10000 (11%)\ttrain_Loss: 0.005283\tval_Loss: 0.006041\n",
      "Train Epoch: 1132/10000 (11%)\ttrain_Loss: 0.005283\tval_Loss: 0.006040\n",
      "Train Epoch: 1133/10000 (11%)\ttrain_Loss: 0.005283\tval_Loss: 0.006040\n",
      "Train Epoch: 1134/10000 (11%)\ttrain_Loss: 0.005282\tval_Loss: 0.006040\n",
      "Train Epoch: 1135/10000 (11%)\ttrain_Loss: 0.005282\tval_Loss: 0.006039\n",
      "Train Epoch: 1136/10000 (11%)\ttrain_Loss: 0.005282\tval_Loss: 0.006039\n",
      "Train Epoch: 1137/10000 (11%)\ttrain_Loss: 0.005282\tval_Loss: 0.006039\n",
      "Train Epoch: 1138/10000 (11%)\ttrain_Loss: 0.005282\tval_Loss: 0.006038\n",
      "Train Epoch: 1139/10000 (11%)\ttrain_Loss: 0.005282\tval_Loss: 0.006038\n",
      "Train Epoch: 1140/10000 (11%)\ttrain_Loss: 0.005281\tval_Loss: 0.006037\n",
      "Train Epoch: 1141/10000 (11%)\ttrain_Loss: 0.005281\tval_Loss: 0.006037\n",
      "Train Epoch: 1142/10000 (11%)\ttrain_Loss: 0.005281\tval_Loss: 0.006037\n",
      "Train Epoch: 1143/10000 (11%)\ttrain_Loss: 0.005281\tval_Loss: 0.006037\n",
      "Train Epoch: 1144/10000 (11%)\ttrain_Loss: 0.005281\tval_Loss: 0.006036\n",
      "Train Epoch: 1145/10000 (11%)\ttrain_Loss: 0.005281\tval_Loss: 0.006036\n",
      "Train Epoch: 1146/10000 (11%)\ttrain_Loss: 0.005280\tval_Loss: 0.006036\n",
      "Train Epoch: 1147/10000 (11%)\ttrain_Loss: 0.005280\tval_Loss: 0.006036\n",
      "Train Epoch: 1148/10000 (11%)\ttrain_Loss: 0.005280\tval_Loss: 0.006035\n",
      "Train Epoch: 1149/10000 (11%)\ttrain_Loss: 0.005280\tval_Loss: 0.006035\n",
      "Train Epoch: 1150/10000 (11%)\ttrain_Loss: 0.005280\tval_Loss: 0.006035\n",
      "Train Epoch: 1151/10000 (12%)\ttrain_Loss: 0.005279\tval_Loss: 0.006035\n",
      "Train Epoch: 1152/10000 (12%)\ttrain_Loss: 0.005279\tval_Loss: 0.006034\n",
      "Train Epoch: 1153/10000 (12%)\ttrain_Loss: 0.005279\tval_Loss: 0.006034\n",
      "Train Epoch: 1154/10000 (12%)\ttrain_Loss: 0.005279\tval_Loss: 0.006034\n",
      "Train Epoch: 1155/10000 (12%)\ttrain_Loss: 0.005279\tval_Loss: 0.006034\n",
      "Train Epoch: 1156/10000 (12%)\ttrain_Loss: 0.005279\tval_Loss: 0.006033\n",
      "Train Epoch: 1157/10000 (12%)\ttrain_Loss: 0.005278\tval_Loss: 0.006033\n",
      "Train Epoch: 1158/10000 (12%)\ttrain_Loss: 0.005278\tval_Loss: 0.006033\n",
      "Train Epoch: 1159/10000 (12%)\ttrain_Loss: 0.005278\tval_Loss: 0.006033\n",
      "Train Epoch: 1160/10000 (12%)\ttrain_Loss: 0.005278\tval_Loss: 0.006033\n",
      "Train Epoch: 1161/10000 (12%)\ttrain_Loss: 0.005278\tval_Loss: 0.006033\n",
      "Train Epoch: 1162/10000 (12%)\ttrain_Loss: 0.005278\tval_Loss: 0.006032\n",
      "Train Epoch: 1163/10000 (12%)\ttrain_Loss: 0.005277\tval_Loss: 0.006032\n",
      "Train Epoch: 1164/10000 (12%)\ttrain_Loss: 0.005277\tval_Loss: 0.006032\n",
      "Train Epoch: 1165/10000 (12%)\ttrain_Loss: 0.005277\tval_Loss: 0.006032\n",
      "Train Epoch: 1166/10000 (12%)\ttrain_Loss: 0.005277\tval_Loss: 0.006032\n",
      "Train Epoch: 1167/10000 (12%)\ttrain_Loss: 0.005277\tval_Loss: 0.006032\n",
      "Train Epoch: 1168/10000 (12%)\ttrain_Loss: 0.005277\tval_Loss: 0.006032\n",
      "Train Epoch: 1169/10000 (12%)\ttrain_Loss: 0.005276\tval_Loss: 0.006032\n",
      "Train Epoch: 1170/10000 (12%)\ttrain_Loss: 0.005276\tval_Loss: 0.006031\n",
      "Train Epoch: 1171/10000 (12%)\ttrain_Loss: 0.005276\tval_Loss: 0.006031\n",
      "Train Epoch: 1172/10000 (12%)\ttrain_Loss: 0.005276\tval_Loss: 0.006031\n",
      "Train Epoch: 1173/10000 (12%)\ttrain_Loss: 0.005276\tval_Loss: 0.006030\n",
      "Train Epoch: 1174/10000 (12%)\ttrain_Loss: 0.005276\tval_Loss: 0.006030\n",
      "Train Epoch: 1175/10000 (12%)\ttrain_Loss: 0.005275\tval_Loss: 0.006030\n",
      "Train Epoch: 1176/10000 (12%)\ttrain_Loss: 0.005275\tval_Loss: 0.006030\n",
      "Train Epoch: 1177/10000 (12%)\ttrain_Loss: 0.005275\tval_Loss: 0.006029\n",
      "Train Epoch: 1178/10000 (12%)\ttrain_Loss: 0.005275\tval_Loss: 0.006029\n",
      "Train Epoch: 1179/10000 (12%)\ttrain_Loss: 0.005275\tval_Loss: 0.006029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1180/10000 (12%)\ttrain_Loss: 0.005275\tval_Loss: 0.006028\n",
      "Train Epoch: 1181/10000 (12%)\ttrain_Loss: 0.005274\tval_Loss: 0.006028\n",
      "Train Epoch: 1182/10000 (12%)\ttrain_Loss: 0.005274\tval_Loss: 0.006028\n",
      "Train Epoch: 1183/10000 (12%)\ttrain_Loss: 0.005274\tval_Loss: 0.006027\n",
      "Train Epoch: 1184/10000 (12%)\ttrain_Loss: 0.005274\tval_Loss: 0.006027\n",
      "Train Epoch: 1185/10000 (12%)\ttrain_Loss: 0.005274\tval_Loss: 0.006027\n",
      "Train Epoch: 1186/10000 (12%)\ttrain_Loss: 0.005274\tval_Loss: 0.006027\n",
      "Train Epoch: 1187/10000 (12%)\ttrain_Loss: 0.005273\tval_Loss: 0.006026\n",
      "Train Epoch: 1188/10000 (12%)\ttrain_Loss: 0.005273\tval_Loss: 0.006026\n",
      "Train Epoch: 1189/10000 (12%)\ttrain_Loss: 0.005273\tval_Loss: 0.006026\n",
      "Train Epoch: 1190/10000 (12%)\ttrain_Loss: 0.005273\tval_Loss: 0.006026\n",
      "Train Epoch: 1191/10000 (12%)\ttrain_Loss: 0.005273\tval_Loss: 0.006025\n",
      "Train Epoch: 1192/10000 (12%)\ttrain_Loss: 0.005273\tval_Loss: 0.006025\n",
      "Train Epoch: 1193/10000 (12%)\ttrain_Loss: 0.005272\tval_Loss: 0.006025\n",
      "Train Epoch: 1194/10000 (12%)\ttrain_Loss: 0.005272\tval_Loss: 0.006025\n",
      "Train Epoch: 1195/10000 (12%)\ttrain_Loss: 0.005272\tval_Loss: 0.006024\n",
      "Train Epoch: 1196/10000 (12%)\ttrain_Loss: 0.005272\tval_Loss: 0.006024\n",
      "Train Epoch: 1197/10000 (12%)\ttrain_Loss: 0.005272\tval_Loss: 0.006024\n",
      "Train Epoch: 1198/10000 (12%)\ttrain_Loss: 0.005272\tval_Loss: 0.006024\n",
      "Train Epoch: 1199/10000 (12%)\ttrain_Loss: 0.005272\tval_Loss: 0.006023\n",
      "Train Epoch: 1200/10000 (12%)\ttrain_Loss: 0.005271\tval_Loss: 0.006023\n",
      "Train Epoch: 1201/10000 (12%)\ttrain_Loss: 0.005271\tval_Loss: 0.006023\n",
      "Train Epoch: 1202/10000 (12%)\ttrain_Loss: 0.005271\tval_Loss: 0.006023\n",
      "Train Epoch: 1203/10000 (12%)\ttrain_Loss: 0.005271\tval_Loss: 0.006023\n",
      "Train Epoch: 1204/10000 (12%)\ttrain_Loss: 0.005271\tval_Loss: 0.006022\n",
      "Train Epoch: 1205/10000 (12%)\ttrain_Loss: 0.005271\tval_Loss: 0.006022\n",
      "Train Epoch: 1206/10000 (12%)\ttrain_Loss: 0.005270\tval_Loss: 0.006022\n",
      "Train Epoch: 1207/10000 (12%)\ttrain_Loss: 0.005270\tval_Loss: 0.006022\n",
      "Train Epoch: 1208/10000 (12%)\ttrain_Loss: 0.005270\tval_Loss: 0.006021\n",
      "Train Epoch: 1209/10000 (12%)\ttrain_Loss: 0.005270\tval_Loss: 0.006021\n",
      "Train Epoch: 1210/10000 (12%)\ttrain_Loss: 0.005270\tval_Loss: 0.006021\n",
      "Train Epoch: 1211/10000 (12%)\ttrain_Loss: 0.005270\tval_Loss: 0.006021\n",
      "Train Epoch: 1212/10000 (12%)\ttrain_Loss: 0.005269\tval_Loss: 0.006021\n",
      "Train Epoch: 1213/10000 (12%)\ttrain_Loss: 0.005269\tval_Loss: 0.006020\n",
      "Train Epoch: 1214/10000 (12%)\ttrain_Loss: 0.005269\tval_Loss: 0.006020\n",
      "Train Epoch: 1215/10000 (12%)\ttrain_Loss: 0.005269\tval_Loss: 0.006020\n",
      "Train Epoch: 1216/10000 (12%)\ttrain_Loss: 0.005269\tval_Loss: 0.006020\n",
      "Train Epoch: 1217/10000 (12%)\ttrain_Loss: 0.005269\tval_Loss: 0.006020\n",
      "Train Epoch: 1218/10000 (12%)\ttrain_Loss: 0.005268\tval_Loss: 0.006019\n",
      "Train Epoch: 1219/10000 (12%)\ttrain_Loss: 0.005268\tval_Loss: 0.006019\n",
      "Train Epoch: 1220/10000 (12%)\ttrain_Loss: 0.005268\tval_Loss: 0.006019\n",
      "Train Epoch: 1221/10000 (12%)\ttrain_Loss: 0.005268\tval_Loss: 0.006019\n",
      "Train Epoch: 1222/10000 (12%)\ttrain_Loss: 0.005268\tval_Loss: 0.006019\n",
      "Train Epoch: 1223/10000 (12%)\ttrain_Loss: 0.005268\tval_Loss: 0.006019\n",
      "Train Epoch: 1224/10000 (12%)\ttrain_Loss: 0.005268\tval_Loss: 0.006018\n",
      "Train Epoch: 1225/10000 (12%)\ttrain_Loss: 0.005267\tval_Loss: 0.006018\n",
      "Train Epoch: 1226/10000 (12%)\ttrain_Loss: 0.005267\tval_Loss: 0.006018\n",
      "Train Epoch: 1227/10000 (12%)\ttrain_Loss: 0.005267\tval_Loss: 0.006018\n",
      "Train Epoch: 1228/10000 (12%)\ttrain_Loss: 0.005267\tval_Loss: 0.006018\n",
      "Train Epoch: 1229/10000 (12%)\ttrain_Loss: 0.005267\tval_Loss: 0.006018\n",
      "Train Epoch: 1230/10000 (12%)\ttrain_Loss: 0.005267\tval_Loss: 0.006017\n",
      "Train Epoch: 1231/10000 (12%)\ttrain_Loss: 0.005266\tval_Loss: 0.006017\n",
      "Train Epoch: 1232/10000 (12%)\ttrain_Loss: 0.005266\tval_Loss: 0.006017\n",
      "Train Epoch: 1233/10000 (12%)\ttrain_Loss: 0.005266\tval_Loss: 0.006017\n",
      "Train Epoch: 1234/10000 (12%)\ttrain_Loss: 0.005266\tval_Loss: 0.006017\n",
      "Train Epoch: 1235/10000 (12%)\ttrain_Loss: 0.005266\tval_Loss: 0.006017\n",
      "Train Epoch: 1236/10000 (12%)\ttrain_Loss: 0.005266\tval_Loss: 0.006016\n",
      "Train Epoch: 1237/10000 (12%)\ttrain_Loss: 0.005265\tval_Loss: 0.006016\n",
      "Train Epoch: 1238/10000 (12%)\ttrain_Loss: 0.005265\tval_Loss: 0.006016\n",
      "Train Epoch: 1239/10000 (12%)\ttrain_Loss: 0.005265\tval_Loss: 0.006016\n",
      "Train Epoch: 1240/10000 (12%)\ttrain_Loss: 0.005265\tval_Loss: 0.006015\n",
      "Train Epoch: 1241/10000 (12%)\ttrain_Loss: 0.005265\tval_Loss: 0.006015\n",
      "Train Epoch: 1242/10000 (12%)\ttrain_Loss: 0.005265\tval_Loss: 0.006015\n",
      "Train Epoch: 1243/10000 (12%)\ttrain_Loss: 0.005264\tval_Loss: 0.006015\n",
      "Train Epoch: 1244/10000 (12%)\ttrain_Loss: 0.005264\tval_Loss: 0.006015\n",
      "Train Epoch: 1245/10000 (12%)\ttrain_Loss: 0.005264\tval_Loss: 0.006015\n",
      "Train Epoch: 1246/10000 (12%)\ttrain_Loss: 0.005264\tval_Loss: 0.006015\n",
      "Train Epoch: 1247/10000 (12%)\ttrain_Loss: 0.005264\tval_Loss: 0.006015\n",
      "Train Epoch: 1248/10000 (12%)\ttrain_Loss: 0.005264\tval_Loss: 0.006015\n",
      "Train Epoch: 1249/10000 (12%)\ttrain_Loss: 0.005263\tval_Loss: 0.006015\n",
      "Train Epoch: 1250/10000 (12%)\ttrain_Loss: 0.005263\tval_Loss: 0.006015\n",
      "Train Epoch: 1251/10000 (12%)\ttrain_Loss: 0.005263\tval_Loss: 0.006014\n",
      "Train Epoch: 1252/10000 (13%)\ttrain_Loss: 0.005263\tval_Loss: 0.006014\n",
      "Train Epoch: 1253/10000 (13%)\ttrain_Loss: 0.005263\tval_Loss: 0.006014\n",
      "Train Epoch: 1254/10000 (13%)\ttrain_Loss: 0.005263\tval_Loss: 0.006014\n",
      "Train Epoch: 1255/10000 (13%)\ttrain_Loss: 0.005262\tval_Loss: 0.006013\n",
      "Train Epoch: 1256/10000 (13%)\ttrain_Loss: 0.005262\tval_Loss: 0.006013\n",
      "Train Epoch: 1257/10000 (13%)\ttrain_Loss: 0.005262\tval_Loss: 0.006013\n",
      "Train Epoch: 1258/10000 (13%)\ttrain_Loss: 0.005262\tval_Loss: 0.006012\n",
      "Train Epoch: 1259/10000 (13%)\ttrain_Loss: 0.005262\tval_Loss: 0.006012\n",
      "Train Epoch: 1260/10000 (13%)\ttrain_Loss: 0.005262\tval_Loss: 0.006012\n",
      "Train Epoch: 1261/10000 (13%)\ttrain_Loss: 0.005261\tval_Loss: 0.006012\n",
      "Train Epoch: 1262/10000 (13%)\ttrain_Loss: 0.005261\tval_Loss: 0.006012\n",
      "Train Epoch: 1263/10000 (13%)\ttrain_Loss: 0.005261\tval_Loss: 0.006012\n",
      "Train Epoch: 1264/10000 (13%)\ttrain_Loss: 0.005261\tval_Loss: 0.006011\n",
      "Train Epoch: 1265/10000 (13%)\ttrain_Loss: 0.005261\tval_Loss: 0.006011\n",
      "Train Epoch: 1266/10000 (13%)\ttrain_Loss: 0.005261\tval_Loss: 0.006011\n",
      "Train Epoch: 1267/10000 (13%)\ttrain_Loss: 0.005261\tval_Loss: 0.006011\n",
      "Train Epoch: 1268/10000 (13%)\ttrain_Loss: 0.005260\tval_Loss: 0.006010\n",
      "Train Epoch: 1269/10000 (13%)\ttrain_Loss: 0.005260\tval_Loss: 0.006010\n",
      "Train Epoch: 1270/10000 (13%)\ttrain_Loss: 0.005260\tval_Loss: 0.006010\n",
      "Train Epoch: 1271/10000 (13%)\ttrain_Loss: 0.005260\tval_Loss: 0.006009\n",
      "Train Epoch: 1272/10000 (13%)\ttrain_Loss: 0.005260\tval_Loss: 0.006009\n",
      "Train Epoch: 1273/10000 (13%)\ttrain_Loss: 0.005260\tval_Loss: 0.006009\n",
      "Train Epoch: 1274/10000 (13%)\ttrain_Loss: 0.005259\tval_Loss: 0.006009\n",
      "Train Epoch: 1275/10000 (13%)\ttrain_Loss: 0.005259\tval_Loss: 0.006008\n",
      "Train Epoch: 1276/10000 (13%)\ttrain_Loss: 0.005259\tval_Loss: 0.006008\n",
      "Train Epoch: 1277/10000 (13%)\ttrain_Loss: 0.005259\tval_Loss: 0.006008\n",
      "Train Epoch: 1278/10000 (13%)\ttrain_Loss: 0.005259\tval_Loss: 0.006008\n",
      "Train Epoch: 1279/10000 (13%)\ttrain_Loss: 0.005259\tval_Loss: 0.006008\n",
      "Train Epoch: 1280/10000 (13%)\ttrain_Loss: 0.005259\tval_Loss: 0.006007\n",
      "Train Epoch: 1281/10000 (13%)\ttrain_Loss: 0.005258\tval_Loss: 0.006007\n",
      "Train Epoch: 1282/10000 (13%)\ttrain_Loss: 0.005258\tval_Loss: 0.006007\n",
      "Train Epoch: 1283/10000 (13%)\ttrain_Loss: 0.005258\tval_Loss: 0.006007\n",
      "Train Epoch: 1284/10000 (13%)\ttrain_Loss: 0.005258\tval_Loss: 0.006007\n",
      "Train Epoch: 1285/10000 (13%)\ttrain_Loss: 0.005258\tval_Loss: 0.006007\n",
      "Train Epoch: 1286/10000 (13%)\ttrain_Loss: 0.005258\tval_Loss: 0.006006\n",
      "Train Epoch: 1287/10000 (13%)\ttrain_Loss: 0.005257\tval_Loss: 0.006006\n",
      "Train Epoch: 1288/10000 (13%)\ttrain_Loss: 0.005257\tval_Loss: 0.006005\n",
      "Train Epoch: 1289/10000 (13%)\ttrain_Loss: 0.005257\tval_Loss: 0.006005\n",
      "Train Epoch: 1290/10000 (13%)\ttrain_Loss: 0.005257\tval_Loss: 0.006005\n",
      "Train Epoch: 1291/10000 (13%)\ttrain_Loss: 0.005257\tval_Loss: 0.006005\n",
      "Train Epoch: 1292/10000 (13%)\ttrain_Loss: 0.005257\tval_Loss: 0.006004\n",
      "Train Epoch: 1293/10000 (13%)\ttrain_Loss: 0.005257\tval_Loss: 0.006004\n",
      "Train Epoch: 1294/10000 (13%)\ttrain_Loss: 0.005256\tval_Loss: 0.006004\n",
      "Train Epoch: 1295/10000 (13%)\ttrain_Loss: 0.005256\tval_Loss: 0.006004\n",
      "Train Epoch: 1296/10000 (13%)\ttrain_Loss: 0.005256\tval_Loss: 0.006004\n",
      "Train Epoch: 1297/10000 (13%)\ttrain_Loss: 0.005256\tval_Loss: 0.006003\n",
      "Train Epoch: 1298/10000 (13%)\ttrain_Loss: 0.005256\tval_Loss: 0.006003\n",
      "Train Epoch: 1299/10000 (13%)\ttrain_Loss: 0.005256\tval_Loss: 0.006003\n",
      "Train Epoch: 1300/10000 (13%)\ttrain_Loss: 0.005256\tval_Loss: 0.006003\n",
      "Train Epoch: 1301/10000 (13%)\ttrain_Loss: 0.005255\tval_Loss: 0.006003\n",
      "Train Epoch: 1302/10000 (13%)\ttrain_Loss: 0.005255\tval_Loss: 0.006002\n",
      "Train Epoch: 1303/10000 (13%)\ttrain_Loss: 0.005255\tval_Loss: 0.006002\n",
      "Train Epoch: 1304/10000 (13%)\ttrain_Loss: 0.005255\tval_Loss: 0.006002\n",
      "Train Epoch: 1305/10000 (13%)\ttrain_Loss: 0.005255\tval_Loss: 0.006002\n",
      "Train Epoch: 1306/10000 (13%)\ttrain_Loss: 0.005255\tval_Loss: 0.006001\n",
      "Train Epoch: 1307/10000 (13%)\ttrain_Loss: 0.005255\tval_Loss: 0.006001\n",
      "Train Epoch: 1308/10000 (13%)\ttrain_Loss: 0.005254\tval_Loss: 0.006001\n",
      "Train Epoch: 1309/10000 (13%)\ttrain_Loss: 0.005254\tval_Loss: 0.006001\n",
      "Train Epoch: 1310/10000 (13%)\ttrain_Loss: 0.005254\tval_Loss: 0.006001\n",
      "Train Epoch: 1311/10000 (13%)\ttrain_Loss: 0.005254\tval_Loss: 0.006000\n",
      "Train Epoch: 1312/10000 (13%)\ttrain_Loss: 0.005254\tval_Loss: 0.006000\n",
      "Train Epoch: 1313/10000 (13%)\ttrain_Loss: 0.005254\tval_Loss: 0.006000\n",
      "Train Epoch: 1314/10000 (13%)\ttrain_Loss: 0.005254\tval_Loss: 0.006000\n",
      "Train Epoch: 1315/10000 (13%)\ttrain_Loss: 0.005253\tval_Loss: 0.005999\n",
      "Train Epoch: 1316/10000 (13%)\ttrain_Loss: 0.005253\tval_Loss: 0.005999\n",
      "Train Epoch: 1317/10000 (13%)\ttrain_Loss: 0.005253\tval_Loss: 0.005999\n",
      "Train Epoch: 1318/10000 (13%)\ttrain_Loss: 0.005253\tval_Loss: 0.005999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1319/10000 (13%)\ttrain_Loss: 0.005253\tval_Loss: 0.005999\n",
      "Train Epoch: 1320/10000 (13%)\ttrain_Loss: 0.005253\tval_Loss: 0.005999\n",
      "Train Epoch: 1321/10000 (13%)\ttrain_Loss: 0.005253\tval_Loss: 0.005998\n",
      "Train Epoch: 1322/10000 (13%)\ttrain_Loss: 0.005253\tval_Loss: 0.005998\n",
      "Train Epoch: 1323/10000 (13%)\ttrain_Loss: 0.005252\tval_Loss: 0.005998\n",
      "Train Epoch: 1324/10000 (13%)\ttrain_Loss: 0.005252\tval_Loss: 0.005997\n",
      "Train Epoch: 1325/10000 (13%)\ttrain_Loss: 0.005252\tval_Loss: 0.005997\n",
      "Train Epoch: 1326/10000 (13%)\ttrain_Loss: 0.005252\tval_Loss: 0.005997\n",
      "Train Epoch: 1327/10000 (13%)\ttrain_Loss: 0.005252\tval_Loss: 0.005997\n",
      "Train Epoch: 1328/10000 (13%)\ttrain_Loss: 0.005252\tval_Loss: 0.005997\n",
      "Train Epoch: 1329/10000 (13%)\ttrain_Loss: 0.005252\tval_Loss: 0.005997\n",
      "Train Epoch: 1330/10000 (13%)\ttrain_Loss: 0.005251\tval_Loss: 0.005996\n",
      "Train Epoch: 1331/10000 (13%)\ttrain_Loss: 0.005251\tval_Loss: 0.005996\n",
      "Train Epoch: 1332/10000 (13%)\ttrain_Loss: 0.005251\tval_Loss: 0.005996\n",
      "Train Epoch: 1333/10000 (13%)\ttrain_Loss: 0.005251\tval_Loss: 0.005996\n",
      "Train Epoch: 1334/10000 (13%)\ttrain_Loss: 0.005251\tval_Loss: 0.005995\n",
      "Train Epoch: 1335/10000 (13%)\ttrain_Loss: 0.005251\tval_Loss: 0.005995\n",
      "Train Epoch: 1336/10000 (13%)\ttrain_Loss: 0.005251\tval_Loss: 0.005995\n",
      "Train Epoch: 1337/10000 (13%)\ttrain_Loss: 0.005250\tval_Loss: 0.005995\n",
      "Train Epoch: 1338/10000 (13%)\ttrain_Loss: 0.005250\tval_Loss: 0.005994\n",
      "Train Epoch: 1339/10000 (13%)\ttrain_Loss: 0.005250\tval_Loss: 0.005994\n",
      "Train Epoch: 1340/10000 (13%)\ttrain_Loss: 0.005250\tval_Loss: 0.005994\n",
      "Train Epoch: 1341/10000 (13%)\ttrain_Loss: 0.005250\tval_Loss: 0.005994\n",
      "Train Epoch: 1342/10000 (13%)\ttrain_Loss: 0.005250\tval_Loss: 0.005994\n",
      "Train Epoch: 1343/10000 (13%)\ttrain_Loss: 0.005250\tval_Loss: 0.005993\n",
      "Train Epoch: 1344/10000 (13%)\ttrain_Loss: 0.005250\tval_Loss: 0.005993\n",
      "Train Epoch: 1345/10000 (13%)\ttrain_Loss: 0.005249\tval_Loss: 0.005993\n",
      "Train Epoch: 1346/10000 (13%)\ttrain_Loss: 0.005249\tval_Loss: 0.005993\n",
      "Train Epoch: 1347/10000 (13%)\ttrain_Loss: 0.005249\tval_Loss: 0.005993\n",
      "Train Epoch: 1348/10000 (13%)\ttrain_Loss: 0.005249\tval_Loss: 0.005992\n",
      "Train Epoch: 1349/10000 (13%)\ttrain_Loss: 0.005249\tval_Loss: 0.005992\n",
      "Train Epoch: 1350/10000 (13%)\ttrain_Loss: 0.005249\tval_Loss: 0.005992\n",
      "Train Epoch: 1351/10000 (14%)\ttrain_Loss: 0.005249\tval_Loss: 0.005992\n",
      "Train Epoch: 1352/10000 (14%)\ttrain_Loss: 0.005248\tval_Loss: 0.005991\n",
      "Train Epoch: 1353/10000 (14%)\ttrain_Loss: 0.005248\tval_Loss: 0.005991\n",
      "Train Epoch: 1354/10000 (14%)\ttrain_Loss: 0.005248\tval_Loss: 0.005991\n",
      "Train Epoch: 1355/10000 (14%)\ttrain_Loss: 0.005248\tval_Loss: 0.005991\n",
      "Train Epoch: 1356/10000 (14%)\ttrain_Loss: 0.005248\tval_Loss: 0.005990\n",
      "Train Epoch: 1357/10000 (14%)\ttrain_Loss: 0.005248\tval_Loss: 0.005990\n",
      "Train Epoch: 1358/10000 (14%)\ttrain_Loss: 0.005248\tval_Loss: 0.005990\n",
      "Train Epoch: 1359/10000 (14%)\ttrain_Loss: 0.005248\tval_Loss: 0.005990\n",
      "Train Epoch: 1360/10000 (14%)\ttrain_Loss: 0.005247\tval_Loss: 0.005990\n",
      "Train Epoch: 1361/10000 (14%)\ttrain_Loss: 0.005247\tval_Loss: 0.005990\n",
      "Train Epoch: 1362/10000 (14%)\ttrain_Loss: 0.005247\tval_Loss: 0.005989\n",
      "Train Epoch: 1363/10000 (14%)\ttrain_Loss: 0.005247\tval_Loss: 0.005989\n",
      "Train Epoch: 1364/10000 (14%)\ttrain_Loss: 0.005247\tval_Loss: 0.005989\n",
      "Train Epoch: 1365/10000 (14%)\ttrain_Loss: 0.005247\tval_Loss: 0.005989\n",
      "Train Epoch: 1366/10000 (14%)\ttrain_Loss: 0.005247\tval_Loss: 0.005989\n",
      "Train Epoch: 1367/10000 (14%)\ttrain_Loss: 0.005247\tval_Loss: 0.005988\n",
      "Train Epoch: 1368/10000 (14%)\ttrain_Loss: 0.005246\tval_Loss: 0.005988\n",
      "Train Epoch: 1369/10000 (14%)\ttrain_Loss: 0.005246\tval_Loss: 0.005988\n",
      "Train Epoch: 1370/10000 (14%)\ttrain_Loss: 0.005246\tval_Loss: 0.005988\n",
      "Train Epoch: 1371/10000 (14%)\ttrain_Loss: 0.005246\tval_Loss: 0.005987\n",
      "Train Epoch: 1372/10000 (14%)\ttrain_Loss: 0.005246\tval_Loss: 0.005987\n",
      "Train Epoch: 1373/10000 (14%)\ttrain_Loss: 0.005246\tval_Loss: 0.005987\n",
      "Train Epoch: 1374/10000 (14%)\ttrain_Loss: 0.005246\tval_Loss: 0.005987\n",
      "Train Epoch: 1375/10000 (14%)\ttrain_Loss: 0.005246\tval_Loss: 0.005987\n",
      "Train Epoch: 1376/10000 (14%)\ttrain_Loss: 0.005245\tval_Loss: 0.005986\n",
      "Train Epoch: 1377/10000 (14%)\ttrain_Loss: 0.005245\tval_Loss: 0.005986\n",
      "Train Epoch: 1378/10000 (14%)\ttrain_Loss: 0.005245\tval_Loss: 0.005986\n",
      "Train Epoch: 1379/10000 (14%)\ttrain_Loss: 0.005245\tval_Loss: 0.005986\n",
      "Train Epoch: 1380/10000 (14%)\ttrain_Loss: 0.005245\tval_Loss: 0.005986\n",
      "Train Epoch: 1381/10000 (14%)\ttrain_Loss: 0.005245\tval_Loss: 0.005985\n",
      "Train Epoch: 1382/10000 (14%)\ttrain_Loss: 0.005245\tval_Loss: 0.005985\n",
      "Train Epoch: 1383/10000 (14%)\ttrain_Loss: 0.005245\tval_Loss: 0.005985\n",
      "Train Epoch: 1384/10000 (14%)\ttrain_Loss: 0.005245\tval_Loss: 0.005985\n",
      "Train Epoch: 1385/10000 (14%)\ttrain_Loss: 0.005244\tval_Loss: 0.005984\n",
      "Train Epoch: 1386/10000 (14%)\ttrain_Loss: 0.005244\tval_Loss: 0.005984\n",
      "Train Epoch: 1387/10000 (14%)\ttrain_Loss: 0.005244\tval_Loss: 0.005984\n",
      "Train Epoch: 1388/10000 (14%)\ttrain_Loss: 0.005244\tval_Loss: 0.005984\n",
      "Train Epoch: 1389/10000 (14%)\ttrain_Loss: 0.005244\tval_Loss: 0.005983\n",
      "Train Epoch: 1390/10000 (14%)\ttrain_Loss: 0.005244\tval_Loss: 0.005983\n",
      "Train Epoch: 1391/10000 (14%)\ttrain_Loss: 0.005244\tval_Loss: 0.005983\n",
      "Train Epoch: 1392/10000 (14%)\ttrain_Loss: 0.005244\tval_Loss: 0.005983\n",
      "Train Epoch: 1393/10000 (14%)\ttrain_Loss: 0.005244\tval_Loss: 0.005983\n",
      "Train Epoch: 1394/10000 (14%)\ttrain_Loss: 0.005243\tval_Loss: 0.005983\n",
      "Train Epoch: 1395/10000 (14%)\ttrain_Loss: 0.005243\tval_Loss: 0.005982\n",
      "Train Epoch: 1396/10000 (14%)\ttrain_Loss: 0.005243\tval_Loss: 0.005982\n",
      "Train Epoch: 1397/10000 (14%)\ttrain_Loss: 0.005243\tval_Loss: 0.005982\n",
      "Train Epoch: 1398/10000 (14%)\ttrain_Loss: 0.005243\tval_Loss: 0.005982\n",
      "Train Epoch: 1399/10000 (14%)\ttrain_Loss: 0.005243\tval_Loss: 0.005981\n",
      "Train Epoch: 1400/10000 (14%)\ttrain_Loss: 0.005243\tval_Loss: 0.005981\n",
      "Train Epoch: 1401/10000 (14%)\ttrain_Loss: 0.005243\tval_Loss: 0.005981\n",
      "Train Epoch: 1402/10000 (14%)\ttrain_Loss: 0.005242\tval_Loss: 0.005981\n",
      "Train Epoch: 1403/10000 (14%)\ttrain_Loss: 0.005242\tval_Loss: 0.005981\n",
      "Train Epoch: 1404/10000 (14%)\ttrain_Loss: 0.005242\tval_Loss: 0.005981\n",
      "Train Epoch: 1405/10000 (14%)\ttrain_Loss: 0.005242\tval_Loss: 0.005980\n",
      "Train Epoch: 1406/10000 (14%)\ttrain_Loss: 0.005242\tval_Loss: 0.005980\n",
      "Train Epoch: 1407/10000 (14%)\ttrain_Loss: 0.005242\tval_Loss: 0.005980\n",
      "Train Epoch: 1408/10000 (14%)\ttrain_Loss: 0.005242\tval_Loss: 0.005980\n",
      "Train Epoch: 1409/10000 (14%)\ttrain_Loss: 0.005242\tval_Loss: 0.005980\n",
      "Train Epoch: 1410/10000 (14%)\ttrain_Loss: 0.005242\tval_Loss: 0.005980\n",
      "Train Epoch: 1411/10000 (14%)\ttrain_Loss: 0.005241\tval_Loss: 0.005980\n",
      "Train Epoch: 1412/10000 (14%)\ttrain_Loss: 0.005241\tval_Loss: 0.005979\n",
      "Train Epoch: 1413/10000 (14%)\ttrain_Loss: 0.005241\tval_Loss: 0.005979\n",
      "Train Epoch: 1414/10000 (14%)\ttrain_Loss: 0.005241\tval_Loss: 0.005979\n",
      "Train Epoch: 1415/10000 (14%)\ttrain_Loss: 0.005241\tval_Loss: 0.005979\n",
      "Train Epoch: 1416/10000 (14%)\ttrain_Loss: 0.005241\tval_Loss: 0.005979\n",
      "Train Epoch: 1417/10000 (14%)\ttrain_Loss: 0.005241\tval_Loss: 0.005979\n",
      "Train Epoch: 1418/10000 (14%)\ttrain_Loss: 0.005241\tval_Loss: 0.005979\n",
      "Train Epoch: 1419/10000 (14%)\ttrain_Loss: 0.005241\tval_Loss: 0.005979\n",
      "Train Epoch: 1420/10000 (14%)\ttrain_Loss: 0.005240\tval_Loss: 0.005979\n",
      "Train Epoch: 1421/10000 (14%)\ttrain_Loss: 0.005240\tval_Loss: 0.005978\n",
      "Train Epoch: 1422/10000 (14%)\ttrain_Loss: 0.005240\tval_Loss: 0.005978\n",
      "Train Epoch: 1423/10000 (14%)\ttrain_Loss: 0.005240\tval_Loss: 0.005978\n",
      "Train Epoch: 1424/10000 (14%)\ttrain_Loss: 0.005240\tval_Loss: 0.005978\n",
      "Train Epoch: 1425/10000 (14%)\ttrain_Loss: 0.005240\tval_Loss: 0.005978\n",
      "Train Epoch: 1426/10000 (14%)\ttrain_Loss: 0.005240\tval_Loss: 0.005978\n",
      "Train Epoch: 1427/10000 (14%)\ttrain_Loss: 0.005240\tval_Loss: 0.005978\n",
      "Train Epoch: 1428/10000 (14%)\ttrain_Loss: 0.005240\tval_Loss: 0.005977\n",
      "Train Epoch: 1429/10000 (14%)\ttrain_Loss: 0.005239\tval_Loss: 0.005977\n",
      "Train Epoch: 1430/10000 (14%)\ttrain_Loss: 0.005239\tval_Loss: 0.005977\n",
      "Train Epoch: 1431/10000 (14%)\ttrain_Loss: 0.005239\tval_Loss: 0.005977\n",
      "Train Epoch: 1432/10000 (14%)\ttrain_Loss: 0.005239\tval_Loss: 0.005977\n",
      "Train Epoch: 1433/10000 (14%)\ttrain_Loss: 0.005239\tval_Loss: 0.005977\n",
      "Train Epoch: 1434/10000 (14%)\ttrain_Loss: 0.005239\tval_Loss: 0.005977\n",
      "Train Epoch: 1435/10000 (14%)\ttrain_Loss: 0.005239\tval_Loss: 0.005976\n",
      "Train Epoch: 1436/10000 (14%)\ttrain_Loss: 0.005239\tval_Loss: 0.005976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1437/10000 (14%)\ttrain_Loss: 0.005239\tval_Loss: 0.005976\n",
      "Train Epoch: 1438/10000 (14%)\ttrain_Loss: 0.005238\tval_Loss: 0.005976\n",
      "Train Epoch: 1439/10000 (14%)\ttrain_Loss: 0.005238\tval_Loss: 0.005976\n",
      "Train Epoch: 1440/10000 (14%)\ttrain_Loss: 0.005238\tval_Loss: 0.005976\n",
      "Train Epoch: 1441/10000 (14%)\ttrain_Loss: 0.005238\tval_Loss: 0.005976\n",
      "Train Epoch: 1442/10000 (14%)\ttrain_Loss: 0.005238\tval_Loss: 0.005975\n",
      "Train Epoch: 1443/10000 (14%)\ttrain_Loss: 0.005238\tval_Loss: 0.005975\n",
      "Train Epoch: 1444/10000 (14%)\ttrain_Loss: 0.005238\tval_Loss: 0.005975\n",
      "Train Epoch: 1445/10000 (14%)\ttrain_Loss: 0.005238\tval_Loss: 0.005975\n",
      "Train Epoch: 1446/10000 (14%)\ttrain_Loss: 0.005238\tval_Loss: 0.005975\n",
      "Train Epoch: 1447/10000 (14%)\ttrain_Loss: 0.005237\tval_Loss: 0.005975\n",
      "Train Epoch: 1448/10000 (14%)\ttrain_Loss: 0.005237\tval_Loss: 0.005975\n",
      "Train Epoch: 1449/10000 (14%)\ttrain_Loss: 0.005237\tval_Loss: 0.005974\n",
      "Train Epoch: 1450/10000 (14%)\ttrain_Loss: 0.005237\tval_Loss: 0.005974\n",
      "Train Epoch: 1451/10000 (14%)\ttrain_Loss: 0.005237\tval_Loss: 0.005974\n",
      "Train Epoch: 1452/10000 (15%)\ttrain_Loss: 0.005237\tval_Loss: 0.005974\n",
      "Train Epoch: 1453/10000 (15%)\ttrain_Loss: 0.005237\tval_Loss: 0.005974\n",
      "Train Epoch: 1454/10000 (15%)\ttrain_Loss: 0.005237\tval_Loss: 0.005973\n",
      "Train Epoch: 1455/10000 (15%)\ttrain_Loss: 0.005237\tval_Loss: 0.005973\n",
      "Train Epoch: 1456/10000 (15%)\ttrain_Loss: 0.005236\tval_Loss: 0.005973\n",
      "Train Epoch: 1457/10000 (15%)\ttrain_Loss: 0.005236\tval_Loss: 0.005973\n",
      "Train Epoch: 1458/10000 (15%)\ttrain_Loss: 0.005236\tval_Loss: 0.005973\n",
      "Train Epoch: 1459/10000 (15%)\ttrain_Loss: 0.005236\tval_Loss: 0.005972\n",
      "Train Epoch: 1460/10000 (15%)\ttrain_Loss: 0.005236\tval_Loss: 0.005972\n",
      "Train Epoch: 1461/10000 (15%)\ttrain_Loss: 0.005236\tval_Loss: 0.005972\n",
      "Train Epoch: 1462/10000 (15%)\ttrain_Loss: 0.005236\tval_Loss: 0.005972\n",
      "Train Epoch: 1463/10000 (15%)\ttrain_Loss: 0.005236\tval_Loss: 0.005971\n",
      "Train Epoch: 1464/10000 (15%)\ttrain_Loss: 0.005235\tval_Loss: 0.005971\n",
      "Train Epoch: 1465/10000 (15%)\ttrain_Loss: 0.005235\tval_Loss: 0.005971\n",
      "Train Epoch: 1466/10000 (15%)\ttrain_Loss: 0.005235\tval_Loss: 0.005971\n",
      "Train Epoch: 1467/10000 (15%)\ttrain_Loss: 0.005235\tval_Loss: 0.005971\n",
      "Train Epoch: 1468/10000 (15%)\ttrain_Loss: 0.005235\tval_Loss: 0.005971\n",
      "Train Epoch: 1469/10000 (15%)\ttrain_Loss: 0.005235\tval_Loss: 0.005971\n",
      "Train Epoch: 1470/10000 (15%)\ttrain_Loss: 0.005235\tval_Loss: 0.005972\n",
      "Train Epoch: 1471/10000 (15%)\ttrain_Loss: 0.005235\tval_Loss: 0.005972\n",
      "Train Epoch: 1472/10000 (15%)\ttrain_Loss: 0.005235\tval_Loss: 0.005972\n",
      "Train Epoch: 1473/10000 (15%)\ttrain_Loss: 0.005234\tval_Loss: 0.005972\n",
      "Train Epoch: 1474/10000 (15%)\ttrain_Loss: 0.005234\tval_Loss: 0.005972\n",
      "Train Epoch: 1475/10000 (15%)\ttrain_Loss: 0.005234\tval_Loss: 0.005971\n",
      "Train Epoch: 1476/10000 (15%)\ttrain_Loss: 0.005234\tval_Loss: 0.005971\n",
      "Train Epoch: 1477/10000 (15%)\ttrain_Loss: 0.005234\tval_Loss: 0.005970\n",
      "Train Epoch: 1478/10000 (15%)\ttrain_Loss: 0.005234\tval_Loss: 0.005970\n",
      "Train Epoch: 1479/10000 (15%)\ttrain_Loss: 0.005234\tval_Loss: 0.005970\n",
      "Train Epoch: 1480/10000 (15%)\ttrain_Loss: 0.005234\tval_Loss: 0.005970\n",
      "Train Epoch: 1481/10000 (15%)\ttrain_Loss: 0.005234\tval_Loss: 0.005969\n",
      "Train Epoch: 1482/10000 (15%)\ttrain_Loss: 0.005233\tval_Loss: 0.005969\n",
      "Train Epoch: 1483/10000 (15%)\ttrain_Loss: 0.005233\tval_Loss: 0.005969\n",
      "Train Epoch: 1484/10000 (15%)\ttrain_Loss: 0.005233\tval_Loss: 0.005969\n",
      "Train Epoch: 1485/10000 (15%)\ttrain_Loss: 0.005233\tval_Loss: 0.005969\n",
      "Train Epoch: 1486/10000 (15%)\ttrain_Loss: 0.005233\tval_Loss: 0.005969\n",
      "Train Epoch: 1487/10000 (15%)\ttrain_Loss: 0.005233\tval_Loss: 0.005969\n",
      "Train Epoch: 1488/10000 (15%)\ttrain_Loss: 0.005233\tval_Loss: 0.005969\n",
      "Train Epoch: 1489/10000 (15%)\ttrain_Loss: 0.005233\tval_Loss: 0.005969\n",
      "Train Epoch: 1490/10000 (15%)\ttrain_Loss: 0.005233\tval_Loss: 0.005968\n",
      "Train Epoch: 1491/10000 (15%)\ttrain_Loss: 0.005232\tval_Loss: 0.005968\n",
      "Train Epoch: 1492/10000 (15%)\ttrain_Loss: 0.005232\tval_Loss: 0.005968\n",
      "Train Epoch: 1493/10000 (15%)\ttrain_Loss: 0.005232\tval_Loss: 0.005967\n",
      "Train Epoch: 1494/10000 (15%)\ttrain_Loss: 0.005232\tval_Loss: 0.005967\n",
      "Train Epoch: 1495/10000 (15%)\ttrain_Loss: 0.005232\tval_Loss: 0.005967\n",
      "Train Epoch: 1496/10000 (15%)\ttrain_Loss: 0.005232\tval_Loss: 0.005967\n",
      "Train Epoch: 1497/10000 (15%)\ttrain_Loss: 0.005232\tval_Loss: 0.005967\n",
      "Train Epoch: 1498/10000 (15%)\ttrain_Loss: 0.005232\tval_Loss: 0.005967\n",
      "Train Epoch: 1499/10000 (15%)\ttrain_Loss: 0.005231\tval_Loss: 0.005967\n",
      "Train Epoch: 1500/10000 (15%)\ttrain_Loss: 0.005231\tval_Loss: 0.005966\n",
      "Train Epoch: 1501/10000 (15%)\ttrain_Loss: 0.005231\tval_Loss: 0.005966\n",
      "Train Epoch: 1502/10000 (15%)\ttrain_Loss: 0.005231\tval_Loss: 0.005966\n",
      "Train Epoch: 1503/10000 (15%)\ttrain_Loss: 0.005231\tval_Loss: 0.005965\n",
      "Train Epoch: 1504/10000 (15%)\ttrain_Loss: 0.005231\tval_Loss: 0.005965\n",
      "Train Epoch: 1505/10000 (15%)\ttrain_Loss: 0.005231\tval_Loss: 0.005965\n",
      "Train Epoch: 1506/10000 (15%)\ttrain_Loss: 0.005231\tval_Loss: 0.005965\n",
      "Train Epoch: 1507/10000 (15%)\ttrain_Loss: 0.005231\tval_Loss: 0.005965\n",
      "Train Epoch: 1508/10000 (15%)\ttrain_Loss: 0.005230\tval_Loss: 0.005965\n",
      "Train Epoch: 1509/10000 (15%)\ttrain_Loss: 0.005230\tval_Loss: 0.005965\n",
      "Train Epoch: 1510/10000 (15%)\ttrain_Loss: 0.005230\tval_Loss: 0.005965\n",
      "Train Epoch: 1511/10000 (15%)\ttrain_Loss: 0.005230\tval_Loss: 0.005965\n",
      "Train Epoch: 1512/10000 (15%)\ttrain_Loss: 0.005230\tval_Loss: 0.005964\n",
      "Train Epoch: 1513/10000 (15%)\ttrain_Loss: 0.005230\tval_Loss: 0.005964\n",
      "Train Epoch: 1514/10000 (15%)\ttrain_Loss: 0.005230\tval_Loss: 0.005964\n",
      "Train Epoch: 1515/10000 (15%)\ttrain_Loss: 0.005230\tval_Loss: 0.005964\n",
      "Train Epoch: 1516/10000 (15%)\ttrain_Loss: 0.005230\tval_Loss: 0.005964\n",
      "Train Epoch: 1517/10000 (15%)\ttrain_Loss: 0.005229\tval_Loss: 0.005963\n",
      "Train Epoch: 1518/10000 (15%)\ttrain_Loss: 0.005229\tval_Loss: 0.005963\n",
      "Train Epoch: 1519/10000 (15%)\ttrain_Loss: 0.005229\tval_Loss: 0.005963\n",
      "Train Epoch: 1520/10000 (15%)\ttrain_Loss: 0.005229\tval_Loss: 0.005963\n",
      "Train Epoch: 1521/10000 (15%)\ttrain_Loss: 0.005229\tval_Loss: 0.005963\n",
      "Train Epoch: 1522/10000 (15%)\ttrain_Loss: 0.005229\tval_Loss: 0.005963\n",
      "Train Epoch: 1523/10000 (15%)\ttrain_Loss: 0.005229\tval_Loss: 0.005963\n",
      "Train Epoch: 1524/10000 (15%)\ttrain_Loss: 0.005229\tval_Loss: 0.005962\n",
      "Train Epoch: 1525/10000 (15%)\ttrain_Loss: 0.005229\tval_Loss: 0.005962\n",
      "Train Epoch: 1526/10000 (15%)\ttrain_Loss: 0.005228\tval_Loss: 0.005962\n",
      "Train Epoch: 1527/10000 (15%)\ttrain_Loss: 0.005228\tval_Loss: 0.005962\n",
      "Train Epoch: 1528/10000 (15%)\ttrain_Loss: 0.005228\tval_Loss: 0.005962\n",
      "Train Epoch: 1529/10000 (15%)\ttrain_Loss: 0.005228\tval_Loss: 0.005962\n",
      "Train Epoch: 1530/10000 (15%)\ttrain_Loss: 0.005228\tval_Loss: 0.005961\n",
      "Train Epoch: 1531/10000 (15%)\ttrain_Loss: 0.005228\tval_Loss: 0.005961\n",
      "Train Epoch: 1532/10000 (15%)\ttrain_Loss: 0.005228\tval_Loss: 0.005961\n",
      "Train Epoch: 1533/10000 (15%)\ttrain_Loss: 0.005228\tval_Loss: 0.005961\n",
      "Train Epoch: 1534/10000 (15%)\ttrain_Loss: 0.005228\tval_Loss: 0.005961\n",
      "Train Epoch: 1535/10000 (15%)\ttrain_Loss: 0.005227\tval_Loss: 0.005961\n",
      "Train Epoch: 1536/10000 (15%)\ttrain_Loss: 0.005227\tval_Loss: 0.005960\n",
      "Train Epoch: 1537/10000 (15%)\ttrain_Loss: 0.005227\tval_Loss: 0.005960\n",
      "Train Epoch: 1538/10000 (15%)\ttrain_Loss: 0.005227\tval_Loss: 0.005960\n",
      "Train Epoch: 1539/10000 (15%)\ttrain_Loss: 0.005227\tval_Loss: 0.005960\n",
      "Train Epoch: 1540/10000 (15%)\ttrain_Loss: 0.005227\tval_Loss: 0.005960\n",
      "Train Epoch: 1541/10000 (15%)\ttrain_Loss: 0.005227\tval_Loss: 0.005960\n",
      "Train Epoch: 1542/10000 (15%)\ttrain_Loss: 0.005227\tval_Loss: 0.005959\n",
      "Train Epoch: 1543/10000 (15%)\ttrain_Loss: 0.005227\tval_Loss: 0.005959\n",
      "Train Epoch: 1544/10000 (15%)\ttrain_Loss: 0.005227\tval_Loss: 0.005959\n",
      "Train Epoch: 1545/10000 (15%)\ttrain_Loss: 0.005226\tval_Loss: 0.005959\n",
      "Train Epoch: 1546/10000 (15%)\ttrain_Loss: 0.005226\tval_Loss: 0.005959\n",
      "Train Epoch: 1547/10000 (15%)\ttrain_Loss: 0.005226\tval_Loss: 0.005959\n",
      "Train Epoch: 1548/10000 (15%)\ttrain_Loss: 0.005226\tval_Loss: 0.005959\n",
      "Train Epoch: 1549/10000 (15%)\ttrain_Loss: 0.005226\tval_Loss: 0.005958\n",
      "Train Epoch: 1550/10000 (15%)\ttrain_Loss: 0.005226\tval_Loss: 0.005958\n",
      "Train Epoch: 1551/10000 (16%)\ttrain_Loss: 0.005226\tval_Loss: 0.005958\n",
      "Train Epoch: 1552/10000 (16%)\ttrain_Loss: 0.005226\tval_Loss: 0.005958\n",
      "Train Epoch: 1553/10000 (16%)\ttrain_Loss: 0.005226\tval_Loss: 0.005958\n",
      "Train Epoch: 1554/10000 (16%)\ttrain_Loss: 0.005225\tval_Loss: 0.005958\n",
      "Train Epoch: 1555/10000 (16%)\ttrain_Loss: 0.005225\tval_Loss: 0.005958\n",
      "Train Epoch: 1556/10000 (16%)\ttrain_Loss: 0.005225\tval_Loss: 0.005957\n",
      "Train Epoch: 1557/10000 (16%)\ttrain_Loss: 0.005225\tval_Loss: 0.005957\n",
      "Train Epoch: 1558/10000 (16%)\ttrain_Loss: 0.005225\tval_Loss: 0.005957\n",
      "Train Epoch: 1559/10000 (16%)\ttrain_Loss: 0.005225\tval_Loss: 0.005957\n",
      "Train Epoch: 1560/10000 (16%)\ttrain_Loss: 0.005225\tval_Loss: 0.005957\n",
      "Train Epoch: 1561/10000 (16%)\ttrain_Loss: 0.005225\tval_Loss: 0.005957\n",
      "Train Epoch: 1562/10000 (16%)\ttrain_Loss: 0.005225\tval_Loss: 0.005956\n",
      "Train Epoch: 1563/10000 (16%)\ttrain_Loss: 0.005225\tval_Loss: 0.005956\n",
      "Train Epoch: 1564/10000 (16%)\ttrain_Loss: 0.005225\tval_Loss: 0.005956\n",
      "Train Epoch: 1565/10000 (16%)\ttrain_Loss: 0.005224\tval_Loss: 0.005956\n",
      "Train Epoch: 1566/10000 (16%)\ttrain_Loss: 0.005224\tval_Loss: 0.005956\n",
      "Train Epoch: 1567/10000 (16%)\ttrain_Loss: 0.005224\tval_Loss: 0.005956\n",
      "Train Epoch: 1568/10000 (16%)\ttrain_Loss: 0.005224\tval_Loss: 0.005956\n",
      "Train Epoch: 1569/10000 (16%)\ttrain_Loss: 0.005224\tval_Loss: 0.005955\n",
      "Train Epoch: 1570/10000 (16%)\ttrain_Loss: 0.005224\tval_Loss: 0.005955\n",
      "Train Epoch: 1571/10000 (16%)\ttrain_Loss: 0.005224\tval_Loss: 0.005955\n",
      "Train Epoch: 1572/10000 (16%)\ttrain_Loss: 0.005224\tval_Loss: 0.005955\n",
      "Train Epoch: 1573/10000 (16%)\ttrain_Loss: 0.005224\tval_Loss: 0.005955\n",
      "Train Epoch: 1574/10000 (16%)\ttrain_Loss: 0.005224\tval_Loss: 0.005955\n",
      "Train Epoch: 1575/10000 (16%)\ttrain_Loss: 0.005224\tval_Loss: 0.005954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1576/10000 (16%)\ttrain_Loss: 0.005223\tval_Loss: 0.005954\n",
      "Train Epoch: 1577/10000 (16%)\ttrain_Loss: 0.005223\tval_Loss: 0.005954\n",
      "Train Epoch: 1578/10000 (16%)\ttrain_Loss: 0.005223\tval_Loss: 0.005954\n",
      "Train Epoch: 1579/10000 (16%)\ttrain_Loss: 0.005223\tval_Loss: 0.005954\n",
      "Train Epoch: 1580/10000 (16%)\ttrain_Loss: 0.005223\tval_Loss: 0.005954\n",
      "Train Epoch: 1581/10000 (16%)\ttrain_Loss: 0.005223\tval_Loss: 0.005954\n",
      "Train Epoch: 1582/10000 (16%)\ttrain_Loss: 0.005223\tval_Loss: 0.005953\n",
      "Train Epoch: 1583/10000 (16%)\ttrain_Loss: 0.005223\tval_Loss: 0.005953\n",
      "Train Epoch: 1584/10000 (16%)\ttrain_Loss: 0.005223\tval_Loss: 0.005953\n",
      "Train Epoch: 1585/10000 (16%)\ttrain_Loss: 0.005223\tval_Loss: 0.005953\n",
      "Train Epoch: 1586/10000 (16%)\ttrain_Loss: 0.005223\tval_Loss: 0.005953\n",
      "Train Epoch: 1587/10000 (16%)\ttrain_Loss: 0.005222\tval_Loss: 0.005953\n",
      "Train Epoch: 1588/10000 (16%)\ttrain_Loss: 0.005222\tval_Loss: 0.005953\n",
      "Train Epoch: 1589/10000 (16%)\ttrain_Loss: 0.005222\tval_Loss: 0.005952\n",
      "Train Epoch: 1590/10000 (16%)\ttrain_Loss: 0.005222\tval_Loss: 0.005952\n",
      "Train Epoch: 1591/10000 (16%)\ttrain_Loss: 0.005222\tval_Loss: 0.005952\n",
      "Train Epoch: 1592/10000 (16%)\ttrain_Loss: 0.005222\tval_Loss: 0.005952\n",
      "Train Epoch: 1593/10000 (16%)\ttrain_Loss: 0.005222\tval_Loss: 0.005951\n",
      "Train Epoch: 1594/10000 (16%)\ttrain_Loss: 0.005222\tval_Loss: 0.005951\n",
      "Train Epoch: 1595/10000 (16%)\ttrain_Loss: 0.005222\tval_Loss: 0.005951\n",
      "Train Epoch: 1596/10000 (16%)\ttrain_Loss: 0.005222\tval_Loss: 0.005951\n",
      "Train Epoch: 1597/10000 (16%)\ttrain_Loss: 0.005222\tval_Loss: 0.005951\n",
      "Train Epoch: 1598/10000 (16%)\ttrain_Loss: 0.005222\tval_Loss: 0.005951\n",
      "Train Epoch: 1599/10000 (16%)\ttrain_Loss: 0.005221\tval_Loss: 0.005951\n",
      "Train Epoch: 1600/10000 (16%)\ttrain_Loss: 0.005221\tval_Loss: 0.005950\n",
      "Train Epoch: 1601/10000 (16%)\ttrain_Loss: 0.005221\tval_Loss: 0.005950\n",
      "Train Epoch: 1602/10000 (16%)\ttrain_Loss: 0.005221\tval_Loss: 0.005950\n",
      "Train Epoch: 1603/10000 (16%)\ttrain_Loss: 0.005221\tval_Loss: 0.005950\n",
      "Train Epoch: 1604/10000 (16%)\ttrain_Loss: 0.005221\tval_Loss: 0.005950\n",
      "Train Epoch: 1605/10000 (16%)\ttrain_Loss: 0.005221\tval_Loss: 0.005950\n",
      "Train Epoch: 1606/10000 (16%)\ttrain_Loss: 0.005221\tval_Loss: 0.005950\n",
      "Train Epoch: 1607/10000 (16%)\ttrain_Loss: 0.005221\tval_Loss: 0.005950\n",
      "Train Epoch: 1608/10000 (16%)\ttrain_Loss: 0.005221\tval_Loss: 0.005950\n",
      "Train Epoch: 1609/10000 (16%)\ttrain_Loss: 0.005221\tval_Loss: 0.005950\n",
      "Train Epoch: 1610/10000 (16%)\ttrain_Loss: 0.005221\tval_Loss: 0.005950\n",
      "Train Epoch: 1611/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005949\n",
      "Train Epoch: 1612/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005949\n",
      "Train Epoch: 1613/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005949\n",
      "Train Epoch: 1614/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005949\n",
      "Train Epoch: 1615/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005949\n",
      "Train Epoch: 1616/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005948\n",
      "Train Epoch: 1617/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005948\n",
      "Train Epoch: 1618/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005948\n",
      "Train Epoch: 1619/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005948\n",
      "Train Epoch: 1620/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005948\n",
      "Train Epoch: 1621/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005948\n",
      "Train Epoch: 1622/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005948\n",
      "Train Epoch: 1623/10000 (16%)\ttrain_Loss: 0.005220\tval_Loss: 0.005948\n",
      "Train Epoch: 1624/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005948\n",
      "Train Epoch: 1625/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005948\n",
      "Train Epoch: 1626/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005948\n",
      "Train Epoch: 1627/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005948\n",
      "Train Epoch: 1628/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005948\n",
      "Train Epoch: 1629/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005947\n",
      "Train Epoch: 1630/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005947\n",
      "Train Epoch: 1631/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005947\n",
      "Train Epoch: 1632/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005947\n",
      "Train Epoch: 1633/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005947\n",
      "Train Epoch: 1634/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005947\n",
      "Train Epoch: 1635/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005947\n",
      "Train Epoch: 1636/10000 (16%)\ttrain_Loss: 0.005219\tval_Loss: 0.005947\n",
      "Train Epoch: 1637/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005947\n",
      "Train Epoch: 1638/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005946\n",
      "Train Epoch: 1639/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005946\n",
      "Train Epoch: 1640/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005946\n",
      "Train Epoch: 1641/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005946\n",
      "Train Epoch: 1642/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005946\n",
      "Train Epoch: 1643/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005946\n",
      "Train Epoch: 1644/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005946\n",
      "Train Epoch: 1645/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005945\n",
      "Train Epoch: 1646/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005945\n",
      "Train Epoch: 1647/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005945\n",
      "Train Epoch: 1648/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005945\n",
      "Train Epoch: 1649/10000 (16%)\ttrain_Loss: 0.005218\tval_Loss: 0.005945\n",
      "Train Epoch: 1650/10000 (16%)\ttrain_Loss: 0.005217\tval_Loss: 0.005945\n",
      "Train Epoch: 1651/10000 (16%)\ttrain_Loss: 0.005217\tval_Loss: 0.005944\n",
      "Train Epoch: 1652/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005944\n",
      "Train Epoch: 1653/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005944\n",
      "Train Epoch: 1654/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005944\n",
      "Train Epoch: 1655/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005944\n",
      "Train Epoch: 1656/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005944\n",
      "Train Epoch: 1657/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005944\n",
      "Train Epoch: 1658/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005944\n",
      "Train Epoch: 1659/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005943\n",
      "Train Epoch: 1660/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005943\n",
      "Train Epoch: 1661/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005943\n",
      "Train Epoch: 1662/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005943\n",
      "Train Epoch: 1663/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005943\n",
      "Train Epoch: 1664/10000 (17%)\ttrain_Loss: 0.005217\tval_Loss: 0.005943\n",
      "Train Epoch: 1665/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005943\n",
      "Train Epoch: 1666/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005943\n",
      "Train Epoch: 1667/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005943\n",
      "Train Epoch: 1668/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005942\n",
      "Train Epoch: 1669/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005942\n",
      "Train Epoch: 1670/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005942\n",
      "Train Epoch: 1671/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005942\n",
      "Train Epoch: 1672/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005941\n",
      "Train Epoch: 1673/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005941\n",
      "Train Epoch: 1674/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005941\n",
      "Train Epoch: 1675/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005941\n",
      "Train Epoch: 1676/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005941\n",
      "Train Epoch: 1677/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005941\n",
      "Train Epoch: 1678/10000 (17%)\ttrain_Loss: 0.005216\tval_Loss: 0.005941\n",
      "Train Epoch: 1679/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005941\n",
      "Train Epoch: 1680/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005941\n",
      "Train Epoch: 1681/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005941\n",
      "Train Epoch: 1682/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005941\n",
      "Train Epoch: 1683/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005941\n",
      "Train Epoch: 1684/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005941\n",
      "Train Epoch: 1685/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005941\n",
      "Train Epoch: 1686/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005940\n",
      "Train Epoch: 1687/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005940\n",
      "Train Epoch: 1688/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005940\n",
      "Train Epoch: 1689/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005940\n",
      "Train Epoch: 1690/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005940\n",
      "Train Epoch: 1691/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005939\n",
      "Train Epoch: 1692/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005939\n",
      "Train Epoch: 1693/10000 (17%)\ttrain_Loss: 0.005215\tval_Loss: 0.005939\n",
      "Train Epoch: 1694/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005939\n",
      "Train Epoch: 1695/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005939\n",
      "Train Epoch: 1696/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005939\n",
      "Train Epoch: 1697/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005939\n",
      "Train Epoch: 1698/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005939\n",
      "Train Epoch: 1699/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005939\n",
      "Train Epoch: 1700/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005939\n",
      "Train Epoch: 1701/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005939\n",
      "Train Epoch: 1702/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005938\n",
      "Train Epoch: 1703/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005938\n",
      "Train Epoch: 1704/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005938\n",
      "Train Epoch: 1705/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005938\n",
      "Train Epoch: 1706/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1707/10000 (17%)\ttrain_Loss: 0.005214\tval_Loss: 0.005938\n",
      "Train Epoch: 1708/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005937\n",
      "Train Epoch: 1709/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005937\n",
      "Train Epoch: 1710/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005937\n",
      "Train Epoch: 1711/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005937\n",
      "Train Epoch: 1712/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005937\n",
      "Train Epoch: 1713/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005937\n",
      "Train Epoch: 1714/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005937\n",
      "Train Epoch: 1715/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005937\n",
      "Train Epoch: 1716/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005937\n",
      "Train Epoch: 1717/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005938\n",
      "Train Epoch: 1718/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005938\n",
      "Train Epoch: 1719/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005938\n",
      "Train Epoch: 1720/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005939\n",
      "Train Epoch: 1721/10000 (17%)\ttrain_Loss: 0.005213\tval_Loss: 0.005939\n",
      "Train Epoch: 1722/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005939\n",
      "Train Epoch: 1723/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005939\n",
      "Train Epoch: 1724/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005938\n",
      "Train Epoch: 1725/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005938\n",
      "Train Epoch: 1726/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005937\n",
      "Train Epoch: 1727/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005937\n",
      "Train Epoch: 1728/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005936\n",
      "Train Epoch: 1729/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005936\n",
      "Train Epoch: 1730/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005936\n",
      "Train Epoch: 1731/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005936\n",
      "Train Epoch: 1732/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005936\n",
      "Train Epoch: 1733/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005936\n",
      "Train Epoch: 1734/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005937\n",
      "Train Epoch: 1735/10000 (17%)\ttrain_Loss: 0.005212\tval_Loss: 0.005937\n",
      "Train Epoch: 1736/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005937\n",
      "Train Epoch: 1737/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005936\n",
      "Train Epoch: 1738/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005936\n",
      "Train Epoch: 1739/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005936\n",
      "Train Epoch: 1740/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005936\n",
      "Train Epoch: 1741/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005936\n",
      "Train Epoch: 1742/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005935\n",
      "Train Epoch: 1743/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005935\n",
      "Train Epoch: 1744/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005935\n",
      "Train Epoch: 1745/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005935\n",
      "Train Epoch: 1746/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005935\n",
      "Train Epoch: 1747/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005935\n",
      "Train Epoch: 1748/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005935\n",
      "Train Epoch: 1749/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005935\n",
      "Train Epoch: 1750/10000 (17%)\ttrain_Loss: 0.005211\tval_Loss: 0.005935\n",
      "Train Epoch: 1751/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005935\n",
      "Train Epoch: 1752/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005935\n",
      "Train Epoch: 1753/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005935\n",
      "Train Epoch: 1754/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005934\n",
      "Train Epoch: 1755/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005934\n",
      "Train Epoch: 1756/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005934\n",
      "Train Epoch: 1757/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005934\n",
      "Train Epoch: 1758/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005934\n",
      "Train Epoch: 1759/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005934\n",
      "Train Epoch: 1760/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005934\n",
      "Train Epoch: 1761/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005934\n",
      "Train Epoch: 1762/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005934\n",
      "Train Epoch: 1763/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005934\n",
      "Train Epoch: 1764/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005934\n",
      "Train Epoch: 1765/10000 (18%)\ttrain_Loss: 0.005210\tval_Loss: 0.005934\n",
      "Train Epoch: 1766/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005933\n",
      "Train Epoch: 1767/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005933\n",
      "Train Epoch: 1768/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005933\n",
      "Train Epoch: 1769/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005933\n",
      "Train Epoch: 1770/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005933\n",
      "Train Epoch: 1771/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005933\n",
      "Train Epoch: 1772/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005932\n",
      "Train Epoch: 1773/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005932\n",
      "Train Epoch: 1774/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005932\n",
      "Train Epoch: 1775/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005932\n",
      "Train Epoch: 1776/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005932\n",
      "Train Epoch: 1777/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005932\n",
      "Train Epoch: 1778/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005932\n",
      "Train Epoch: 1779/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005932\n",
      "Train Epoch: 1780/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005932\n",
      "Train Epoch: 1781/10000 (18%)\ttrain_Loss: 0.005209\tval_Loss: 0.005932\n",
      "Train Epoch: 1782/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005932\n",
      "Train Epoch: 1783/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005932\n",
      "Train Epoch: 1784/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005932\n",
      "Train Epoch: 1785/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005931\n",
      "Train Epoch: 1786/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005931\n",
      "Train Epoch: 1787/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005931\n",
      "Train Epoch: 1788/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005931\n",
      "Train Epoch: 1789/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005931\n",
      "Train Epoch: 1790/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005931\n",
      "Train Epoch: 1791/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005931\n",
      "Train Epoch: 1792/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005931\n",
      "Train Epoch: 1793/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005931\n",
      "Train Epoch: 1794/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005931\n",
      "Train Epoch: 1795/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005931\n",
      "Train Epoch: 1796/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005930\n",
      "Train Epoch: 1797/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005930\n",
      "Train Epoch: 1798/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005930\n",
      "Train Epoch: 1799/10000 (18%)\ttrain_Loss: 0.005208\tval_Loss: 0.005930\n",
      "Train Epoch: 1800/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005930\n",
      "Train Epoch: 1801/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005930\n",
      "Train Epoch: 1802/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005930\n",
      "Train Epoch: 1803/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005930\n",
      "Train Epoch: 1804/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005930\n",
      "Train Epoch: 1805/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005930\n",
      "Train Epoch: 1806/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005929\n",
      "Train Epoch: 1807/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005929\n",
      "Train Epoch: 1808/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005929\n",
      "Train Epoch: 1809/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005929\n",
      "Train Epoch: 1810/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005929\n",
      "Train Epoch: 1811/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005929\n",
      "Train Epoch: 1812/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005929\n",
      "Train Epoch: 1813/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005929\n",
      "Train Epoch: 1814/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005929\n",
      "Train Epoch: 1815/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005928\n",
      "Train Epoch: 1816/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005928\n",
      "Train Epoch: 1817/10000 (18%)\ttrain_Loss: 0.005207\tval_Loss: 0.005928\n",
      "Train Epoch: 1818/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005928\n",
      "Train Epoch: 1819/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005928\n",
      "Train Epoch: 1820/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005928\n",
      "Train Epoch: 1821/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005928\n",
      "Train Epoch: 1822/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005928\n",
      "Train Epoch: 1823/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005928\n",
      "Train Epoch: 1824/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005928\n",
      "Train Epoch: 1825/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005928\n",
      "Train Epoch: 1826/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005928\n",
      "Train Epoch: 1827/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005927\n",
      "Train Epoch: 1828/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005927\n",
      "Train Epoch: 1829/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005927\n",
      "Train Epoch: 1830/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005927\n",
      "Train Epoch: 1831/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1832/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005927\n",
      "Train Epoch: 1833/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005927\n",
      "Train Epoch: 1834/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005927\n",
      "Train Epoch: 1835/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005927\n",
      "Train Epoch: 1836/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005927\n",
      "Train Epoch: 1837/10000 (18%)\ttrain_Loss: 0.005206\tval_Loss: 0.005926\n",
      "Train Epoch: 1838/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005926\n",
      "Train Epoch: 1839/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005926\n",
      "Train Epoch: 1840/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005926\n",
      "Train Epoch: 1841/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005926\n",
      "Train Epoch: 1842/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005926\n",
      "Train Epoch: 1843/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005926\n",
      "Train Epoch: 1844/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005926\n",
      "Train Epoch: 1845/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005926\n",
      "Train Epoch: 1846/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005926\n",
      "Train Epoch: 1847/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005926\n",
      "Train Epoch: 1848/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005926\n",
      "Train Epoch: 1849/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005926\n",
      "Train Epoch: 1850/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005925\n",
      "Train Epoch: 1851/10000 (18%)\ttrain_Loss: 0.005205\tval_Loss: 0.005925\n",
      "Train Epoch: 1852/10000 (19%)\ttrain_Loss: 0.005205\tval_Loss: 0.005925\n",
      "Train Epoch: 1853/10000 (19%)\ttrain_Loss: 0.005205\tval_Loss: 0.005925\n",
      "Train Epoch: 1854/10000 (19%)\ttrain_Loss: 0.005205\tval_Loss: 0.005925\n",
      "Train Epoch: 1855/10000 (19%)\ttrain_Loss: 0.005205\tval_Loss: 0.005925\n",
      "Train Epoch: 1856/10000 (19%)\ttrain_Loss: 0.005205\tval_Loss: 0.005925\n",
      "Train Epoch: 1857/10000 (19%)\ttrain_Loss: 0.005205\tval_Loss: 0.005925\n",
      "Train Epoch: 1858/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005925\n",
      "Train Epoch: 1859/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005925\n",
      "Train Epoch: 1860/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005925\n",
      "Train Epoch: 1861/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005925\n",
      "Train Epoch: 1862/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1863/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1864/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1865/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1866/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1867/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1868/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1869/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1870/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1871/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1872/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1873/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1874/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1875/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005924\n",
      "Train Epoch: 1876/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005923\n",
      "Train Epoch: 1877/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005923\n",
      "Train Epoch: 1878/10000 (19%)\ttrain_Loss: 0.005204\tval_Loss: 0.005923\n",
      "Train Epoch: 1879/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005923\n",
      "Train Epoch: 1880/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005923\n",
      "Train Epoch: 1881/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005923\n",
      "Train Epoch: 1882/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005923\n",
      "Train Epoch: 1883/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005923\n",
      "Train Epoch: 1884/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005923\n",
      "Train Epoch: 1885/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005923\n",
      "Train Epoch: 1886/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005923\n",
      "Train Epoch: 1887/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005923\n",
      "Train Epoch: 1888/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005923\n",
      "Train Epoch: 1889/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005923\n",
      "Train Epoch: 1890/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005922\n",
      "Train Epoch: 1891/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005922\n",
      "Train Epoch: 1892/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005922\n",
      "Train Epoch: 1893/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005922\n",
      "Train Epoch: 1894/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005922\n",
      "Train Epoch: 1895/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005922\n",
      "Train Epoch: 1896/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005922\n",
      "Train Epoch: 1897/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005922\n",
      "Train Epoch: 1898/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005922\n",
      "Train Epoch: 1899/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005922\n",
      "Train Epoch: 1900/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005922\n",
      "Train Epoch: 1901/10000 (19%)\ttrain_Loss: 0.005203\tval_Loss: 0.005922\n",
      "Train Epoch: 1902/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005922\n",
      "Train Epoch: 1903/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005922\n",
      "Train Epoch: 1904/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1905/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1906/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1907/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1908/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1909/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1910/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1911/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1912/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1913/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1914/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1915/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1916/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1917/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1918/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005921\n",
      "Train Epoch: 1919/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005920\n",
      "Train Epoch: 1920/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005920\n",
      "Train Epoch: 1921/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005920\n",
      "Train Epoch: 1922/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005920\n",
      "Train Epoch: 1923/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005920\n",
      "Train Epoch: 1924/10000 (19%)\ttrain_Loss: 0.005202\tval_Loss: 0.005920\n",
      "Train Epoch: 1925/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005920\n",
      "Train Epoch: 1926/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005920\n",
      "Train Epoch: 1927/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005920\n",
      "Train Epoch: 1928/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005920\n",
      "Train Epoch: 1929/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005920\n",
      "Train Epoch: 1930/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005920\n",
      "Train Epoch: 1931/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005920\n",
      "Train Epoch: 1932/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005920\n",
      "Train Epoch: 1933/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1934/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1935/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1936/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1937/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1938/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1939/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1940/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1941/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1942/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1943/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1944/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1945/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1946/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1947/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1948/10000 (19%)\ttrain_Loss: 0.005201\tval_Loss: 0.005919\n",
      "Train Epoch: 1949/10000 (19%)\ttrain_Loss: 0.005200\tval_Loss: 0.005919\n",
      "Train Epoch: 1950/10000 (19%)\ttrain_Loss: 0.005200\tval_Loss: 0.005919\n",
      "Train Epoch: 1951/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005919\n",
      "Train Epoch: 1952/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1953/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1954/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1955/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1956/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1957/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1958/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1959/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1960/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1961/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1962/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1963/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1964/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1965/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1966/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1967/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005918\n",
      "Train Epoch: 1968/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005917\n",
      "Train Epoch: 1969/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005917\n",
      "Train Epoch: 1970/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005917\n",
      "Train Epoch: 1971/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005917\n",
      "Train Epoch: 1972/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005917\n",
      "Train Epoch: 1973/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005917\n",
      "Train Epoch: 1974/10000 (20%)\ttrain_Loss: 0.005200\tval_Loss: 0.005917\n",
      "Train Epoch: 1975/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005917\n",
      "Train Epoch: 1976/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005917\n",
      "Train Epoch: 1977/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005917\n",
      "Train Epoch: 1978/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005917\n",
      "Train Epoch: 1979/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005916\n",
      "Train Epoch: 1980/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005916\n",
      "Train Epoch: 1981/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005916\n",
      "Train Epoch: 1982/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005916\n",
      "Train Epoch: 1983/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005915\n",
      "Train Epoch: 1984/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005915\n",
      "Train Epoch: 1985/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005915\n",
      "Train Epoch: 1986/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005916\n",
      "Train Epoch: 1987/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005916\n",
      "Train Epoch: 1988/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005916\n",
      "Train Epoch: 1989/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005917\n",
      "Train Epoch: 1990/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005917\n",
      "Train Epoch: 1991/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005917\n",
      "Train Epoch: 1992/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005917\n",
      "Train Epoch: 1993/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005917\n",
      "Train Epoch: 1994/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005916\n",
      "Train Epoch: 1995/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005916\n",
      "Train Epoch: 1996/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005916\n",
      "Train Epoch: 1997/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005915\n",
      "Train Epoch: 1998/10000 (20%)\ttrain_Loss: 0.005199\tval_Loss: 0.005915\n",
      "Train Epoch: 1999/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2000/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2001/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2002/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2003/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005916\n",
      "Train Epoch: 2004/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005916\n",
      "Train Epoch: 2005/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005916\n",
      "Train Epoch: 2006/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005916\n",
      "Train Epoch: 2007/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2008/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2009/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2010/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2011/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2012/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2013/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2014/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2015/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2016/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2017/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2018/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2019/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005915\n",
      "Train Epoch: 2020/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005914\n",
      "Train Epoch: 2021/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005914\n",
      "Train Epoch: 2022/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005914\n",
      "Train Epoch: 2023/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005914\n",
      "Train Epoch: 2024/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005914\n",
      "Train Epoch: 2025/10000 (20%)\ttrain_Loss: 0.005198\tval_Loss: 0.005914\n",
      "Train Epoch: 2026/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005914\n",
      "Train Epoch: 2027/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005914\n",
      "Train Epoch: 2028/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005914\n",
      "Train Epoch: 2029/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005914\n",
      "Train Epoch: 2030/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005914\n",
      "Train Epoch: 2031/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005914\n",
      "Train Epoch: 2032/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005914\n",
      "Train Epoch: 2033/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005914\n",
      "Train Epoch: 2034/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2035/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2036/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2037/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2038/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2039/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2040/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2041/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2042/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2043/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2044/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2045/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2046/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2047/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2048/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005913\n",
      "Train Epoch: 2049/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005912\n",
      "Train Epoch: 2050/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005912\n",
      "Train Epoch: 2051/10000 (20%)\ttrain_Loss: 0.005197\tval_Loss: 0.005912\n",
      "Train Epoch: 2052/10000 (21%)\ttrain_Loss: 0.005197\tval_Loss: 0.005912\n",
      "Train Epoch: 2053/10000 (21%)\ttrain_Loss: 0.005197\tval_Loss: 0.005912\n",
      "Train Epoch: 2054/10000 (21%)\ttrain_Loss: 0.005197\tval_Loss: 0.005912\n",
      "Train Epoch: 2055/10000 (21%)\ttrain_Loss: 0.005197\tval_Loss: 0.005912\n",
      "Train Epoch: 2056/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005912\n",
      "Train Epoch: 2057/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2058/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2059/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2060/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2061/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2062/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2063/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2064/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2065/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2066/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2067/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2068/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2069/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2070/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2071/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2072/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2073/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2074/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005911\n",
      "Train Epoch: 2075/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2076/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2077/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2078/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2079/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2080/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2081/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2082/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2083/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2084/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2085/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2086/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2087/10000 (21%)\ttrain_Loss: 0.005196\tval_Loss: 0.005910\n",
      "Train Epoch: 2088/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005910\n",
      "Train Epoch: 2089/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005910\n",
      "Train Epoch: 2090/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005910\n",
      "Train Epoch: 2091/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005910\n",
      "Train Epoch: 2092/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005910\n",
      "Train Epoch: 2093/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2094/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2095/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2096/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2097/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2098/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2099/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2100/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2101/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2102/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2103/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2104/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2105/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2106/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2107/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2108/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2109/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2110/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005909\n",
      "Train Epoch: 2111/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005908\n",
      "Train Epoch: 2112/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005908\n",
      "Train Epoch: 2113/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005908\n",
      "Train Epoch: 2114/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005908\n",
      "Train Epoch: 2115/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005908\n",
      "Train Epoch: 2116/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005908\n",
      "Train Epoch: 2117/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005908\n",
      "Train Epoch: 2118/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005908\n",
      "Train Epoch: 2119/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005908\n",
      "Train Epoch: 2120/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005908\n",
      "Train Epoch: 2121/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005908\n",
      "Train Epoch: 2122/10000 (21%)\ttrain_Loss: 0.005195\tval_Loss: 0.005908\n",
      "Train Epoch: 2123/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005908\n",
      "Train Epoch: 2124/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005908\n",
      "Train Epoch: 2125/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005908\n",
      "Train Epoch: 2126/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005908\n",
      "Train Epoch: 2127/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005908\n",
      "Train Epoch: 2128/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005908\n",
      "Train Epoch: 2129/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005908\n",
      "Train Epoch: 2130/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2131/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2132/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2133/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2134/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2135/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2136/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2137/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2138/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2139/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2140/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2141/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2142/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2143/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2144/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2145/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2146/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2147/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2148/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005907\n",
      "Train Epoch: 2149/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005906\n",
      "Train Epoch: 2150/10000 (21%)\ttrain_Loss: 0.005194\tval_Loss: 0.005906\n",
      "Train Epoch: 2151/10000 (22%)\ttrain_Loss: 0.005194\tval_Loss: 0.005906\n",
      "Train Epoch: 2152/10000 (22%)\ttrain_Loss: 0.005194\tval_Loss: 0.005906\n",
      "Train Epoch: 2153/10000 (22%)\ttrain_Loss: 0.005194\tval_Loss: 0.005906\n",
      "Train Epoch: 2154/10000 (22%)\ttrain_Loss: 0.005194\tval_Loss: 0.005906\n",
      "Train Epoch: 2155/10000 (22%)\ttrain_Loss: 0.005194\tval_Loss: 0.005906\n",
      "Train Epoch: 2156/10000 (22%)\ttrain_Loss: 0.005194\tval_Loss: 0.005906\n",
      "Train Epoch: 2157/10000 (22%)\ttrain_Loss: 0.005194\tval_Loss: 0.005906\n",
      "Train Epoch: 2158/10000 (22%)\ttrain_Loss: 0.005194\tval_Loss: 0.005906\n",
      "Train Epoch: 2159/10000 (22%)\ttrain_Loss: 0.005194\tval_Loss: 0.005906\n",
      "Train Epoch: 2160/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005906\n",
      "Train Epoch: 2161/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005906\n",
      "Train Epoch: 2162/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005906\n",
      "Train Epoch: 2163/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005906\n",
      "Train Epoch: 2164/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005906\n",
      "Train Epoch: 2165/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005906\n",
      "Train Epoch: 2166/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005906\n",
      "Train Epoch: 2167/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2168/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2169/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2170/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2171/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2172/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2173/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2174/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2175/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2176/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2177/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2178/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2179/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2180/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2181/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2182/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2183/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2184/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2185/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2186/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2187/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005905\n",
      "Train Epoch: 2188/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2189/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2190/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2191/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2192/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2193/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2194/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2195/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2196/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2197/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2198/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2199/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2200/10000 (22%)\ttrain_Loss: 0.005193\tval_Loss: 0.005904\n",
      "Train Epoch: 2201/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005904\n",
      "Train Epoch: 2202/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005904\n",
      "Train Epoch: 2203/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005904\n",
      "Train Epoch: 2204/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005904\n",
      "Train Epoch: 2205/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005904\n",
      "Train Epoch: 2206/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005904\n",
      "Train Epoch: 2207/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005904\n",
      "Train Epoch: 2208/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005904\n",
      "Train Epoch: 2209/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005904\n",
      "Train Epoch: 2210/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2211/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2212/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2213/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2214/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2215/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2216/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2217/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2218/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2219/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2220/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2221/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2222/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2223/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2224/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2225/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2226/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2227/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2228/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2229/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2230/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2231/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n",
      "Train Epoch: 2232/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2233/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2234/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2235/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2236/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2237/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2238/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2239/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2240/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2241/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2242/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2243/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2244/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2245/10000 (22%)\ttrain_Loss: 0.005192\tval_Loss: 0.005902\n",
      "Train Epoch: 2246/10000 (22%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2247/10000 (22%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2248/10000 (22%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2249/10000 (22%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2250/10000 (22%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2251/10000 (22%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2252/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2253/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2254/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2255/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2256/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2257/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2258/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2259/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2260/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2261/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2262/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2263/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2264/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005902\n",
      "Train Epoch: 2265/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2266/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2267/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2268/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2269/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2270/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2271/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2272/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2273/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2274/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2275/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2276/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2277/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2278/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2279/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005900\n",
      "Train Epoch: 2280/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005900\n",
      "Train Epoch: 2281/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005900\n",
      "Train Epoch: 2282/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2283/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2284/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2285/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2286/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2287/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2288/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005901\n",
      "Train Epoch: 2289/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005900\n",
      "Train Epoch: 2290/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005900\n",
      "Train Epoch: 2291/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005900\n",
      "Train Epoch: 2292/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005900\n",
      "Train Epoch: 2293/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005900\n",
      "Train Epoch: 2294/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005900\n",
      "Train Epoch: 2295/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005900\n",
      "Train Epoch: 2296/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005900\n",
      "Train Epoch: 2297/10000 (23%)\ttrain_Loss: 0.005191\tval_Loss: 0.005900\n",
      "Train Epoch: 2298/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2299/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2300/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2301/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2302/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2303/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2304/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2305/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2306/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2307/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2308/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2309/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2310/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2311/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2312/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2313/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2314/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2315/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005900\n",
      "Train Epoch: 2316/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2317/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2318/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2319/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2320/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2321/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2322/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2323/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2324/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2325/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2326/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2327/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2328/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2329/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2330/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2331/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2332/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2333/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2334/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2335/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2336/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2337/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2338/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2339/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2340/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2341/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2342/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2343/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2344/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2345/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2346/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2347/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2348/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2349/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2350/10000 (23%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2351/10000 (24%)\ttrain_Loss: 0.005190\tval_Loss: 0.005898\n",
      "Train Epoch: 2352/10000 (24%)\ttrain_Loss: 0.005190\tval_Loss: 0.005898\n",
      "Train Epoch: 2353/10000 (24%)\ttrain_Loss: 0.005190\tval_Loss: 0.005898\n",
      "Train Epoch: 2354/10000 (24%)\ttrain_Loss: 0.005190\tval_Loss: 0.005899\n",
      "Train Epoch: 2355/10000 (24%)\ttrain_Loss: 0.005190\tval_Loss: 0.005898\n",
      "Train Epoch: 2356/10000 (24%)\ttrain_Loss: 0.005190\tval_Loss: 0.005898\n",
      "Train Epoch: 2357/10000 (24%)\ttrain_Loss: 0.005190\tval_Loss: 0.005898\n",
      "Train Epoch: 2358/10000 (24%)\ttrain_Loss: 0.005190\tval_Loss: 0.005898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2359/10000 (24%)\ttrain_Loss: 0.005190\tval_Loss: 0.005898\n",
      "Train Epoch: 2360/10000 (24%)\ttrain_Loss: 0.005190\tval_Loss: 0.005898\n",
      "Train Epoch: 2361/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2362/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2363/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2364/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2365/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2366/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2367/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2368/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2369/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2370/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2371/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2372/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2373/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2374/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2375/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2376/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2377/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2378/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2379/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2380/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2381/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2382/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2383/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2384/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2385/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2386/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2387/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2388/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2389/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2390/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2391/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2392/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2393/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2394/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2395/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2396/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005898\n",
      "Train Epoch: 2397/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2398/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2399/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2400/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2401/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2402/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2403/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2404/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2405/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2406/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2407/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2408/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2409/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2410/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2411/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2412/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2413/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2414/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2415/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2416/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2417/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2418/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2419/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2420/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2421/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2422/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2423/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2424/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2425/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2426/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005897\n",
      "Train Epoch: 2427/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2428/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2429/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2430/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2431/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2432/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2433/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2434/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2435/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2436/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2437/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2438/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2439/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2440/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2441/10000 (24%)\ttrain_Loss: 0.005189\tval_Loss: 0.005896\n",
      "Train Epoch: 2442/10000 (24%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2443/10000 (24%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2444/10000 (24%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2445/10000 (24%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2446/10000 (24%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2447/10000 (24%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2448/10000 (24%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2449/10000 (24%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2450/10000 (24%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2451/10000 (24%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2452/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2453/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2454/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2455/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2456/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2457/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2458/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2459/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2460/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2461/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2462/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2463/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2464/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2465/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2466/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2467/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2468/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2469/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2470/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2471/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2472/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2473/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005896\n",
      "Train Epoch: 2474/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2475/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2476/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2477/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2478/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2479/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2480/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2481/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2482/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2483/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2484/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2485/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2486/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2487/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2488/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2489/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2490/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2491/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2492/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2493/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2494/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2495/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005895\n",
      "Train Epoch: 2496/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2497/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2498/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2499/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2500/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2501/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2502/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2503/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2504/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2505/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2506/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2507/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2508/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2509/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2510/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2511/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2512/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2513/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2514/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2515/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2516/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2517/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2518/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2519/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2520/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2521/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2522/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2523/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2524/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2525/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2526/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2527/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2528/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2529/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005894\n",
      "Train Epoch: 2530/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2531/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2532/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2533/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2534/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2535/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2536/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2537/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2538/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2539/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2540/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2541/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2542/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2543/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2544/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2545/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2546/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2547/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2548/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2549/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2550/10000 (25%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2551/10000 (26%)\ttrain_Loss: 0.005188\tval_Loss: 0.005893\n",
      "Train Epoch: 2552/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005893\n",
      "Train Epoch: 2553/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2554/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2555/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005893\n",
      "Train Epoch: 2556/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005893\n",
      "Train Epoch: 2557/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005893\n",
      "Train Epoch: 2558/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005893\n",
      "Train Epoch: 2559/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2560/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2561/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2562/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2563/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2564/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2565/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2566/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2567/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2568/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2569/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2570/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2571/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2572/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2573/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2574/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2575/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2576/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2577/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2578/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2579/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2580/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2581/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2582/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2583/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2584/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2585/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2586/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2587/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2588/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2589/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2590/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2591/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2592/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2593/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2594/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2595/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2596/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2597/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2598/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005892\n",
      "Train Epoch: 2599/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2600/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2601/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2602/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2603/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2604/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2605/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2606/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2607/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2608/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2609/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2610/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2611/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2612/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2613/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2614/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2615/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2616/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2617/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2618/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2619/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2620/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2621/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2622/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2623/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2624/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2625/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2626/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2627/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2628/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005891\n",
      "Train Epoch: 2629/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2630/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2631/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2632/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2633/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2634/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2635/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2636/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2637/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2638/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2639/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2640/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2641/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2642/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2643/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2644/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2645/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2646/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2647/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2648/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2649/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2650/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2651/10000 (26%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2652/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2653/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2654/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2655/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2656/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2657/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2658/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2659/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2660/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2661/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2662/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2663/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2664/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005890\n",
      "Train Epoch: 2665/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2666/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2667/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2668/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2669/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2670/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2671/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2672/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2673/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2674/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2675/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2676/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2677/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2678/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2679/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2680/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2681/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2682/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2683/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2684/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2685/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2686/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2687/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2688/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2689/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2690/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2691/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2692/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2693/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2694/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2695/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2696/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2697/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2698/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2699/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2700/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2701/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2702/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2703/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2704/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2705/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2706/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2707/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2708/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2709/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005889\n",
      "Train Epoch: 2710/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2711/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2712/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2713/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2714/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2715/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2716/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2717/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2718/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2719/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2720/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2721/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2722/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2723/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2724/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2725/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2726/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2727/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2728/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2729/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2730/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2731/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2732/10000 (27%)\ttrain_Loss: 0.005187\tval_Loss: 0.005888\n",
      "Train Epoch: 2733/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2734/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2735/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2736/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2737/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2738/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2739/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2740/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2741/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2742/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2743/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2744/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2745/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2746/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005888\n",
      "Train Epoch: 2747/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2748/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2749/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2750/10000 (27%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2751/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2752/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2753/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2754/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2755/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2756/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2757/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2758/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2759/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2760/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2761/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2762/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2763/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2764/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2765/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2766/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2767/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2768/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2769/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2770/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2771/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2772/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2773/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2774/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2775/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2776/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2777/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2778/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2779/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2780/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2781/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2782/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2783/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2784/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2785/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2786/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2787/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2788/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2789/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2790/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2791/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005887\n",
      "Train Epoch: 2792/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2793/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2794/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2795/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2796/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2797/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2798/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2799/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2800/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2801/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2802/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2803/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2804/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2805/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2806/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2807/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2808/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2809/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2810/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2811/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2812/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2813/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2814/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2815/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2816/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2817/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2818/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2819/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2820/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2821/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2822/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2823/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2824/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2825/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2826/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2827/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2828/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2829/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2830/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2831/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2832/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2833/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005886\n",
      "Train Epoch: 2834/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2835/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2836/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2837/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2838/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2839/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2840/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2841/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2842/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2843/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2844/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2845/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2846/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2847/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2848/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2849/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2850/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2851/10000 (28%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2852/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2853/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2854/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2855/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2856/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2857/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2858/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2859/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2860/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2861/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2862/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2863/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2864/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2865/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2866/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2867/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2868/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2869/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2870/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2871/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2872/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2873/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2874/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2875/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2876/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2877/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2878/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2879/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2880/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2881/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2882/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2883/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2884/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2885/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2886/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2887/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2888/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2889/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2890/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2891/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2892/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2893/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2894/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005885\n",
      "Train Epoch: 2895/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2896/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2897/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2898/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2899/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2900/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2901/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2902/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2903/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2904/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2905/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2906/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2907/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2908/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2909/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2910/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2911/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2912/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2913/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2914/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2915/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2916/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2917/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2918/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2919/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2920/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2921/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2922/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2923/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2924/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2925/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2926/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2927/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2928/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2929/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2930/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2931/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2932/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2933/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2934/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2935/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2936/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2937/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2938/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2939/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2940/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2941/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2942/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2943/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2944/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2945/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2946/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2947/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2948/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2949/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005884\n",
      "Train Epoch: 2950/10000 (29%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2951/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2952/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2953/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2954/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2955/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2956/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2957/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2958/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2959/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2960/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2961/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2962/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2963/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2964/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2965/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2966/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2967/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2968/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2969/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2970/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2971/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2972/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2973/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2974/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2975/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2976/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2977/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2978/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2979/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2980/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2981/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2982/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2983/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2984/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2985/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2986/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2987/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2988/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2989/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2990/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2991/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2992/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2993/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2994/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2995/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2996/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2997/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2998/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 2999/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3000/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3001/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3002/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3003/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3004/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3005/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3006/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3007/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3008/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3009/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3010/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3011/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3012/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3013/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3014/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3015/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3016/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3017/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3018/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3019/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3020/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3021/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3022/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3023/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3024/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3025/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3026/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3027/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3028/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3029/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005883\n",
      "Train Epoch: 3030/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3031/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3032/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3033/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3034/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3035/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3036/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3037/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3038/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3039/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3040/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3041/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3042/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3043/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3044/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3045/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3046/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3047/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3048/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3049/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3050/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3051/10000 (30%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3052/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3053/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3054/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3055/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3056/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3057/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3058/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3059/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3060/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3061/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3062/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3063/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3064/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3065/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3066/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3067/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3068/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3069/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3070/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3071/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3072/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3073/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3074/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3075/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3076/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3077/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3078/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3079/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3080/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3081/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3082/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3083/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3084/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3085/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3086/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3087/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3088/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3089/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3090/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3091/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3092/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3093/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3094/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3095/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3096/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3097/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3098/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3099/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3100/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3101/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3102/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3103/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3104/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3105/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3106/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3107/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005881\n",
      "Train Epoch: 3108/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005881\n",
      "Train Epoch: 3109/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005881\n",
      "Train Epoch: 3110/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3111/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3112/10000 (31%)\ttrain_Loss: 0.005186\tval_Loss: 0.005882\n",
      "Train Epoch: 3113/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005882\n",
      "Train Epoch: 3114/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005882\n",
      "Train Epoch: 3115/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005882\n",
      "Train Epoch: 3116/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3117/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3118/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3119/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3120/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3121/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005882\n",
      "Train Epoch: 3122/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005882\n",
      "Train Epoch: 3123/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005882\n",
      "Train Epoch: 3124/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005882\n",
      "Train Epoch: 3125/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3126/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3127/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3128/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3129/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3130/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005882\n",
      "Train Epoch: 3131/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005882\n",
      "Train Epoch: 3132/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3133/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3134/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3135/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3136/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3137/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3138/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3139/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3140/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3141/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3142/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3143/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3144/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3145/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3146/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3147/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3148/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3149/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3150/10000 (31%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3151/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3152/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3153/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3154/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3155/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3156/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3157/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3158/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3159/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3160/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3161/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3162/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3163/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3164/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3165/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3166/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3167/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3168/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3169/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3170/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3171/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3172/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3173/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3174/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3175/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3176/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3177/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3178/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3179/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3180/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3181/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3182/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3183/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3184/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3185/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3186/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3187/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3188/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3189/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3190/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3191/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3192/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3193/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3194/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3195/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3196/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3197/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3198/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3199/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3200/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3201/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3202/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3203/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3204/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3205/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3206/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3207/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3208/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3209/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3210/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3211/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3212/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3213/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3214/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3215/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3216/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3217/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3218/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3219/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3220/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3221/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3222/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3223/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3224/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3225/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3226/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3227/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3228/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3229/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3230/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3231/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3232/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3233/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3234/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3235/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3236/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3237/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3238/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3239/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3240/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3241/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3242/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3243/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3244/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3245/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3246/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3247/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3248/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3249/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3250/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3251/10000 (32%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3252/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3253/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3254/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3255/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3256/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3257/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3258/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3259/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3260/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3261/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3262/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3263/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3264/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3265/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3266/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3267/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3268/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3269/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3270/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3271/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3272/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3273/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3274/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3275/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3276/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3277/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3278/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3279/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3280/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3281/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3282/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3283/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3284/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3285/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3286/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3287/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3288/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3289/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3290/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3291/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3292/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3293/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3294/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3295/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3296/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3297/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3298/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3299/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3300/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3301/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3302/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3303/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3304/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3305/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3306/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3307/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3308/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3309/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3310/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3311/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3312/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3313/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3314/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3315/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3316/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3317/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3318/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3319/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3320/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3321/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3322/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3323/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3324/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3325/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3326/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3327/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3328/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3329/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3330/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3331/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3332/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3333/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3334/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3335/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3336/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3337/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3338/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3339/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3340/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3341/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3342/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3343/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3344/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3345/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3346/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3347/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3348/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3349/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3350/10000 (33%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3351/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3352/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3353/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3354/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3355/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3356/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3357/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3358/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3359/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3360/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3361/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3362/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3363/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3364/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3365/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3366/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3367/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3368/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3369/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3370/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3371/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3372/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3373/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3374/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3375/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3376/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3377/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3378/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3379/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3380/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3381/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3382/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3383/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3384/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3385/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3386/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3387/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3388/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3389/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3390/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3391/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3392/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3393/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3394/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3395/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3396/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3397/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3398/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3399/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3400/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3401/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3402/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3403/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3404/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3405/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3406/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3407/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3408/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3409/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3410/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3411/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3412/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3413/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005881\n",
      "Train Epoch: 3414/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3415/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3416/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3417/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3418/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3419/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3420/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3421/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3422/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3423/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3424/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3425/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3426/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3427/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3428/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3429/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3430/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3431/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3432/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3433/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3434/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3435/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3436/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3437/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3438/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3439/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3440/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3441/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3442/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3443/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3444/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3445/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3446/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3447/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3448/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3449/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3450/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3451/10000 (34%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3452/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3453/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3454/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3455/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3456/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3457/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3458/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3459/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3460/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3461/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3462/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3463/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3464/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3465/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3466/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3467/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3468/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3469/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3470/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3471/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3472/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3473/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3474/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3475/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3476/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3477/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3478/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3479/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3480/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3481/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3482/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3483/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3484/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3485/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3486/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3487/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3488/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3489/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3490/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3491/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3492/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3493/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3494/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3495/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3496/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3497/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3498/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3499/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3500/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3501/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3502/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3503/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3504/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3505/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3506/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3507/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3508/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3509/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3510/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3511/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3512/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3513/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3514/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3515/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3516/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3517/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3518/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3519/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3520/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3521/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3522/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3523/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3524/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3525/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3526/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3527/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3528/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3529/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3530/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3531/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3532/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3533/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3534/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3535/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3536/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3537/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3538/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3539/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3540/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3541/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3542/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3543/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3544/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3545/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3546/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3547/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3548/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005880\n",
      "Train Epoch: 3549/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3550/10000 (35%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3551/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3552/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3553/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3554/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3555/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3556/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3557/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3558/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3559/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3560/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3561/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3562/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3563/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3564/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3565/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3566/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3567/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3568/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3569/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3570/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3571/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3572/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005877\n",
      "Train Epoch: 3573/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3574/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3575/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3576/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3577/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3578/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3579/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3580/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3581/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3582/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3583/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3584/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3585/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3586/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3587/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3588/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3589/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3590/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3591/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3592/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3593/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3594/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3595/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3596/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3597/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3598/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3599/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3600/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3601/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3602/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3603/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3604/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3605/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3606/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3607/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3608/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3609/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3610/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3611/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3612/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3613/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3614/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3615/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3616/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3617/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3618/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3619/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3620/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3621/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3622/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3623/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3624/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3625/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3626/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3627/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3628/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3629/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3630/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3631/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3632/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3633/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3634/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3635/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3636/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3637/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3638/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3639/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3640/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3641/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3642/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3643/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3644/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3645/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3646/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3647/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3648/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3649/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3650/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3651/10000 (36%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3652/10000 (37%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3653/10000 (37%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3654/10000 (37%)\ttrain_Loss: 0.005185\tval_Loss: 0.005879\n",
      "Train Epoch: 3655/10000 (37%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3656/10000 (37%)\ttrain_Loss: 0.005185\tval_Loss: 0.005878\n",
      "Train Epoch: 3657/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3658/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3659/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3660/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3661/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3662/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3663/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3664/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3665/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3666/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3667/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3668/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3669/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3670/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3671/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3672/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3673/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3674/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3675/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3676/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3677/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3678/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3679/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n",
      "Train Epoch: 3680/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n",
      "Train Epoch: 3681/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n",
      "Train Epoch: 3682/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n",
      "Train Epoch: 3683/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n",
      "Train Epoch: 3684/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3685/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3686/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3687/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3688/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3689/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3690/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3691/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3692/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3693/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n",
      "Train Epoch: 3694/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n",
      "Train Epoch: 3695/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3696/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3697/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3698/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3699/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3700/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3701/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3702/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3703/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3704/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3705/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3706/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3707/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n",
      "Train Epoch: 3708/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n",
      "Train Epoch: 3709/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n",
      "Train Epoch: 3710/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3711/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3712/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3713/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3714/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3715/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3716/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3717/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3718/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3719/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3720/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3721/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3722/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n",
      "Train Epoch: 3723/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n",
      "Train Epoch: 3724/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3725/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3726/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3727/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3728/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3729/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3730/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3731/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3732/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3733/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3734/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3735/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3736/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3737/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3738/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3739/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3740/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3741/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3742/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3743/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3744/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3745/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3746/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3747/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3748/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3749/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3750/10000 (37%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3751/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3752/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3753/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3754/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3755/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3756/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3757/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3758/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3759/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3760/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3761/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3762/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3763/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3764/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3765/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3766/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3767/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3768/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3769/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3770/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3771/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3772/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3773/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3774/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3775/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3776/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3777/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3778/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3779/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3780/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3781/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3782/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3783/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3784/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3785/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3786/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3787/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3788/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3789/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3790/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3791/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3792/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3793/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3794/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3795/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3796/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3797/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3798/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3799/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3800/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3801/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3802/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3803/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3804/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3805/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3806/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3807/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3808/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3809/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3810/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3811/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3812/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3813/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3814/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3815/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3816/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3817/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3818/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3819/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3820/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3821/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3822/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3823/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3824/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3825/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3826/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3827/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3828/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3829/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3830/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3831/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3832/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3833/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3834/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3835/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3836/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3837/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3838/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3839/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3840/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3841/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3842/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3843/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3844/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3845/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3846/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3847/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3848/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3849/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3850/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3851/10000 (38%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3852/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3853/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3854/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3855/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3856/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3857/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3858/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3859/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3860/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3861/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3862/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3863/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3864/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3865/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3866/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3867/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3868/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3869/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3870/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3871/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3872/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3873/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3874/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3875/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3876/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3877/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3878/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3879/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3880/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3881/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3882/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3883/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3884/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3885/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3886/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3887/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3888/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3889/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3890/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3891/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3892/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3893/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3894/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3895/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3896/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3897/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3898/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3899/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3900/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3901/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3902/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3903/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3904/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3905/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3906/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3907/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3908/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3909/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3910/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3911/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3912/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3913/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3914/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3915/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3916/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3917/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3918/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3919/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3920/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3921/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3922/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3923/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3924/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3925/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3926/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3927/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3928/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3929/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3930/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3931/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3932/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3933/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3934/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3935/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3936/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3937/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3938/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3939/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3940/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3941/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3942/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3943/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3944/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3945/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3946/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3947/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3948/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3949/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3950/10000 (39%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3951/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3952/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3953/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3954/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3955/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3956/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3957/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3958/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3959/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3960/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3961/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3962/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3963/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3964/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3965/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3966/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3967/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3968/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3969/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3970/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3971/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3972/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3973/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3974/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3975/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3976/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3977/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3978/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3979/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3980/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3981/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3982/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3983/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3984/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3985/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3986/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3987/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3988/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3989/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3990/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3991/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3992/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3993/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3994/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 3995/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3996/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 3997/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3998/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 3999/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4000/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4001/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4002/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4003/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4004/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4005/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4006/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4007/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4008/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4009/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4010/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4011/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4012/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4013/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 4014/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005878\n",
      "Train Epoch: 4015/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4016/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4017/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4018/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4019/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4020/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4021/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4022/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4023/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4024/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4025/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4026/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4027/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4028/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4029/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4030/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4031/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4032/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4033/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4034/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4035/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4036/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4037/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4038/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4039/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4040/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4041/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4042/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4043/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4044/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4045/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4046/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4047/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4048/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4049/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4050/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4051/10000 (40%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4052/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4053/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4054/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4055/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4056/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4057/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4058/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4059/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4060/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4061/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4062/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4063/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4064/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4065/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4066/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4067/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4068/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4069/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4070/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4071/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4072/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4073/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4074/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4075/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4076/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4077/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4078/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4079/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4080/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4081/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4082/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4083/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4084/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4085/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4086/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4087/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4088/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4089/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4090/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4091/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005875\n",
      "Train Epoch: 4092/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4093/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4094/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4095/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4096/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4097/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4098/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4099/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4100/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4101/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005875\n",
      "Train Epoch: 4102/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4103/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4104/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4105/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005877\n",
      "Train Epoch: 4106/10000 (41%)\ttrain_Loss: 0.005184\tval_Loss: 0.005876\n",
      "Train Epoch: 4107/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4108/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4109/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4110/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4111/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4112/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4113/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4114/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4115/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4116/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4117/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4118/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4119/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4120/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4121/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4122/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4123/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4124/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4125/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4126/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4127/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4128/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4129/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4130/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4131/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4132/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4133/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4134/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4135/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4136/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4137/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4138/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4139/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4140/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4141/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4142/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4143/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4144/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4145/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4146/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4147/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4148/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4149/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4150/10000 (41%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4151/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4152/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4153/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4154/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4155/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4156/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4157/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4158/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4159/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4160/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4161/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4162/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4163/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4164/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4165/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4166/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4167/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4168/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4169/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4170/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4171/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4172/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4173/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4174/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4175/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4176/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4177/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4178/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4179/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4180/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4181/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4182/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4183/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4184/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4185/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4186/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4187/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4188/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4189/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4190/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4191/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4192/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4193/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4194/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4195/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4196/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005877\n",
      "Train Epoch: 4197/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4198/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4199/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4200/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4201/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4202/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4203/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4204/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4205/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4206/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4207/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4208/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4209/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4210/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4211/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4212/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4213/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4214/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4215/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4216/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4217/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4218/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4219/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4220/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4221/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4222/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4223/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4224/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4225/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4226/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4227/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4228/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4229/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4230/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4231/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4232/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4233/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4234/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4235/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4236/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4237/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4238/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4239/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4240/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4241/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4242/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4243/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4244/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4245/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4246/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4247/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4248/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4249/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4250/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4251/10000 (42%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4252/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4253/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4254/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4255/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4256/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4257/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4258/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4259/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4260/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4261/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4262/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4263/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4264/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4265/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4266/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4267/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4268/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4269/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4270/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4271/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4272/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4273/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4274/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4275/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4276/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4277/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4278/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4279/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4280/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4281/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4282/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4283/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4284/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4285/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4286/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4287/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4288/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4289/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4290/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4291/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4292/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4293/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4294/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4295/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4296/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4297/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4298/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4299/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4300/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4301/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4302/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4303/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4304/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4305/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4306/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4307/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4308/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4309/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4310/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4311/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4312/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4313/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4314/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4315/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4316/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4317/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4318/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4319/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4320/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4321/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4322/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4323/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4324/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4325/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4326/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4327/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4328/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4329/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4330/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4331/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4332/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4333/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4334/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4335/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4336/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4337/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4338/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4339/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4340/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4341/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4342/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4343/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4344/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4345/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4346/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4347/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4348/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4349/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4350/10000 (43%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4351/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4352/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4353/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4354/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4355/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4356/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4357/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4358/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4359/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4360/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4361/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4362/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4363/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4364/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4365/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4366/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4367/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4368/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4369/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4370/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4371/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4372/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4373/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4374/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4375/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4376/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4377/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4378/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4379/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4380/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4381/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4382/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4383/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4384/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4385/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4386/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4387/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4388/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4389/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4390/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4391/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4392/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4393/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4394/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4395/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4396/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4397/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4398/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4399/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4400/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4401/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4402/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4403/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4404/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4405/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4406/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4407/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4408/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4409/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4410/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4411/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4412/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4413/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005874\n",
      "Train Epoch: 4414/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4415/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4416/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4417/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4418/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4419/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4420/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4421/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005874\n",
      "Train Epoch: 4422/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4423/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4424/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005876\n",
      "Train Epoch: 4425/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4426/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4427/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4428/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4429/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4430/10000 (44%)\ttrain_Loss: 0.005183\tval_Loss: 0.005875\n",
      "Train Epoch: 4431/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4432/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4433/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4434/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4435/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4436/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4437/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005876\n",
      "Train Epoch: 4438/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4439/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4440/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4441/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4442/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4443/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4444/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4445/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4446/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4447/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4448/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4449/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4450/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4451/10000 (44%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4452/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4453/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4454/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4455/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4456/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4457/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4458/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4459/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4460/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4461/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4462/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4463/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4464/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4465/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4466/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4467/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4468/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4469/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4470/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4471/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4472/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4473/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4474/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4475/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4476/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4477/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4478/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4479/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4480/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4481/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4482/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4483/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4484/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4485/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4486/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4487/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4488/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4489/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4490/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4491/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4492/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4493/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4494/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4495/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4496/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4497/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4498/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4499/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4500/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4501/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4502/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4503/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4504/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4505/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4506/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4507/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4508/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4509/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4510/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4511/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4512/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4513/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4514/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4515/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4516/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4517/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4518/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4519/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4520/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4521/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4522/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4523/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4524/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4525/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4526/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4527/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4528/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4529/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4530/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4531/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4532/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4533/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4534/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4535/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4536/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4537/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4538/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4539/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4540/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4541/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4542/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4543/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4544/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4545/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4546/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4547/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4548/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4549/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4550/10000 (45%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4551/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4552/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4553/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4554/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005876\n",
      "Train Epoch: 4555/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005876\n",
      "Train Epoch: 4556/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4557/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4558/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4559/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4560/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4561/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4562/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4563/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4564/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4565/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005876\n",
      "Train Epoch: 4566/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4567/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4568/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4569/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4570/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4571/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4572/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4573/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005872\n",
      "Train Epoch: 4574/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4575/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4576/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4577/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4578/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4579/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4580/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4581/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4582/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4583/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4584/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4585/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4586/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4587/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4588/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4589/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4590/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4591/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4592/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4593/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4594/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4595/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4596/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4597/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4598/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4599/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4600/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4601/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4602/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4603/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4604/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4605/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4606/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4607/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4608/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4609/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4610/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4611/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4612/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4613/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4614/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4615/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4616/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4617/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4618/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4619/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4620/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4621/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4622/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4623/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4624/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4625/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4626/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4627/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4628/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4629/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4630/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4631/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4632/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4633/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4634/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4635/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4636/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4637/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4638/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4639/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4640/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4641/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4642/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4643/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005872\n",
      "Train Epoch: 4644/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4645/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4646/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4647/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4648/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4649/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4650/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4651/10000 (46%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4652/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4653/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4654/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4655/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4656/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4657/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4658/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4659/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4660/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4661/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4662/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4663/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4664/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005875\n",
      "Train Epoch: 4665/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005874\n",
      "Train Epoch: 4666/10000 (47%)\ttrain_Loss: 0.005182\tval_Loss: 0.005873\n",
      "Train Epoch: 4667/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4668/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4669/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4670/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4671/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4672/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4673/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4674/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4675/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4676/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4677/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4678/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4679/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4680/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4681/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4682/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4683/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4684/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4685/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4686/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4687/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4688/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4689/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4690/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4691/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4692/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4693/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4694/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4695/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4696/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4697/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4698/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4699/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4700/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4701/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4702/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4703/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4704/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4705/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4706/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4707/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4708/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4709/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005875\n",
      "Train Epoch: 4710/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4711/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4712/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4713/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4714/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4715/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4716/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4717/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4718/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4719/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4720/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4721/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4722/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4723/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4724/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4725/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4726/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4727/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4728/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4729/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4730/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4731/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4732/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4733/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4734/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4735/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4736/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4737/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4738/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4739/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4740/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4741/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4742/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4743/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4744/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4745/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4746/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4747/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4748/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4749/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4750/10000 (47%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4751/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4752/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4753/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4754/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4755/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4756/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4757/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4758/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4759/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4760/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4761/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4762/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4763/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4764/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4765/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4766/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4767/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4768/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4769/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4770/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4771/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4772/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4773/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005875\n",
      "Train Epoch: 4774/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4775/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4776/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4777/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4778/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4779/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005871\n",
      "Train Epoch: 4780/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005871\n",
      "Train Epoch: 4781/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4782/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4783/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005875\n",
      "Train Epoch: 4784/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4785/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4786/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4787/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4788/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4789/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4790/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005871\n",
      "Train Epoch: 4791/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4792/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4793/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005875\n",
      "Train Epoch: 4794/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4795/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4796/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4797/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4798/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4799/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4800/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4801/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4802/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4803/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4804/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4805/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4806/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4807/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4808/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4809/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4810/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005871\n",
      "Train Epoch: 4811/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4812/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4813/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4814/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4815/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4816/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4817/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4818/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4819/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4820/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4821/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4822/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4823/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4824/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4825/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4826/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4827/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4828/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4829/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4830/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005871\n",
      "Train Epoch: 4831/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4832/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4833/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4834/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4835/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4836/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4837/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4838/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4839/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4840/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005871\n",
      "Train Epoch: 4841/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4842/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4843/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005874\n",
      "Train Epoch: 4844/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4845/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4846/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005873\n",
      "Train Epoch: 4847/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4848/10000 (48%)\ttrain_Loss: 0.005181\tval_Loss: 0.005872\n",
      "Train Epoch: 4849/10000 (48%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4850/10000 (48%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4851/10000 (48%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4852/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4853/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005874\n",
      "Train Epoch: 4854/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4855/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4856/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4857/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4858/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4859/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4860/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4861/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4862/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4863/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005874\n",
      "Train Epoch: 4864/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4865/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4866/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4867/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4868/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4869/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4870/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4871/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4872/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4873/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005874\n",
      "Train Epoch: 4874/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4875/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4876/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4877/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4878/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4879/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4880/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4881/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4882/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4883/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4884/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4885/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4886/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4887/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4888/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4889/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4890/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4891/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4892/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4893/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4894/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4895/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4896/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4897/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4898/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4899/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4900/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4901/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4902/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4903/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4904/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4905/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4906/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4907/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4908/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4909/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4910/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4911/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4912/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4913/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4914/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4915/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4916/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4917/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4918/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4919/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4920/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4921/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4922/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4923/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4924/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4925/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4926/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4927/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4928/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005870\n",
      "Train Epoch: 4929/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4930/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4931/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4932/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4933/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4934/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4935/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4936/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4937/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4938/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4939/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4940/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4941/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4942/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4943/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4944/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4945/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4946/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4947/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4948/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4949/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4950/10000 (49%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4951/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4952/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4953/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4954/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4955/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4956/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4957/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4958/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4959/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4960/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4961/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4962/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4963/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4964/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4965/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4966/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4967/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4968/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4969/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4970/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4971/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4972/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4973/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4974/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4975/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005870\n",
      "Train Epoch: 4976/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4977/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n",
      "Train Epoch: 4978/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005873\n",
      "Train Epoch: 4979/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4980/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4981/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4982/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005870\n",
      "Train Epoch: 4983/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005870\n",
      "Train Epoch: 4984/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005870\n",
      "Train Epoch: 4985/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4986/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4987/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4988/10000 (50%)\ttrain_Loss: 0.005180\tval_Loss: 0.005871\n",
      "Train Epoch: 4989/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 4990/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 4991/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 4992/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 4993/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 4994/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 4995/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 4996/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 4997/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005873\n",
      "Train Epoch: 4998/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 4999/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5000/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5001/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5002/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5003/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5004/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005869\n",
      "Train Epoch: 5005/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5006/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5007/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5008/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5009/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5010/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5011/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5012/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5013/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5014/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5015/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5016/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5017/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5018/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5019/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5020/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5021/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5022/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5023/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5024/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5025/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5026/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5027/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5028/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5029/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5030/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5031/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5032/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5033/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5034/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5035/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5036/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5037/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5038/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5039/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5040/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5041/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5042/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5043/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5044/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5045/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5046/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5047/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5048/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5049/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5050/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5051/10000 (50%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5052/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5053/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5054/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5055/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5056/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5057/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005869\n",
      "Train Epoch: 5058/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005868\n",
      "Train Epoch: 5059/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005869\n",
      "Train Epoch: 5060/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5061/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5062/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5063/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5064/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5065/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5066/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5067/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005869\n",
      "Train Epoch: 5068/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005869\n",
      "Train Epoch: 5069/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5070/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5071/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005872\n",
      "Train Epoch: 5072/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5073/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5074/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5075/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5076/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5077/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5078/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5079/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5080/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5081/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5082/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5083/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005869\n",
      "Train Epoch: 5084/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005869\n",
      "Train Epoch: 5085/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5086/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5087/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5088/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5089/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5090/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5091/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5092/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5093/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5094/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5095/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5096/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5097/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5098/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005869\n",
      "Train Epoch: 5099/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5100/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5101/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005871\n",
      "Train Epoch: 5102/10000 (51%)\ttrain_Loss: 0.005179\tval_Loss: 0.005870\n",
      "Train Epoch: 5103/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5104/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5105/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005871\n",
      "Train Epoch: 5106/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005871\n",
      "Train Epoch: 5107/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5108/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5109/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5110/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5111/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005871\n",
      "Train Epoch: 5112/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5113/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5114/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5115/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5116/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5117/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5118/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5119/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5120/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005871\n",
      "Train Epoch: 5121/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5122/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5123/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5124/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5125/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5126/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5127/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5128/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5129/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5130/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5131/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5132/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5133/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5134/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5135/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005871\n",
      "Train Epoch: 5136/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5137/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5138/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5139/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5140/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5141/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5142/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5143/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5144/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5145/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5146/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5147/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005868\n",
      "Train Epoch: 5148/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005868\n",
      "Train Epoch: 5149/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5150/10000 (51%)\ttrain_Loss: 0.005178\tval_Loss: 0.005871\n",
      "Train Epoch: 5151/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005871\n",
      "Train Epoch: 5152/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5153/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5154/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5155/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5156/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005868\n",
      "Train Epoch: 5157/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005867\n",
      "Train Epoch: 5158/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5159/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5160/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005871\n",
      "Train Epoch: 5161/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5162/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5163/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5164/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5165/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005868\n",
      "Train Epoch: 5166/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5167/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5168/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5169/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5170/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5171/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5172/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5173/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5174/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5175/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5176/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5177/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5178/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005868\n",
      "Train Epoch: 5179/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5180/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5181/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5182/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005868\n",
      "Train Epoch: 5183/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005868\n",
      "Train Epoch: 5184/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5185/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5186/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5187/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005868\n",
      "Train Epoch: 5188/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005868\n",
      "Train Epoch: 5189/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5190/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5191/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5192/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005868\n",
      "Train Epoch: 5193/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5194/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5195/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005870\n",
      "Train Epoch: 5196/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005869\n",
      "Train Epoch: 5197/10000 (52%)\ttrain_Loss: 0.005178\tval_Loss: 0.005868\n",
      "Train Epoch: 5198/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5199/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5200/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5201/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5202/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5203/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5204/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5205/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5206/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5207/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5208/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5209/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5210/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5211/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5212/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5213/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5214/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5215/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5216/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5217/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5218/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5219/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5220/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5221/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5222/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5223/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5224/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5225/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5226/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5227/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5228/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5229/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5230/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5231/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5232/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5233/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5234/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5235/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5236/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5237/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5238/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5239/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5240/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5241/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5242/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5243/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5244/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5245/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5246/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5247/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5248/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5249/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5250/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5251/10000 (52%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5252/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5253/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5254/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5255/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5256/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5257/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5258/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5259/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5260/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5261/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5262/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5263/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5264/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5265/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5266/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5267/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5268/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5269/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005866\n",
      "Train Epoch: 5270/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5271/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005869\n",
      "Train Epoch: 5272/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5273/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5274/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5275/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005868\n",
      "Train Epoch: 5276/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5277/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005866\n",
      "Train Epoch: 5278/10000 (53%)\ttrain_Loss: 0.005177\tval_Loss: 0.005867\n",
      "Train Epoch: 5279/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5280/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5281/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5282/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5283/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5284/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5285/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5286/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5287/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5288/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5289/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5290/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5291/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5292/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5293/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005865\n",
      "Train Epoch: 5294/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5295/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5296/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5297/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5298/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5299/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5300/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5301/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005865\n",
      "Train Epoch: 5302/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5303/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5304/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5305/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5306/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5307/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5308/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5309/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005865\n",
      "Train Epoch: 5310/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5311/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5312/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5313/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5314/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5315/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5316/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5317/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005865\n",
      "Train Epoch: 5318/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5319/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5320/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5321/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5322/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5323/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5324/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5325/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005865\n",
      "Train Epoch: 5326/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5327/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5328/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5329/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5330/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5331/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5332/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005865\n",
      "Train Epoch: 5333/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5334/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5335/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5336/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5337/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5338/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5339/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5340/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005865\n",
      "Train Epoch: 5341/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5342/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5343/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5344/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5345/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5346/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005868\n",
      "Train Epoch: 5347/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005867\n",
      "Train Epoch: 5348/10000 (53%)\ttrain_Loss: 0.005176\tval_Loss: 0.005866\n",
      "Train Epoch: 5349/10000 (53%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5350/10000 (53%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5351/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5352/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005864\n",
      "Train Epoch: 5353/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005864\n",
      "Train Epoch: 5354/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5355/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005867\n",
      "Train Epoch: 5356/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5357/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5358/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5359/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5360/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5361/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5362/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5363/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005867\n",
      "Train Epoch: 5364/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005867\n",
      "Train Epoch: 5365/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5366/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5367/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005864\n",
      "Train Epoch: 5368/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5369/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005867\n",
      "Train Epoch: 5370/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005867\n",
      "Train Epoch: 5371/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5372/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5373/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5374/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005864\n",
      "Train Epoch: 5375/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005864\n",
      "Train Epoch: 5376/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5377/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5378/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5379/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5380/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5381/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5382/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005864\n",
      "Train Epoch: 5383/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5384/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5385/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5386/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5387/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5388/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5389/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5390/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5391/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5392/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5393/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5394/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5395/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5396/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5397/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005864\n",
      "Train Epoch: 5398/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5399/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5400/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005864\n",
      "Train Epoch: 5401/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005864\n",
      "Train Epoch: 5402/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5403/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005866\n",
      "Train Epoch: 5404/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5405/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005864\n",
      "Train Epoch: 5406/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5407/10000 (54%)\ttrain_Loss: 0.005175\tval_Loss: 0.005865\n",
      "Train Epoch: 5408/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5409/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5410/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5411/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5412/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5413/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5414/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5415/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5416/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5417/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5418/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5419/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5420/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5421/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005863\n",
      "Train Epoch: 5422/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005863\n",
      "Train Epoch: 5423/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5424/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5425/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5426/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5427/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5428/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005863\n",
      "Train Epoch: 5429/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5430/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5431/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005866\n",
      "Train Epoch: 5432/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5433/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5434/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5435/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005863\n",
      "Train Epoch: 5436/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005863\n",
      "Train Epoch: 5437/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5438/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5439/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5440/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5441/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5442/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005863\n",
      "Train Epoch: 5443/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005863\n",
      "Train Epoch: 5444/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005863\n",
      "Train Epoch: 5445/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5446/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005865\n",
      "Train Epoch: 5447/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5448/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5449/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005863\n",
      "Train Epoch: 5450/10000 (54%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5451/10000 (55%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5452/10000 (55%)\ttrain_Loss: 0.005174\tval_Loss: 0.005863\n",
      "Train Epoch: 5453/10000 (55%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5454/10000 (55%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5455/10000 (55%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5456/10000 (55%)\ttrain_Loss: 0.005174\tval_Loss: 0.005862\n",
      "Train Epoch: 5457/10000 (55%)\ttrain_Loss: 0.005174\tval_Loss: 0.005863\n",
      "Train Epoch: 5458/10000 (55%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5459/10000 (55%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5460/10000 (55%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n",
      "Train Epoch: 5461/10000 (55%)\ttrain_Loss: 0.005174\tval_Loss: 0.005864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5462/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005864\n",
      "Train Epoch: 5463/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5464/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5465/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5466/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005864\n",
      "Train Epoch: 5467/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005864\n",
      "Train Epoch: 5468/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005864\n",
      "Train Epoch: 5469/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005864\n",
      "Train Epoch: 5470/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5471/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005861\n",
      "Train Epoch: 5472/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5473/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005864\n",
      "Train Epoch: 5474/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005864\n",
      "Train Epoch: 5475/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005864\n",
      "Train Epoch: 5476/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5477/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5478/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5479/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5480/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5481/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005864\n",
      "Train Epoch: 5482/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005864\n",
      "Train Epoch: 5483/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5484/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5485/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005861\n",
      "Train Epoch: 5486/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5487/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5488/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005864\n",
      "Train Epoch: 5489/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005864\n",
      "Train Epoch: 5490/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5491/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5492/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005861\n",
      "Train Epoch: 5493/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5494/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5495/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5496/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5497/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5498/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5499/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005861\n",
      "Train Epoch: 5500/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005861\n",
      "Train Epoch: 5501/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5502/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5503/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005863\n",
      "Train Epoch: 5504/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5505/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005861\n",
      "Train Epoch: 5506/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5507/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5508/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005861\n",
      "Train Epoch: 5509/10000 (55%)\ttrain_Loss: 0.005173\tval_Loss: 0.005862\n",
      "Train Epoch: 5510/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5511/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5512/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5513/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5514/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005863\n",
      "Train Epoch: 5515/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005864\n",
      "Train Epoch: 5516/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005863\n",
      "Train Epoch: 5517/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5518/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5519/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5520/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5521/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5522/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5523/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5524/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5525/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005860\n",
      "Train Epoch: 5526/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005860\n",
      "Train Epoch: 5527/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5528/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5529/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5530/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5531/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5532/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5533/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5534/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005860\n",
      "Train Epoch: 5535/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5536/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5537/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5538/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5539/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005859\n",
      "Train Epoch: 5540/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005860\n",
      "Train Epoch: 5541/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5542/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5543/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5544/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5545/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5546/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005860\n",
      "Train Epoch: 5547/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005860\n",
      "Train Epoch: 5548/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5549/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005862\n",
      "Train Epoch: 5550/10000 (55%)\ttrain_Loss: 0.005172\tval_Loss: 0.005861\n",
      "Train Epoch: 5551/10000 (56%)\ttrain_Loss: 0.005172\tval_Loss: 0.005860\n",
      "Train Epoch: 5552/10000 (56%)\ttrain_Loss: 0.005172\tval_Loss: 0.005860\n",
      "Train Epoch: 5553/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005861\n",
      "Train Epoch: 5554/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005862\n",
      "Train Epoch: 5555/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005862\n",
      "Train Epoch: 5556/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005861\n",
      "Train Epoch: 5557/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5558/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5559/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5560/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5561/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5562/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5563/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5564/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005859\n",
      "Train Epoch: 5565/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5566/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5567/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5568/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005859\n",
      "Train Epoch: 5569/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5570/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005862\n",
      "Train Epoch: 5571/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005861\n",
      "Train Epoch: 5572/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5573/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005859\n",
      "Train Epoch: 5574/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5575/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005859\n",
      "Train Epoch: 5576/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005859\n",
      "Train Epoch: 5577/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5578/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005861\n",
      "Train Epoch: 5579/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005861\n",
      "Train Epoch: 5580/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005861\n",
      "Train Epoch: 5581/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5582/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005859\n",
      "Train Epoch: 5583/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005859\n",
      "Train Epoch: 5584/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005858\n",
      "Train Epoch: 5585/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005858\n",
      "Train Epoch: 5586/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005859\n",
      "Train Epoch: 5587/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5588/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5589/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5590/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005860\n",
      "Train Epoch: 5591/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005859\n",
      "Train Epoch: 5592/10000 (56%)\ttrain_Loss: 0.005171\tval_Loss: 0.005858\n",
      "Train Epoch: 5593/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005858\n",
      "Train Epoch: 5594/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005859\n",
      "Train Epoch: 5595/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005860\n",
      "Train Epoch: 5596/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005860\n",
      "Train Epoch: 5597/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005859\n",
      "Train Epoch: 5598/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005858\n",
      "Train Epoch: 5599/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005860\n",
      "Train Epoch: 5600/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005861\n",
      "Train Epoch: 5601/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005860\n",
      "Train Epoch: 5602/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005859\n",
      "Train Epoch: 5603/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005859\n",
      "Train Epoch: 5604/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005858\n",
      "Train Epoch: 5605/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005857\n",
      "Train Epoch: 5606/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005858\n",
      "Train Epoch: 5607/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005859\n",
      "Train Epoch: 5608/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005860\n",
      "Train Epoch: 5609/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005860\n",
      "Train Epoch: 5610/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005858\n",
      "Train Epoch: 5611/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005856\n",
      "Train Epoch: 5612/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005857\n",
      "Train Epoch: 5613/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005859\n",
      "Train Epoch: 5614/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005860\n",
      "Train Epoch: 5615/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005860\n",
      "Train Epoch: 5616/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005859\n",
      "Train Epoch: 5617/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005860\n",
      "Train Epoch: 5618/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005859\n",
      "Train Epoch: 5619/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005857\n",
      "Train Epoch: 5620/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005856\n",
      "Train Epoch: 5621/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005857\n",
      "Train Epoch: 5622/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005859\n",
      "Train Epoch: 5623/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005859\n",
      "Train Epoch: 5624/10000 (56%)\ttrain_Loss: 0.005170\tval_Loss: 0.005859\n",
      "Train Epoch: 5625/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005858\n",
      "Train Epoch: 5626/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005859\n",
      "Train Epoch: 5627/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005859\n",
      "Train Epoch: 5628/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005858\n",
      "Train Epoch: 5629/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005856\n",
      "Train Epoch: 5630/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005857\n",
      "Train Epoch: 5631/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005859\n",
      "Train Epoch: 5632/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005858\n",
      "Train Epoch: 5633/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005857\n",
      "Train Epoch: 5634/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005857\n",
      "Train Epoch: 5635/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005858\n",
      "Train Epoch: 5636/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005858\n",
      "Train Epoch: 5637/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005858\n",
      "Train Epoch: 5638/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005859\n",
      "Train Epoch: 5639/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005860\n",
      "Train Epoch: 5640/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005859\n",
      "Train Epoch: 5641/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005858\n",
      "Train Epoch: 5642/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005857\n",
      "Train Epoch: 5643/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005856\n",
      "Train Epoch: 5644/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005858\n",
      "Train Epoch: 5645/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005859\n",
      "Train Epoch: 5646/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005859\n",
      "Train Epoch: 5647/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005858\n",
      "Train Epoch: 5648/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005858\n",
      "Train Epoch: 5649/10000 (56%)\ttrain_Loss: 0.005169\tval_Loss: 0.005858\n",
      "Train Epoch: 5650/10000 (56%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5651/10000 (56%)\ttrain_Loss: 0.005168\tval_Loss: 0.005856\n",
      "Train Epoch: 5652/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5653/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005859\n",
      "Train Epoch: 5654/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005859\n",
      "Train Epoch: 5655/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005858\n",
      "Train Epoch: 5656/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5657/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5658/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5659/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5660/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5661/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005858\n",
      "Train Epoch: 5662/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005858\n",
      "Train Epoch: 5663/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5664/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5665/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5666/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5667/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005856\n",
      "Train Epoch: 5668/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5669/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005858\n",
      "Train Epoch: 5670/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005858\n",
      "Train Epoch: 5671/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5672/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005856\n",
      "Train Epoch: 5673/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5674/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005856\n",
      "Train Epoch: 5675/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5676/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5677/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5678/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005857\n",
      "Train Epoch: 5679/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005856\n",
      "Train Epoch: 5680/10000 (57%)\ttrain_Loss: 0.005168\tval_Loss: 0.005855\n",
      "Train Epoch: 5681/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005856\n",
      "Train Epoch: 5682/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005856\n",
      "Train Epoch: 5683/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005857\n",
      "Train Epoch: 5684/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005857\n",
      "Train Epoch: 5685/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005857\n",
      "Train Epoch: 5686/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005857\n",
      "Train Epoch: 5687/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005856\n",
      "Train Epoch: 5688/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005855\n",
      "Train Epoch: 5689/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005855\n",
      "Train Epoch: 5690/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005856\n",
      "Train Epoch: 5691/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005857\n",
      "Train Epoch: 5692/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005857\n",
      "Train Epoch: 5693/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005856\n",
      "Train Epoch: 5694/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005855\n",
      "Train Epoch: 5695/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005854\n",
      "Train Epoch: 5696/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005853\n",
      "Train Epoch: 5697/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005854\n",
      "Train Epoch: 5698/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005857\n",
      "Train Epoch: 5699/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5700/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005857\n",
      "Train Epoch: 5701/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005856\n",
      "Train Epoch: 5702/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005855\n",
      "Train Epoch: 5703/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005854\n",
      "Train Epoch: 5704/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005854\n",
      "Train Epoch: 5705/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005855\n",
      "Train Epoch: 5706/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005856\n",
      "Train Epoch: 5707/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005856\n",
      "Train Epoch: 5708/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005855\n",
      "Train Epoch: 5709/10000 (57%)\ttrain_Loss: 0.005167\tval_Loss: 0.005854\n",
      "Train Epoch: 5710/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005853\n",
      "Train Epoch: 5711/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005854\n",
      "Train Epoch: 5712/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005855\n",
      "Train Epoch: 5713/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005856\n",
      "Train Epoch: 5714/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005855\n",
      "Train Epoch: 5715/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005855\n",
      "Train Epoch: 5716/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005855\n",
      "Train Epoch: 5717/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005853\n",
      "Train Epoch: 5718/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005853\n",
      "Train Epoch: 5719/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005855\n",
      "Train Epoch: 5720/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005855\n",
      "Train Epoch: 5721/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005854\n",
      "Train Epoch: 5722/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005854\n",
      "Train Epoch: 5723/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005853\n",
      "Train Epoch: 5724/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005854\n",
      "Train Epoch: 5725/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005854\n",
      "Train Epoch: 5726/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005853\n",
      "Train Epoch: 5727/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005854\n",
      "Train Epoch: 5728/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005855\n",
      "Train Epoch: 5729/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005854\n",
      "Train Epoch: 5730/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005853\n",
      "Train Epoch: 5731/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005854\n",
      "Train Epoch: 5732/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005855\n",
      "Train Epoch: 5733/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005855\n",
      "Train Epoch: 5734/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005854\n",
      "Train Epoch: 5735/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005853\n",
      "Train Epoch: 5736/10000 (57%)\ttrain_Loss: 0.005166\tval_Loss: 0.005852\n",
      "Train Epoch: 5737/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005851\n",
      "Train Epoch: 5738/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005852\n",
      "Train Epoch: 5739/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005854\n",
      "Train Epoch: 5740/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005854\n",
      "Train Epoch: 5741/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005854\n",
      "Train Epoch: 5742/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005853\n",
      "Train Epoch: 5743/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005852\n",
      "Train Epoch: 5744/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005853\n",
      "Train Epoch: 5745/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005852\n",
      "Train Epoch: 5746/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005853\n",
      "Train Epoch: 5747/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005853\n",
      "Train Epoch: 5748/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005853\n",
      "Train Epoch: 5749/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005852\n",
      "Train Epoch: 5750/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005851\n",
      "Train Epoch: 5751/10000 (57%)\ttrain_Loss: 0.005165\tval_Loss: 0.005852\n",
      "Train Epoch: 5752/10000 (58%)\ttrain_Loss: 0.005165\tval_Loss: 0.005853\n",
      "Train Epoch: 5753/10000 (58%)\ttrain_Loss: 0.005165\tval_Loss: 0.005854\n",
      "Train Epoch: 5754/10000 (58%)\ttrain_Loss: 0.005165\tval_Loss: 0.005853\n",
      "Train Epoch: 5755/10000 (58%)\ttrain_Loss: 0.005165\tval_Loss: 0.005852\n",
      "Train Epoch: 5756/10000 (58%)\ttrain_Loss: 0.005165\tval_Loss: 0.005852\n",
      "Train Epoch: 5757/10000 (58%)\ttrain_Loss: 0.005165\tval_Loss: 0.005852\n",
      "Train Epoch: 5758/10000 (58%)\ttrain_Loss: 0.005165\tval_Loss: 0.005853\n",
      "Train Epoch: 5759/10000 (58%)\ttrain_Loss: 0.005165\tval_Loss: 0.005853\n",
      "Train Epoch: 5760/10000 (58%)\ttrain_Loss: 0.005165\tval_Loss: 0.005853\n",
      "Train Epoch: 5761/10000 (58%)\ttrain_Loss: 0.005165\tval_Loss: 0.005852\n",
      "Train Epoch: 5762/10000 (58%)\ttrain_Loss: 0.005165\tval_Loss: 0.005851\n",
      "Train Epoch: 5763/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005851\n",
      "Train Epoch: 5764/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005852\n",
      "Train Epoch: 5765/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005852\n",
      "Train Epoch: 5766/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005852\n",
      "Train Epoch: 5767/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005852\n",
      "Train Epoch: 5768/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005851\n",
      "Train Epoch: 5769/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005850\n",
      "Train Epoch: 5770/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005851\n",
      "Train Epoch: 5771/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005852\n",
      "Train Epoch: 5772/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005852\n",
      "Train Epoch: 5773/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005852\n",
      "Train Epoch: 5774/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005851\n",
      "Train Epoch: 5775/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005850\n",
      "Train Epoch: 5776/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005851\n",
      "Train Epoch: 5777/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005852\n",
      "Train Epoch: 5778/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005852\n",
      "Train Epoch: 5779/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005851\n",
      "Train Epoch: 5780/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005851\n",
      "Train Epoch: 5781/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005850\n",
      "Train Epoch: 5782/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005850\n",
      "Train Epoch: 5783/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005851\n",
      "Train Epoch: 5784/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005852\n",
      "Train Epoch: 5785/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005852\n",
      "Train Epoch: 5786/10000 (58%)\ttrain_Loss: 0.005164\tval_Loss: 0.005850\n",
      "Train Epoch: 5787/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005848\n",
      "Train Epoch: 5788/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005849\n",
      "Train Epoch: 5789/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005851\n",
      "Train Epoch: 5790/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005851\n",
      "Train Epoch: 5791/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005850\n",
      "Train Epoch: 5792/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005850\n",
      "Train Epoch: 5793/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005850\n",
      "Train Epoch: 5794/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005850\n",
      "Train Epoch: 5795/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005850\n",
      "Train Epoch: 5796/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005850\n",
      "Train Epoch: 5797/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005851\n",
      "Train Epoch: 5798/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005850\n",
      "Train Epoch: 5799/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005848\n",
      "Train Epoch: 5800/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005848\n",
      "Train Epoch: 5801/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005849\n",
      "Train Epoch: 5802/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005850\n",
      "Train Epoch: 5803/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005849\n",
      "Train Epoch: 5804/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005849\n",
      "Train Epoch: 5805/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005848\n",
      "Train Epoch: 5806/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005848\n",
      "Train Epoch: 5807/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005850\n",
      "Train Epoch: 5808/10000 (58%)\ttrain_Loss: 0.005163\tval_Loss: 0.005850\n",
      "Train Epoch: 5809/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005849\n",
      "Train Epoch: 5810/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005849\n",
      "Train Epoch: 5811/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005849\n",
      "Train Epoch: 5812/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005848\n",
      "Train Epoch: 5813/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005848\n",
      "Train Epoch: 5814/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005848\n",
      "Train Epoch: 5815/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005848\n",
      "Train Epoch: 5816/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005848\n",
      "Train Epoch: 5817/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005848\n",
      "Train Epoch: 5818/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005849\n",
      "Train Epoch: 5819/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005849\n",
      "Train Epoch: 5820/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005848\n",
      "Train Epoch: 5821/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005847\n",
      "Train Epoch: 5822/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005847\n",
      "Train Epoch: 5823/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005847\n",
      "Train Epoch: 5824/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005849\n",
      "Train Epoch: 5825/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5826/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005849\n",
      "Train Epoch: 5827/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005848\n",
      "Train Epoch: 5828/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005848\n",
      "Train Epoch: 5829/10000 (58%)\ttrain_Loss: 0.005162\tval_Loss: 0.005848\n",
      "Train Epoch: 5830/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005848\n",
      "Train Epoch: 5831/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005847\n",
      "Train Epoch: 5832/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005846\n",
      "Train Epoch: 5833/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005846\n",
      "Train Epoch: 5834/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005847\n",
      "Train Epoch: 5835/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005847\n",
      "Train Epoch: 5836/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005847\n",
      "Train Epoch: 5837/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005848\n",
      "Train Epoch: 5838/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005847\n",
      "Train Epoch: 5839/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005847\n",
      "Train Epoch: 5840/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005847\n",
      "Train Epoch: 5841/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005847\n",
      "Train Epoch: 5842/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005847\n",
      "Train Epoch: 5843/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005846\n",
      "Train Epoch: 5844/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005846\n",
      "Train Epoch: 5845/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005845\n",
      "Train Epoch: 5846/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005847\n",
      "Train Epoch: 5847/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005847\n",
      "Train Epoch: 5848/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005846\n",
      "Train Epoch: 5849/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005845\n",
      "Train Epoch: 5850/10000 (58%)\ttrain_Loss: 0.005161\tval_Loss: 0.005845\n",
      "Train Epoch: 5851/10000 (58%)\ttrain_Loss: 0.005160\tval_Loss: 0.005847\n",
      "Train Epoch: 5852/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005846\n",
      "Train Epoch: 5853/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005845\n",
      "Train Epoch: 5854/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005845\n",
      "Train Epoch: 5855/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005846\n",
      "Train Epoch: 5856/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005846\n",
      "Train Epoch: 5857/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005846\n",
      "Train Epoch: 5858/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005847\n",
      "Train Epoch: 5859/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005847\n",
      "Train Epoch: 5860/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005844\n",
      "Train Epoch: 5861/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005843\n",
      "Train Epoch: 5862/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005844\n",
      "Train Epoch: 5863/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005846\n",
      "Train Epoch: 5864/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005846\n",
      "Train Epoch: 5865/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005845\n",
      "Train Epoch: 5866/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005845\n",
      "Train Epoch: 5867/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005844\n",
      "Train Epoch: 5868/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005843\n",
      "Train Epoch: 5869/10000 (59%)\ttrain_Loss: 0.005160\tval_Loss: 0.005843\n",
      "Train Epoch: 5870/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005845\n",
      "Train Epoch: 5871/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005846\n",
      "Train Epoch: 5872/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005845\n",
      "Train Epoch: 5873/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005845\n",
      "Train Epoch: 5874/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005844\n",
      "Train Epoch: 5875/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005843\n",
      "Train Epoch: 5876/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005843\n",
      "Train Epoch: 5877/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005845\n",
      "Train Epoch: 5878/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005845\n",
      "Train Epoch: 5879/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005844\n",
      "Train Epoch: 5880/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005843\n",
      "Train Epoch: 5881/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005843\n",
      "Train Epoch: 5882/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005843\n",
      "Train Epoch: 5883/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005844\n",
      "Train Epoch: 5884/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005844\n",
      "Train Epoch: 5885/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005844\n",
      "Train Epoch: 5886/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005843\n",
      "Train Epoch: 5887/10000 (59%)\ttrain_Loss: 0.005159\tval_Loss: 0.005842\n",
      "Train Epoch: 5888/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005841\n",
      "Train Epoch: 5889/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005842\n",
      "Train Epoch: 5890/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005843\n",
      "Train Epoch: 5891/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005844\n",
      "Train Epoch: 5892/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005844\n",
      "Train Epoch: 5893/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005844\n",
      "Train Epoch: 5894/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005842\n",
      "Train Epoch: 5895/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005840\n",
      "Train Epoch: 5896/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005841\n",
      "Train Epoch: 5897/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005843\n",
      "Train Epoch: 5898/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005844\n",
      "Train Epoch: 5899/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005844\n",
      "Train Epoch: 5900/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005843\n",
      "Train Epoch: 5901/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005842\n",
      "Train Epoch: 5902/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005840\n",
      "Train Epoch: 5903/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005840\n",
      "Train Epoch: 5904/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005841\n",
      "Train Epoch: 5905/10000 (59%)\ttrain_Loss: 0.005158\tval_Loss: 0.005842\n",
      "Train Epoch: 5906/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005843\n",
      "Train Epoch: 5907/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005842\n",
      "Train Epoch: 5908/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005841\n",
      "Train Epoch: 5909/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005840\n",
      "Train Epoch: 5910/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005840\n",
      "Train Epoch: 5911/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005841\n",
      "Train Epoch: 5912/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005841\n",
      "Train Epoch: 5913/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005841\n",
      "Train Epoch: 5914/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005841\n",
      "Train Epoch: 5915/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005841\n",
      "Train Epoch: 5916/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005841\n",
      "Train Epoch: 5917/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005841\n",
      "Train Epoch: 5918/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005841\n",
      "Train Epoch: 5919/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005841\n",
      "Train Epoch: 5920/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005840\n",
      "Train Epoch: 5921/10000 (59%)\ttrain_Loss: 0.005157\tval_Loss: 0.005840\n",
      "Train Epoch: 5922/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005840\n",
      "Train Epoch: 5923/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005840\n",
      "Train Epoch: 5924/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005840\n",
      "Train Epoch: 5925/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005840\n",
      "Train Epoch: 5926/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005840\n",
      "Train Epoch: 5927/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005839\n",
      "Train Epoch: 5928/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005839\n",
      "Train Epoch: 5929/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005839\n",
      "Train Epoch: 5930/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005840\n",
      "Train Epoch: 5931/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005840\n",
      "Train Epoch: 5932/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005839\n",
      "Train Epoch: 5933/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005839\n",
      "Train Epoch: 5934/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005840\n",
      "Train Epoch: 5935/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005839\n",
      "Train Epoch: 5936/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005838\n",
      "Train Epoch: 5937/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005838\n",
      "Train Epoch: 5938/10000 (59%)\ttrain_Loss: 0.005156\tval_Loss: 0.005838\n",
      "Train Epoch: 5939/10000 (59%)\ttrain_Loss: 0.005155\tval_Loss: 0.005839\n",
      "Train Epoch: 5940/10000 (59%)\ttrain_Loss: 0.005155\tval_Loss: 0.005839\n",
      "Train Epoch: 5941/10000 (59%)\ttrain_Loss: 0.005155\tval_Loss: 0.005839\n",
      "Train Epoch: 5942/10000 (59%)\ttrain_Loss: 0.005155\tval_Loss: 0.005839\n",
      "Train Epoch: 5943/10000 (59%)\ttrain_Loss: 0.005155\tval_Loss: 0.005839\n",
      "Train Epoch: 5944/10000 (59%)\ttrain_Loss: 0.005155\tval_Loss: 0.005838\n",
      "Train Epoch: 5945/10000 (59%)\ttrain_Loss: 0.005155\tval_Loss: 0.005838\n",
      "Train Epoch: 5946/10000 (59%)\ttrain_Loss: 0.005155\tval_Loss: 0.005837\n",
      "Train Epoch: 5947/10000 (59%)\ttrain_Loss: 0.005155\tval_Loss: 0.005838\n",
      "Train Epoch: 5948/10000 (59%)\ttrain_Loss: 0.005155\tval_Loss: 0.005838\n",
      "Train Epoch: 5949/10000 (59%)\ttrain_Loss: 0.005155\tval_Loss: 0.005838\n",
      "Train Epoch: 5950/10000 (59%)\ttrain_Loss: 0.005155\tval_Loss: 0.005838\n",
      "Train Epoch: 5951/10000 (60%)\ttrain_Loss: 0.005155\tval_Loss: 0.005837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5952/10000 (60%)\ttrain_Loss: 0.005155\tval_Loss: 0.005837\n",
      "Train Epoch: 5953/10000 (60%)\ttrain_Loss: 0.005155\tval_Loss: 0.005837\n",
      "Train Epoch: 5954/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005837\n",
      "Train Epoch: 5955/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005836\n",
      "Train Epoch: 5956/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005837\n",
      "Train Epoch: 5957/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005837\n",
      "Train Epoch: 5958/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005838\n",
      "Train Epoch: 5959/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005837\n",
      "Train Epoch: 5960/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005836\n",
      "Train Epoch: 5961/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005836\n",
      "Train Epoch: 5962/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005837\n",
      "Train Epoch: 5963/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005837\n",
      "Train Epoch: 5964/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005836\n",
      "Train Epoch: 5965/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005835\n",
      "Train Epoch: 5966/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005835\n",
      "Train Epoch: 5967/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005836\n",
      "Train Epoch: 5968/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005837\n",
      "Train Epoch: 5969/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005837\n",
      "Train Epoch: 5970/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005835\n",
      "Train Epoch: 5971/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005834\n",
      "Train Epoch: 5972/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005835\n",
      "Train Epoch: 5973/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005836\n",
      "Train Epoch: 5974/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005836\n",
      "Train Epoch: 5975/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005836\n",
      "Train Epoch: 5976/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005835\n",
      "Train Epoch: 5977/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005835\n",
      "Train Epoch: 5978/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005835\n",
      "Train Epoch: 5979/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005835\n",
      "Train Epoch: 5980/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005835\n",
      "Train Epoch: 5981/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005835\n",
      "Train Epoch: 5982/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005834\n",
      "Train Epoch: 5983/10000 (60%)\ttrain_Loss: 0.005152\tval_Loss: 0.005833\n",
      "Train Epoch: 5984/10000 (60%)\ttrain_Loss: 0.005152\tval_Loss: 0.005846\n",
      "Train Epoch: 5985/10000 (60%)\ttrain_Loss: 0.005155\tval_Loss: 0.005857\n",
      "Train Epoch: 5986/10000 (60%)\ttrain_Loss: 0.005157\tval_Loss: 0.005819\n",
      "Train Epoch: 5987/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005806\n",
      "Train Epoch: 5988/10000 (60%)\ttrain_Loss: 0.005156\tval_Loss: 0.005839\n",
      "Train Epoch: 5989/10000 (60%)\ttrain_Loss: 0.005152\tval_Loss: 0.005860\n",
      "Train Epoch: 5990/10000 (60%)\ttrain_Loss: 0.005155\tval_Loss: 0.005832\n",
      "Train Epoch: 5991/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005824\n",
      "Train Epoch: 5992/10000 (60%)\ttrain_Loss: 0.005154\tval_Loss: 0.005845\n",
      "Train Epoch: 5993/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005845\n",
      "Train Epoch: 5994/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005818\n",
      "Train Epoch: 5995/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005819\n",
      "Train Epoch: 5996/10000 (60%)\ttrain_Loss: 0.005153\tval_Loss: 0.005844\n",
      "Train Epoch: 5997/10000 (60%)\ttrain_Loss: 0.005152\tval_Loss: 0.005845\n",
      "Train Epoch: 5998/10000 (60%)\ttrain_Loss: 0.005152\tval_Loss: 0.005828\n",
      "Train Epoch: 5999/10000 (60%)\ttrain_Loss: 0.005152\tval_Loss: 0.005832\n",
      "Train Epoch: 6000/10000 (60%)\ttrain_Loss: 0.005152\tval_Loss: 0.005847\n",
      "Train Epoch: 6001/10000 (60%)\ttrain_Loss: 0.005152\tval_Loss: 0.005837\n",
      "Train Epoch: 6002/10000 (60%)\ttrain_Loss: 0.005151\tval_Loss: 0.005821\n",
      "Train Epoch: 6003/10000 (60%)\ttrain_Loss: 0.005152\tval_Loss: 0.005828\n",
      "Train Epoch: 6004/10000 (60%)\ttrain_Loss: 0.005151\tval_Loss: 0.005840\n",
      "Train Epoch: 6005/10000 (60%)\ttrain_Loss: 0.005152\tval_Loss: 0.005831\n",
      "Train Epoch: 6006/10000 (60%)\ttrain_Loss: 0.005151\tval_Loss: 0.005823\n",
      "Train Epoch: 6007/10000 (60%)\ttrain_Loss: 0.005151\tval_Loss: 0.005832\n",
      "Train Epoch: 6008/10000 (60%)\ttrain_Loss: 0.005151\tval_Loss: 0.005840\n",
      "Train Epoch: 6009/10000 (60%)\ttrain_Loss: 0.005151\tval_Loss: 0.005831\n",
      "Train Epoch: 6010/10000 (60%)\ttrain_Loss: 0.005151\tval_Loss: 0.005827\n",
      "Train Epoch: 6011/10000 (60%)\ttrain_Loss: 0.005151\tval_Loss: 0.005836\n",
      "Train Epoch: 6012/10000 (60%)\ttrain_Loss: 0.005151\tval_Loss: 0.005837\n",
      "Train Epoch: 6013/10000 (60%)\ttrain_Loss: 0.005151\tval_Loss: 0.005827\n",
      "Train Epoch: 6014/10000 (60%)\ttrain_Loss: 0.005150\tval_Loss: 0.005826\n",
      "Train Epoch: 6015/10000 (60%)\ttrain_Loss: 0.005150\tval_Loss: 0.005833\n",
      "Train Epoch: 6016/10000 (60%)\ttrain_Loss: 0.005150\tval_Loss: 0.005833\n",
      "Train Epoch: 6017/10000 (60%)\ttrain_Loss: 0.005150\tval_Loss: 0.005826\n",
      "Train Epoch: 6018/10000 (60%)\ttrain_Loss: 0.005150\tval_Loss: 0.005829\n",
      "Train Epoch: 6019/10000 (60%)\ttrain_Loss: 0.005150\tval_Loss: 0.005836\n",
      "Train Epoch: 6020/10000 (60%)\ttrain_Loss: 0.005150\tval_Loss: 0.005833\n",
      "Train Epoch: 6021/10000 (60%)\ttrain_Loss: 0.005150\tval_Loss: 0.005827\n",
      "Train Epoch: 6022/10000 (60%)\ttrain_Loss: 0.005150\tval_Loss: 0.005829\n",
      "Train Epoch: 6023/10000 (60%)\ttrain_Loss: 0.005150\tval_Loss: 0.005833\n",
      "Train Epoch: 6024/10000 (60%)\ttrain_Loss: 0.005150\tval_Loss: 0.005830\n",
      "Train Epoch: 6025/10000 (60%)\ttrain_Loss: 0.005149\tval_Loss: 0.005827\n",
      "Train Epoch: 6026/10000 (60%)\ttrain_Loss: 0.005149\tval_Loss: 0.005830\n",
      "Train Epoch: 6027/10000 (60%)\ttrain_Loss: 0.005149\tval_Loss: 0.005832\n",
      "Train Epoch: 6028/10000 (60%)\ttrain_Loss: 0.005149\tval_Loss: 0.005829\n",
      "Train Epoch: 6029/10000 (60%)\ttrain_Loss: 0.005149\tval_Loss: 0.005828\n",
      "Train Epoch: 6030/10000 (60%)\ttrain_Loss: 0.005149\tval_Loss: 0.005830\n",
      "Train Epoch: 6031/10000 (60%)\ttrain_Loss: 0.005149\tval_Loss: 0.005830\n",
      "Train Epoch: 6032/10000 (60%)\ttrain_Loss: 0.005149\tval_Loss: 0.005827\n",
      "Train Epoch: 6033/10000 (60%)\ttrain_Loss: 0.005149\tval_Loss: 0.005827\n",
      "Train Epoch: 6034/10000 (60%)\ttrain_Loss: 0.005149\tval_Loss: 0.005830\n",
      "Train Epoch: 6035/10000 (60%)\ttrain_Loss: 0.005149\tval_Loss: 0.005831\n",
      "Train Epoch: 6036/10000 (60%)\ttrain_Loss: 0.005149\tval_Loss: 0.005829\n",
      "Train Epoch: 6037/10000 (60%)\ttrain_Loss: 0.005148\tval_Loss: 0.005828\n",
      "Train Epoch: 6038/10000 (60%)\ttrain_Loss: 0.005148\tval_Loss: 0.005828\n",
      "Train Epoch: 6039/10000 (60%)\ttrain_Loss: 0.005148\tval_Loss: 0.005827\n",
      "Train Epoch: 6040/10000 (60%)\ttrain_Loss: 0.005148\tval_Loss: 0.005827\n",
      "Train Epoch: 6041/10000 (60%)\ttrain_Loss: 0.005148\tval_Loss: 0.005828\n",
      "Train Epoch: 6042/10000 (60%)\ttrain_Loss: 0.005148\tval_Loss: 0.005829\n",
      "Train Epoch: 6043/10000 (60%)\ttrain_Loss: 0.005148\tval_Loss: 0.005828\n",
      "Train Epoch: 6044/10000 (60%)\ttrain_Loss: 0.005148\tval_Loss: 0.005828\n",
      "Train Epoch: 6045/10000 (60%)\ttrain_Loss: 0.005148\tval_Loss: 0.005828\n",
      "Train Epoch: 6046/10000 (60%)\ttrain_Loss: 0.005148\tval_Loss: 0.005828\n",
      "Train Epoch: 6047/10000 (60%)\ttrain_Loss: 0.005148\tval_Loss: 0.005827\n",
      "Train Epoch: 6048/10000 (60%)\ttrain_Loss: 0.005148\tval_Loss: 0.005827\n",
      "Train Epoch: 6049/10000 (60%)\ttrain_Loss: 0.005147\tval_Loss: 0.005827\n",
      "Train Epoch: 6050/10000 (60%)\ttrain_Loss: 0.005147\tval_Loss: 0.005827\n",
      "Train Epoch: 6051/10000 (60%)\ttrain_Loss: 0.005147\tval_Loss: 0.005826\n",
      "Train Epoch: 6052/10000 (61%)\ttrain_Loss: 0.005147\tval_Loss: 0.005827\n",
      "Train Epoch: 6053/10000 (61%)\ttrain_Loss: 0.005147\tval_Loss: 0.005827\n",
      "Train Epoch: 6054/10000 (61%)\ttrain_Loss: 0.005147\tval_Loss: 0.005827\n",
      "Train Epoch: 6055/10000 (61%)\ttrain_Loss: 0.005147\tval_Loss: 0.005826\n",
      "Train Epoch: 6056/10000 (61%)\ttrain_Loss: 0.005147\tval_Loss: 0.005826\n",
      "Train Epoch: 6057/10000 (61%)\ttrain_Loss: 0.005147\tval_Loss: 0.005826\n",
      "Train Epoch: 6058/10000 (61%)\ttrain_Loss: 0.005147\tval_Loss: 0.005826\n",
      "Train Epoch: 6059/10000 (61%)\ttrain_Loss: 0.005147\tval_Loss: 0.005825\n",
      "Train Epoch: 6060/10000 (61%)\ttrain_Loss: 0.005147\tval_Loss: 0.005825\n",
      "Train Epoch: 6061/10000 (61%)\ttrain_Loss: 0.005146\tval_Loss: 0.005826\n",
      "Train Epoch: 6062/10000 (61%)\ttrain_Loss: 0.005146\tval_Loss: 0.005826\n",
      "Train Epoch: 6063/10000 (61%)\ttrain_Loss: 0.005146\tval_Loss: 0.005826\n",
      "Train Epoch: 6064/10000 (61%)\ttrain_Loss: 0.005146\tval_Loss: 0.005825\n",
      "Train Epoch: 6065/10000 (61%)\ttrain_Loss: 0.005146\tval_Loss: 0.005824\n",
      "Train Epoch: 6066/10000 (61%)\ttrain_Loss: 0.005146\tval_Loss: 0.005824\n",
      "Train Epoch: 6067/10000 (61%)\ttrain_Loss: 0.005146\tval_Loss: 0.005824\n",
      "Train Epoch: 6068/10000 (61%)\ttrain_Loss: 0.005146\tval_Loss: 0.005825\n",
      "Train Epoch: 6069/10000 (61%)\ttrain_Loss: 0.005146\tval_Loss: 0.005826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6070/10000 (61%)\ttrain_Loss: 0.005146\tval_Loss: 0.005826\n",
      "Train Epoch: 6071/10000 (61%)\ttrain_Loss: 0.005146\tval_Loss: 0.005825\n",
      "Train Epoch: 6072/10000 (61%)\ttrain_Loss: 0.005146\tval_Loss: 0.005824\n",
      "Train Epoch: 6073/10000 (61%)\ttrain_Loss: 0.005145\tval_Loss: 0.005823\n",
      "Train Epoch: 6074/10000 (61%)\ttrain_Loss: 0.005145\tval_Loss: 0.005823\n",
      "Train Epoch: 6075/10000 (61%)\ttrain_Loss: 0.005145\tval_Loss: 0.005824\n",
      "Train Epoch: 6076/10000 (61%)\ttrain_Loss: 0.005145\tval_Loss: 0.005825\n",
      "Train Epoch: 6077/10000 (61%)\ttrain_Loss: 0.005145\tval_Loss: 0.005825\n",
      "Train Epoch: 6078/10000 (61%)\ttrain_Loss: 0.005145\tval_Loss: 0.005824\n",
      "Train Epoch: 6079/10000 (61%)\ttrain_Loss: 0.005145\tval_Loss: 0.005823\n",
      "Train Epoch: 6080/10000 (61%)\ttrain_Loss: 0.005145\tval_Loss: 0.005822\n",
      "Train Epoch: 6081/10000 (61%)\ttrain_Loss: 0.005145\tval_Loss: 0.005821\n",
      "Train Epoch: 6082/10000 (61%)\ttrain_Loss: 0.005145\tval_Loss: 0.005823\n",
      "Train Epoch: 6083/10000 (61%)\ttrain_Loss: 0.005145\tval_Loss: 0.005825\n",
      "Train Epoch: 6084/10000 (61%)\ttrain_Loss: 0.005145\tval_Loss: 0.005824\n",
      "Train Epoch: 6085/10000 (61%)\ttrain_Loss: 0.005144\tval_Loss: 0.005823\n",
      "Train Epoch: 6086/10000 (61%)\ttrain_Loss: 0.005144\tval_Loss: 0.005822\n",
      "Train Epoch: 6087/10000 (61%)\ttrain_Loss: 0.005144\tval_Loss: 0.005821\n",
      "Train Epoch: 6088/10000 (61%)\ttrain_Loss: 0.005144\tval_Loss: 0.005820\n",
      "Train Epoch: 6089/10000 (61%)\ttrain_Loss: 0.005144\tval_Loss: 0.005821\n",
      "Train Epoch: 6090/10000 (61%)\ttrain_Loss: 0.005144\tval_Loss: 0.005824\n",
      "Train Epoch: 6091/10000 (61%)\ttrain_Loss: 0.005144\tval_Loss: 0.005824\n",
      "Train Epoch: 6092/10000 (61%)\ttrain_Loss: 0.005144\tval_Loss: 0.005822\n",
      "Train Epoch: 6093/10000 (61%)\ttrain_Loss: 0.005144\tval_Loss: 0.005822\n",
      "Train Epoch: 6094/10000 (61%)\ttrain_Loss: 0.005144\tval_Loss: 0.005821\n",
      "Train Epoch: 6095/10000 (61%)\ttrain_Loss: 0.005144\tval_Loss: 0.005820\n",
      "Train Epoch: 6096/10000 (61%)\ttrain_Loss: 0.005143\tval_Loss: 0.005821\n",
      "Train Epoch: 6097/10000 (61%)\ttrain_Loss: 0.005143\tval_Loss: 0.005822\n",
      "Train Epoch: 6098/10000 (61%)\ttrain_Loss: 0.005143\tval_Loss: 0.005822\n",
      "Train Epoch: 6099/10000 (61%)\ttrain_Loss: 0.005143\tval_Loss: 0.005821\n",
      "Train Epoch: 6100/10000 (61%)\ttrain_Loss: 0.005143\tval_Loss: 0.005820\n",
      "Train Epoch: 6101/10000 (61%)\ttrain_Loss: 0.005143\tval_Loss: 0.005820\n",
      "Train Epoch: 6102/10000 (61%)\ttrain_Loss: 0.005143\tval_Loss: 0.005818\n",
      "Train Epoch: 6103/10000 (61%)\ttrain_Loss: 0.005143\tval_Loss: 0.005818\n",
      "Train Epoch: 6104/10000 (61%)\ttrain_Loss: 0.005143\tval_Loss: 0.005820\n",
      "Train Epoch: 6105/10000 (61%)\ttrain_Loss: 0.005143\tval_Loss: 0.005821\n",
      "Train Epoch: 6106/10000 (61%)\ttrain_Loss: 0.005142\tval_Loss: 0.005821\n",
      "Train Epoch: 6107/10000 (61%)\ttrain_Loss: 0.005142\tval_Loss: 0.005820\n",
      "Train Epoch: 6108/10000 (61%)\ttrain_Loss: 0.005142\tval_Loss: 0.005819\n",
      "Train Epoch: 6109/10000 (61%)\ttrain_Loss: 0.005142\tval_Loss: 0.005819\n",
      "Train Epoch: 6110/10000 (61%)\ttrain_Loss: 0.005142\tval_Loss: 0.005820\n",
      "Train Epoch: 6111/10000 (61%)\ttrain_Loss: 0.005142\tval_Loss: 0.005820\n",
      "Train Epoch: 6112/10000 (61%)\ttrain_Loss: 0.005142\tval_Loss: 0.005819\n",
      "Train Epoch: 6113/10000 (61%)\ttrain_Loss: 0.005142\tval_Loss: 0.005819\n",
      "Train Epoch: 6114/10000 (61%)\ttrain_Loss: 0.005142\tval_Loss: 0.005819\n",
      "Train Epoch: 6115/10000 (61%)\ttrain_Loss: 0.005142\tval_Loss: 0.005817\n",
      "Train Epoch: 6116/10000 (61%)\ttrain_Loss: 0.005142\tval_Loss: 0.005817\n",
      "Train Epoch: 6117/10000 (61%)\ttrain_Loss: 0.005141\tval_Loss: 0.005818\n",
      "Train Epoch: 6118/10000 (61%)\ttrain_Loss: 0.005141\tval_Loss: 0.005818\n",
      "Train Epoch: 6119/10000 (61%)\ttrain_Loss: 0.005141\tval_Loss: 0.005819\n",
      "Train Epoch: 6120/10000 (61%)\ttrain_Loss: 0.005141\tval_Loss: 0.005818\n",
      "Train Epoch: 6121/10000 (61%)\ttrain_Loss: 0.005141\tval_Loss: 0.005816\n",
      "Train Epoch: 6122/10000 (61%)\ttrain_Loss: 0.005141\tval_Loss: 0.005817\n",
      "Train Epoch: 6123/10000 (61%)\ttrain_Loss: 0.005141\tval_Loss: 0.005817\n",
      "Train Epoch: 6124/10000 (61%)\ttrain_Loss: 0.005141\tval_Loss: 0.005817\n",
      "Train Epoch: 6125/10000 (61%)\ttrain_Loss: 0.005141\tval_Loss: 0.005818\n",
      "Train Epoch: 6126/10000 (61%)\ttrain_Loss: 0.005141\tval_Loss: 0.005817\n",
      "Train Epoch: 6127/10000 (61%)\ttrain_Loss: 0.005140\tval_Loss: 0.005817\n",
      "Train Epoch: 6128/10000 (61%)\ttrain_Loss: 0.005140\tval_Loss: 0.005816\n",
      "Train Epoch: 6129/10000 (61%)\ttrain_Loss: 0.005140\tval_Loss: 0.005816\n",
      "Train Epoch: 6130/10000 (61%)\ttrain_Loss: 0.005140\tval_Loss: 0.005817\n",
      "Train Epoch: 6131/10000 (61%)\ttrain_Loss: 0.005140\tval_Loss: 0.005817\n",
      "Train Epoch: 6132/10000 (61%)\ttrain_Loss: 0.005140\tval_Loss: 0.005817\n",
      "Train Epoch: 6133/10000 (61%)\ttrain_Loss: 0.005140\tval_Loss: 0.005816\n",
      "Train Epoch: 6134/10000 (61%)\ttrain_Loss: 0.005140\tval_Loss: 0.005815\n",
      "Train Epoch: 6135/10000 (61%)\ttrain_Loss: 0.005140\tval_Loss: 0.005814\n",
      "Train Epoch: 6136/10000 (61%)\ttrain_Loss: 0.005140\tval_Loss: 0.005813\n",
      "Train Epoch: 6137/10000 (61%)\ttrain_Loss: 0.005139\tval_Loss: 0.005815\n",
      "Train Epoch: 6138/10000 (61%)\ttrain_Loss: 0.005139\tval_Loss: 0.005816\n",
      "Train Epoch: 6139/10000 (61%)\ttrain_Loss: 0.005139\tval_Loss: 0.005817\n",
      "Train Epoch: 6140/10000 (61%)\ttrain_Loss: 0.005139\tval_Loss: 0.005816\n",
      "Train Epoch: 6141/10000 (61%)\ttrain_Loss: 0.005139\tval_Loss: 0.005815\n",
      "Train Epoch: 6142/10000 (61%)\ttrain_Loss: 0.005139\tval_Loss: 0.005815\n",
      "Train Epoch: 6143/10000 (61%)\ttrain_Loss: 0.005139\tval_Loss: 0.005814\n",
      "Train Epoch: 6144/10000 (61%)\ttrain_Loss: 0.005139\tval_Loss: 0.005814\n",
      "Train Epoch: 6145/10000 (61%)\ttrain_Loss: 0.005139\tval_Loss: 0.005814\n",
      "Train Epoch: 6146/10000 (61%)\ttrain_Loss: 0.005139\tval_Loss: 0.005814\n",
      "Train Epoch: 6147/10000 (61%)\ttrain_Loss: 0.005138\tval_Loss: 0.005813\n",
      "Train Epoch: 6148/10000 (61%)\ttrain_Loss: 0.005138\tval_Loss: 0.005814\n",
      "Train Epoch: 6149/10000 (61%)\ttrain_Loss: 0.005138\tval_Loss: 0.005814\n",
      "Train Epoch: 6150/10000 (61%)\ttrain_Loss: 0.005138\tval_Loss: 0.005813\n",
      "Train Epoch: 6151/10000 (62%)\ttrain_Loss: 0.005138\tval_Loss: 0.005813\n",
      "Train Epoch: 6152/10000 (62%)\ttrain_Loss: 0.005138\tval_Loss: 0.005812\n",
      "Train Epoch: 6153/10000 (62%)\ttrain_Loss: 0.005138\tval_Loss: 0.005811\n",
      "Train Epoch: 6154/10000 (62%)\ttrain_Loss: 0.005138\tval_Loss: 0.005812\n",
      "Train Epoch: 6155/10000 (62%)\ttrain_Loss: 0.005138\tval_Loss: 0.005814\n",
      "Train Epoch: 6156/10000 (62%)\ttrain_Loss: 0.005137\tval_Loss: 0.005814\n",
      "Train Epoch: 6157/10000 (62%)\ttrain_Loss: 0.005137\tval_Loss: 0.005813\n",
      "Train Epoch: 6158/10000 (62%)\ttrain_Loss: 0.005137\tval_Loss: 0.005812\n",
      "Train Epoch: 6159/10000 (62%)\ttrain_Loss: 0.005137\tval_Loss: 0.005812\n",
      "Train Epoch: 6160/10000 (62%)\ttrain_Loss: 0.005137\tval_Loss: 0.005810\n",
      "Train Epoch: 6161/10000 (62%)\ttrain_Loss: 0.005137\tval_Loss: 0.005810\n",
      "Train Epoch: 6162/10000 (62%)\ttrain_Loss: 0.005137\tval_Loss: 0.005813\n",
      "Train Epoch: 6163/10000 (62%)\ttrain_Loss: 0.005137\tval_Loss: 0.005812\n",
      "Train Epoch: 6164/10000 (62%)\ttrain_Loss: 0.005137\tval_Loss: 0.005811\n",
      "Train Epoch: 6165/10000 (62%)\ttrain_Loss: 0.005137\tval_Loss: 0.005812\n",
      "Train Epoch: 6166/10000 (62%)\ttrain_Loss: 0.005136\tval_Loss: 0.005811\n",
      "Train Epoch: 6167/10000 (62%)\ttrain_Loss: 0.005136\tval_Loss: 0.005810\n",
      "Train Epoch: 6168/10000 (62%)\ttrain_Loss: 0.005136\tval_Loss: 0.005811\n",
      "Train Epoch: 6169/10000 (62%)\ttrain_Loss: 0.005136\tval_Loss: 0.005811\n",
      "Train Epoch: 6170/10000 (62%)\ttrain_Loss: 0.005136\tval_Loss: 0.005809\n",
      "Train Epoch: 6171/10000 (62%)\ttrain_Loss: 0.005136\tval_Loss: 0.005808\n",
      "Train Epoch: 6172/10000 (62%)\ttrain_Loss: 0.005136\tval_Loss: 0.005810\n",
      "Train Epoch: 6173/10000 (62%)\ttrain_Loss: 0.005136\tval_Loss: 0.005810\n",
      "Train Epoch: 6174/10000 (62%)\ttrain_Loss: 0.005135\tval_Loss: 0.005809\n",
      "Train Epoch: 6175/10000 (62%)\ttrain_Loss: 0.005135\tval_Loss: 0.005810\n",
      "Train Epoch: 6176/10000 (62%)\ttrain_Loss: 0.005135\tval_Loss: 0.005809\n",
      "Train Epoch: 6177/10000 (62%)\ttrain_Loss: 0.005135\tval_Loss: 0.005807\n",
      "Train Epoch: 6178/10000 (62%)\ttrain_Loss: 0.005135\tval_Loss: 0.005807\n",
      "Train Epoch: 6179/10000 (62%)\ttrain_Loss: 0.005135\tval_Loss: 0.005810\n",
      "Train Epoch: 6180/10000 (62%)\ttrain_Loss: 0.005135\tval_Loss: 0.005811\n",
      "Train Epoch: 6181/10000 (62%)\ttrain_Loss: 0.005135\tval_Loss: 0.005810\n",
      "Train Epoch: 6182/10000 (62%)\ttrain_Loss: 0.005135\tval_Loss: 0.005809\n",
      "Train Epoch: 6183/10000 (62%)\ttrain_Loss: 0.005134\tval_Loss: 0.005809\n",
      "Train Epoch: 6184/10000 (62%)\ttrain_Loss: 0.005134\tval_Loss: 0.005806\n",
      "Train Epoch: 6185/10000 (62%)\ttrain_Loss: 0.005134\tval_Loss: 0.005806\n",
      "Train Epoch: 6186/10000 (62%)\ttrain_Loss: 0.005134\tval_Loss: 0.005807\n",
      "Train Epoch: 6187/10000 (62%)\ttrain_Loss: 0.005134\tval_Loss: 0.005807\n",
      "Train Epoch: 6188/10000 (62%)\ttrain_Loss: 0.005134\tval_Loss: 0.005809\n",
      "Train Epoch: 6189/10000 (62%)\ttrain_Loss: 0.005134\tval_Loss: 0.005808\n",
      "Train Epoch: 6190/10000 (62%)\ttrain_Loss: 0.005134\tval_Loss: 0.005806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6191/10000 (62%)\ttrain_Loss: 0.005134\tval_Loss: 0.005806\n",
      "Train Epoch: 6192/10000 (62%)\ttrain_Loss: 0.005133\tval_Loss: 0.005805\n",
      "Train Epoch: 6193/10000 (62%)\ttrain_Loss: 0.005133\tval_Loss: 0.005805\n",
      "Train Epoch: 6194/10000 (62%)\ttrain_Loss: 0.005133\tval_Loss: 0.005807\n",
      "Train Epoch: 6195/10000 (62%)\ttrain_Loss: 0.005133\tval_Loss: 0.005808\n",
      "Train Epoch: 6196/10000 (62%)\ttrain_Loss: 0.005133\tval_Loss: 0.005806\n",
      "Train Epoch: 6197/10000 (62%)\ttrain_Loss: 0.005133\tval_Loss: 0.005805\n",
      "Train Epoch: 6198/10000 (62%)\ttrain_Loss: 0.005133\tval_Loss: 0.005806\n",
      "Train Epoch: 6199/10000 (62%)\ttrain_Loss: 0.005133\tval_Loss: 0.005805\n",
      "Train Epoch: 6200/10000 (62%)\ttrain_Loss: 0.005132\tval_Loss: 0.005804\n",
      "Train Epoch: 6201/10000 (62%)\ttrain_Loss: 0.005132\tval_Loss: 0.005805\n",
      "Train Epoch: 6202/10000 (62%)\ttrain_Loss: 0.005132\tval_Loss: 0.005806\n",
      "Train Epoch: 6203/10000 (62%)\ttrain_Loss: 0.005132\tval_Loss: 0.005805\n",
      "Train Epoch: 6204/10000 (62%)\ttrain_Loss: 0.005132\tval_Loss: 0.005804\n",
      "Train Epoch: 6205/10000 (62%)\ttrain_Loss: 0.005132\tval_Loss: 0.005804\n",
      "Train Epoch: 6206/10000 (62%)\ttrain_Loss: 0.005132\tval_Loss: 0.005803\n",
      "Train Epoch: 6207/10000 (62%)\ttrain_Loss: 0.005132\tval_Loss: 0.005804\n",
      "Train Epoch: 6208/10000 (62%)\ttrain_Loss: 0.005131\tval_Loss: 0.005805\n",
      "Train Epoch: 6209/10000 (62%)\ttrain_Loss: 0.005131\tval_Loss: 0.005804\n",
      "Train Epoch: 6210/10000 (62%)\ttrain_Loss: 0.005131\tval_Loss: 0.005803\n",
      "Train Epoch: 6211/10000 (62%)\ttrain_Loss: 0.005131\tval_Loss: 0.005802\n",
      "Train Epoch: 6212/10000 (62%)\ttrain_Loss: 0.005131\tval_Loss: 0.005803\n",
      "Train Epoch: 6213/10000 (62%)\ttrain_Loss: 0.005131\tval_Loss: 0.005803\n",
      "Train Epoch: 6214/10000 (62%)\ttrain_Loss: 0.005131\tval_Loss: 0.005803\n",
      "Train Epoch: 6215/10000 (62%)\ttrain_Loss: 0.005131\tval_Loss: 0.005803\n",
      "Train Epoch: 6216/10000 (62%)\ttrain_Loss: 0.005130\tval_Loss: 0.005802\n",
      "Train Epoch: 6217/10000 (62%)\ttrain_Loss: 0.005130\tval_Loss: 0.005802\n",
      "Train Epoch: 6218/10000 (62%)\ttrain_Loss: 0.005130\tval_Loss: 0.005802\n",
      "Train Epoch: 6219/10000 (62%)\ttrain_Loss: 0.005130\tval_Loss: 0.005801\n",
      "Train Epoch: 6220/10000 (62%)\ttrain_Loss: 0.005130\tval_Loss: 0.005802\n",
      "Train Epoch: 6221/10000 (62%)\ttrain_Loss: 0.005130\tval_Loss: 0.005800\n",
      "Train Epoch: 6222/10000 (62%)\ttrain_Loss: 0.005130\tval_Loss: 0.005801\n",
      "Train Epoch: 6223/10000 (62%)\ttrain_Loss: 0.005130\tval_Loss: 0.005801\n",
      "Train Epoch: 6224/10000 (62%)\ttrain_Loss: 0.005129\tval_Loss: 0.005801\n",
      "Train Epoch: 6225/10000 (62%)\ttrain_Loss: 0.005129\tval_Loss: 0.005801\n",
      "Train Epoch: 6226/10000 (62%)\ttrain_Loss: 0.005129\tval_Loss: 0.005800\n",
      "Train Epoch: 6227/10000 (62%)\ttrain_Loss: 0.005129\tval_Loss: 0.005798\n",
      "Train Epoch: 6228/10000 (62%)\ttrain_Loss: 0.005129\tval_Loss: 0.005799\n",
      "Train Epoch: 6229/10000 (62%)\ttrain_Loss: 0.005129\tval_Loss: 0.005800\n",
      "Train Epoch: 6230/10000 (62%)\ttrain_Loss: 0.005129\tval_Loss: 0.005800\n",
      "Train Epoch: 6231/10000 (62%)\ttrain_Loss: 0.005129\tval_Loss: 0.005801\n",
      "Train Epoch: 6232/10000 (62%)\ttrain_Loss: 0.005128\tval_Loss: 0.005801\n",
      "Train Epoch: 6233/10000 (62%)\ttrain_Loss: 0.005128\tval_Loss: 0.005801\n",
      "Train Epoch: 6234/10000 (62%)\ttrain_Loss: 0.005128\tval_Loss: 0.005800\n",
      "Train Epoch: 6235/10000 (62%)\ttrain_Loss: 0.005128\tval_Loss: 0.005797\n",
      "Train Epoch: 6236/10000 (62%)\ttrain_Loss: 0.005128\tval_Loss: 0.005797\n",
      "Train Epoch: 6237/10000 (62%)\ttrain_Loss: 0.005128\tval_Loss: 0.005798\n",
      "Train Epoch: 6238/10000 (62%)\ttrain_Loss: 0.005128\tval_Loss: 0.005797\n",
      "Train Epoch: 6239/10000 (62%)\ttrain_Loss: 0.005128\tval_Loss: 0.005797\n",
      "Train Epoch: 6240/10000 (62%)\ttrain_Loss: 0.005127\tval_Loss: 0.005798\n",
      "Train Epoch: 6241/10000 (62%)\ttrain_Loss: 0.005127\tval_Loss: 0.005798\n",
      "Train Epoch: 6242/10000 (62%)\ttrain_Loss: 0.005127\tval_Loss: 0.005797\n",
      "Train Epoch: 6243/10000 (62%)\ttrain_Loss: 0.005127\tval_Loss: 0.005797\n",
      "Train Epoch: 6244/10000 (62%)\ttrain_Loss: 0.005127\tval_Loss: 0.005798\n",
      "Train Epoch: 6245/10000 (62%)\ttrain_Loss: 0.005127\tval_Loss: 0.005796\n",
      "Train Epoch: 6246/10000 (62%)\ttrain_Loss: 0.005127\tval_Loss: 0.005796\n",
      "Train Epoch: 6247/10000 (62%)\ttrain_Loss: 0.005126\tval_Loss: 0.005796\n",
      "Train Epoch: 6248/10000 (62%)\ttrain_Loss: 0.005126\tval_Loss: 0.005796\n",
      "Train Epoch: 6249/10000 (62%)\ttrain_Loss: 0.005126\tval_Loss: 0.005797\n",
      "Train Epoch: 6250/10000 (62%)\ttrain_Loss: 0.005126\tval_Loss: 0.005796\n",
      "Train Epoch: 6251/10000 (62%)\ttrain_Loss: 0.005126\tval_Loss: 0.005796\n",
      "Train Epoch: 6252/10000 (63%)\ttrain_Loss: 0.005126\tval_Loss: 0.005795\n",
      "Train Epoch: 6253/10000 (63%)\ttrain_Loss: 0.005126\tval_Loss: 0.005794\n",
      "Train Epoch: 6254/10000 (63%)\ttrain_Loss: 0.005125\tval_Loss: 0.005795\n",
      "Train Epoch: 6255/10000 (63%)\ttrain_Loss: 0.005125\tval_Loss: 0.005794\n",
      "Train Epoch: 6256/10000 (63%)\ttrain_Loss: 0.005125\tval_Loss: 0.005794\n",
      "Train Epoch: 6257/10000 (63%)\ttrain_Loss: 0.005125\tval_Loss: 0.005795\n",
      "Train Epoch: 6258/10000 (63%)\ttrain_Loss: 0.005125\tval_Loss: 0.005795\n",
      "Train Epoch: 6259/10000 (63%)\ttrain_Loss: 0.005125\tval_Loss: 0.005794\n",
      "Train Epoch: 6260/10000 (63%)\ttrain_Loss: 0.005125\tval_Loss: 0.005793\n",
      "Train Epoch: 6261/10000 (63%)\ttrain_Loss: 0.005125\tval_Loss: 0.005791\n",
      "Train Epoch: 6262/10000 (63%)\ttrain_Loss: 0.005124\tval_Loss: 0.005791\n",
      "Train Epoch: 6263/10000 (63%)\ttrain_Loss: 0.005124\tval_Loss: 0.005793\n",
      "Train Epoch: 6264/10000 (63%)\ttrain_Loss: 0.005124\tval_Loss: 0.005795\n",
      "Train Epoch: 6265/10000 (63%)\ttrain_Loss: 0.005124\tval_Loss: 0.005794\n",
      "Train Epoch: 6266/10000 (63%)\ttrain_Loss: 0.005124\tval_Loss: 0.005793\n",
      "Train Epoch: 6267/10000 (63%)\ttrain_Loss: 0.005124\tval_Loss: 0.005794\n",
      "Train Epoch: 6268/10000 (63%)\ttrain_Loss: 0.005124\tval_Loss: 0.005792\n",
      "Train Epoch: 6269/10000 (63%)\ttrain_Loss: 0.005123\tval_Loss: 0.005791\n",
      "Train Epoch: 6270/10000 (63%)\ttrain_Loss: 0.005123\tval_Loss: 0.005792\n",
      "Train Epoch: 6271/10000 (63%)\ttrain_Loss: 0.005123\tval_Loss: 0.005792\n",
      "Train Epoch: 6272/10000 (63%)\ttrain_Loss: 0.005123\tval_Loss: 0.005791\n",
      "Train Epoch: 6273/10000 (63%)\ttrain_Loss: 0.005123\tval_Loss: 0.005791\n",
      "Train Epoch: 6274/10000 (63%)\ttrain_Loss: 0.005123\tval_Loss: 0.005791\n",
      "Train Epoch: 6275/10000 (63%)\ttrain_Loss: 0.005123\tval_Loss: 0.005788\n",
      "Train Epoch: 6276/10000 (63%)\ttrain_Loss: 0.005122\tval_Loss: 0.005789\n",
      "Train Epoch: 6277/10000 (63%)\ttrain_Loss: 0.005122\tval_Loss: 0.005792\n",
      "Train Epoch: 6278/10000 (63%)\ttrain_Loss: 0.005122\tval_Loss: 0.005789\n",
      "Train Epoch: 6279/10000 (63%)\ttrain_Loss: 0.005122\tval_Loss: 0.005789\n",
      "Train Epoch: 6280/10000 (63%)\ttrain_Loss: 0.005122\tval_Loss: 0.005790\n",
      "Train Epoch: 6281/10000 (63%)\ttrain_Loss: 0.005122\tval_Loss: 0.005789\n",
      "Train Epoch: 6282/10000 (63%)\ttrain_Loss: 0.005121\tval_Loss: 0.005789\n",
      "Train Epoch: 6283/10000 (63%)\ttrain_Loss: 0.005121\tval_Loss: 0.005789\n",
      "Train Epoch: 6284/10000 (63%)\ttrain_Loss: 0.005121\tval_Loss: 0.005788\n",
      "Train Epoch: 6285/10000 (63%)\ttrain_Loss: 0.005121\tval_Loss: 0.005790\n",
      "Train Epoch: 6286/10000 (63%)\ttrain_Loss: 0.005121\tval_Loss: 0.005789\n",
      "Train Epoch: 6287/10000 (63%)\ttrain_Loss: 0.005121\tval_Loss: 0.005785\n",
      "Train Epoch: 6288/10000 (63%)\ttrain_Loss: 0.005121\tval_Loss: 0.005788\n",
      "Train Epoch: 6289/10000 (63%)\ttrain_Loss: 0.005120\tval_Loss: 0.005787\n",
      "Train Epoch: 6290/10000 (63%)\ttrain_Loss: 0.005120\tval_Loss: 0.005787\n",
      "Train Epoch: 6291/10000 (63%)\ttrain_Loss: 0.005120\tval_Loss: 0.005788\n",
      "Train Epoch: 6292/10000 (63%)\ttrain_Loss: 0.005120\tval_Loss: 0.005787\n",
      "Train Epoch: 6293/10000 (63%)\ttrain_Loss: 0.005120\tval_Loss: 0.005785\n",
      "Train Epoch: 6294/10000 (63%)\ttrain_Loss: 0.005120\tval_Loss: 0.005786\n",
      "Train Epoch: 6295/10000 (63%)\ttrain_Loss: 0.005120\tval_Loss: 0.005787\n",
      "Train Epoch: 6296/10000 (63%)\ttrain_Loss: 0.005119\tval_Loss: 0.005786\n",
      "Train Epoch: 6297/10000 (63%)\ttrain_Loss: 0.005119\tval_Loss: 0.005786\n",
      "Train Epoch: 6298/10000 (63%)\ttrain_Loss: 0.005119\tval_Loss: 0.005786\n",
      "Train Epoch: 6299/10000 (63%)\ttrain_Loss: 0.005119\tval_Loss: 0.005786\n",
      "Train Epoch: 6300/10000 (63%)\ttrain_Loss: 0.005119\tval_Loss: 0.005785\n",
      "Train Epoch: 6301/10000 (63%)\ttrain_Loss: 0.005119\tval_Loss: 0.005784\n",
      "Train Epoch: 6302/10000 (63%)\ttrain_Loss: 0.005119\tval_Loss: 0.005784\n",
      "Train Epoch: 6303/10000 (63%)\ttrain_Loss: 0.005118\tval_Loss: 0.005784\n",
      "Train Epoch: 6304/10000 (63%)\ttrain_Loss: 0.005118\tval_Loss: 0.005785\n",
      "Train Epoch: 6305/10000 (63%)\ttrain_Loss: 0.005118\tval_Loss: 0.005784\n",
      "Train Epoch: 6306/10000 (63%)\ttrain_Loss: 0.005118\tval_Loss: 0.005784\n",
      "Train Epoch: 6307/10000 (63%)\ttrain_Loss: 0.005118\tval_Loss: 0.005784\n",
      "Train Epoch: 6308/10000 (63%)\ttrain_Loss: 0.005118\tval_Loss: 0.005783\n",
      "Train Epoch: 6309/10000 (63%)\ttrain_Loss: 0.005117\tval_Loss: 0.005781\n",
      "Train Epoch: 6310/10000 (63%)\ttrain_Loss: 0.005117\tval_Loss: 0.005783\n",
      "Train Epoch: 6311/10000 (63%)\ttrain_Loss: 0.005117\tval_Loss: 0.005782\n",
      "Train Epoch: 6312/10000 (63%)\ttrain_Loss: 0.005117\tval_Loss: 0.005782\n",
      "Train Epoch: 6313/10000 (63%)\ttrain_Loss: 0.005117\tval_Loss: 0.005782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6314/10000 (63%)\ttrain_Loss: 0.005117\tval_Loss: 0.005781\n",
      "Train Epoch: 6315/10000 (63%)\ttrain_Loss: 0.005116\tval_Loss: 0.005781\n",
      "Train Epoch: 6316/10000 (63%)\ttrain_Loss: 0.005116\tval_Loss: 0.005780\n",
      "Train Epoch: 6317/10000 (63%)\ttrain_Loss: 0.005116\tval_Loss: 0.005781\n",
      "Train Epoch: 6318/10000 (63%)\ttrain_Loss: 0.005116\tval_Loss: 0.005782\n",
      "Train Epoch: 6319/10000 (63%)\ttrain_Loss: 0.005116\tval_Loss: 0.005780\n",
      "Train Epoch: 6320/10000 (63%)\ttrain_Loss: 0.005116\tval_Loss: 0.005780\n",
      "Train Epoch: 6321/10000 (63%)\ttrain_Loss: 0.005115\tval_Loss: 0.005782\n",
      "Train Epoch: 6322/10000 (63%)\ttrain_Loss: 0.005115\tval_Loss: 0.005779\n",
      "Train Epoch: 6323/10000 (63%)\ttrain_Loss: 0.005115\tval_Loss: 0.005777\n",
      "Train Epoch: 6324/10000 (63%)\ttrain_Loss: 0.005115\tval_Loss: 0.005781\n",
      "Train Epoch: 6325/10000 (63%)\ttrain_Loss: 0.005115\tval_Loss: 0.005778\n",
      "Train Epoch: 6326/10000 (63%)\ttrain_Loss: 0.005115\tval_Loss: 0.005777\n",
      "Train Epoch: 6327/10000 (63%)\ttrain_Loss: 0.005114\tval_Loss: 0.005780\n",
      "Train Epoch: 6328/10000 (63%)\ttrain_Loss: 0.005114\tval_Loss: 0.005777\n",
      "Train Epoch: 6329/10000 (63%)\ttrain_Loss: 0.005114\tval_Loss: 0.005778\n",
      "Train Epoch: 6330/10000 (63%)\ttrain_Loss: 0.005114\tval_Loss: 0.005780\n",
      "Train Epoch: 6331/10000 (63%)\ttrain_Loss: 0.005114\tval_Loss: 0.005776\n",
      "Train Epoch: 6332/10000 (63%)\ttrain_Loss: 0.005114\tval_Loss: 0.005778\n",
      "Train Epoch: 6333/10000 (63%)\ttrain_Loss: 0.005113\tval_Loss: 0.005780\n",
      "Train Epoch: 6334/10000 (63%)\ttrain_Loss: 0.005113\tval_Loss: 0.005775\n",
      "Train Epoch: 6335/10000 (63%)\ttrain_Loss: 0.005113\tval_Loss: 0.005775\n",
      "Train Epoch: 6336/10000 (63%)\ttrain_Loss: 0.005113\tval_Loss: 0.005777\n",
      "Train Epoch: 6337/10000 (63%)\ttrain_Loss: 0.005113\tval_Loss: 0.005773\n",
      "Train Epoch: 6338/10000 (63%)\ttrain_Loss: 0.005113\tval_Loss: 0.005775\n",
      "Train Epoch: 6339/10000 (63%)\ttrain_Loss: 0.005112\tval_Loss: 0.005778\n",
      "Train Epoch: 6340/10000 (63%)\ttrain_Loss: 0.005112\tval_Loss: 0.005776\n",
      "Train Epoch: 6341/10000 (63%)\ttrain_Loss: 0.005112\tval_Loss: 0.005775\n",
      "Train Epoch: 6342/10000 (63%)\ttrain_Loss: 0.005112\tval_Loss: 0.005777\n",
      "Train Epoch: 6343/10000 (63%)\ttrain_Loss: 0.005112\tval_Loss: 0.005775\n",
      "Train Epoch: 6344/10000 (63%)\ttrain_Loss: 0.005112\tval_Loss: 0.005773\n",
      "Train Epoch: 6345/10000 (63%)\ttrain_Loss: 0.005111\tval_Loss: 0.005773\n",
      "Train Epoch: 6346/10000 (63%)\ttrain_Loss: 0.005111\tval_Loss: 0.005773\n",
      "Train Epoch: 6347/10000 (63%)\ttrain_Loss: 0.005111\tval_Loss: 0.005773\n",
      "Train Epoch: 6348/10000 (63%)\ttrain_Loss: 0.005111\tval_Loss: 0.005774\n",
      "Train Epoch: 6349/10000 (63%)\ttrain_Loss: 0.005111\tval_Loss: 0.005773\n",
      "Train Epoch: 6350/10000 (63%)\ttrain_Loss: 0.005111\tval_Loss: 0.005771\n",
      "Train Epoch: 6351/10000 (64%)\ttrain_Loss: 0.005110\tval_Loss: 0.005772\n",
      "Train Epoch: 6352/10000 (64%)\ttrain_Loss: 0.005110\tval_Loss: 0.005771\n",
      "Train Epoch: 6353/10000 (64%)\ttrain_Loss: 0.005110\tval_Loss: 0.005771\n",
      "Train Epoch: 6354/10000 (64%)\ttrain_Loss: 0.005110\tval_Loss: 0.005774\n",
      "Train Epoch: 6355/10000 (64%)\ttrain_Loss: 0.005110\tval_Loss: 0.005771\n",
      "Train Epoch: 6356/10000 (64%)\ttrain_Loss: 0.005110\tval_Loss: 0.005770\n",
      "Train Epoch: 6357/10000 (64%)\ttrain_Loss: 0.005109\tval_Loss: 0.005771\n",
      "Train Epoch: 6358/10000 (64%)\ttrain_Loss: 0.005109\tval_Loss: 0.005771\n",
      "Train Epoch: 6359/10000 (64%)\ttrain_Loss: 0.005109\tval_Loss: 0.005772\n",
      "Train Epoch: 6360/10000 (64%)\ttrain_Loss: 0.005109\tval_Loss: 0.005772\n",
      "Train Epoch: 6361/10000 (64%)\ttrain_Loss: 0.005109\tval_Loss: 0.005770\n",
      "Train Epoch: 6362/10000 (64%)\ttrain_Loss: 0.005108\tval_Loss: 0.005769\n",
      "Train Epoch: 6363/10000 (64%)\ttrain_Loss: 0.005108\tval_Loss: 0.005769\n",
      "Train Epoch: 6364/10000 (64%)\ttrain_Loss: 0.005108\tval_Loss: 0.005770\n",
      "Train Epoch: 6365/10000 (64%)\ttrain_Loss: 0.005108\tval_Loss: 0.005768\n",
      "Train Epoch: 6366/10000 (64%)\ttrain_Loss: 0.005108\tval_Loss: 0.005768\n",
      "Train Epoch: 6367/10000 (64%)\ttrain_Loss: 0.005108\tval_Loss: 0.005768\n",
      "Train Epoch: 6368/10000 (64%)\ttrain_Loss: 0.005107\tval_Loss: 0.005766\n",
      "Train Epoch: 6369/10000 (64%)\ttrain_Loss: 0.005107\tval_Loss: 0.005768\n",
      "Train Epoch: 6370/10000 (64%)\ttrain_Loss: 0.005107\tval_Loss: 0.005769\n",
      "Train Epoch: 6371/10000 (64%)\ttrain_Loss: 0.005107\tval_Loss: 0.005767\n",
      "Train Epoch: 6372/10000 (64%)\ttrain_Loss: 0.005107\tval_Loss: 0.005767\n",
      "Train Epoch: 6373/10000 (64%)\ttrain_Loss: 0.005106\tval_Loss: 0.005766\n",
      "Train Epoch: 6374/10000 (64%)\ttrain_Loss: 0.005106\tval_Loss: 0.005765\n",
      "Train Epoch: 6375/10000 (64%)\ttrain_Loss: 0.005106\tval_Loss: 0.005765\n",
      "Train Epoch: 6376/10000 (64%)\ttrain_Loss: 0.005106\tval_Loss: 0.005764\n",
      "Train Epoch: 6377/10000 (64%)\ttrain_Loss: 0.005106\tval_Loss: 0.005765\n",
      "Train Epoch: 6378/10000 (64%)\ttrain_Loss: 0.005106\tval_Loss: 0.005766\n",
      "Train Epoch: 6379/10000 (64%)\ttrain_Loss: 0.005105\tval_Loss: 0.005766\n",
      "Train Epoch: 6380/10000 (64%)\ttrain_Loss: 0.005105\tval_Loss: 0.005765\n",
      "Train Epoch: 6381/10000 (64%)\ttrain_Loss: 0.005105\tval_Loss: 0.005765\n",
      "Train Epoch: 6382/10000 (64%)\ttrain_Loss: 0.005105\tval_Loss: 0.005765\n",
      "Train Epoch: 6383/10000 (64%)\ttrain_Loss: 0.005105\tval_Loss: 0.005763\n",
      "Train Epoch: 6384/10000 (64%)\ttrain_Loss: 0.005104\tval_Loss: 0.005762\n",
      "Train Epoch: 6385/10000 (64%)\ttrain_Loss: 0.005104\tval_Loss: 0.005763\n",
      "Train Epoch: 6386/10000 (64%)\ttrain_Loss: 0.005104\tval_Loss: 0.005762\n",
      "Train Epoch: 6387/10000 (64%)\ttrain_Loss: 0.005104\tval_Loss: 0.005763\n",
      "Train Epoch: 6388/10000 (64%)\ttrain_Loss: 0.005104\tval_Loss: 0.005763\n",
      "Train Epoch: 6389/10000 (64%)\ttrain_Loss: 0.005103\tval_Loss: 0.005762\n",
      "Train Epoch: 6390/10000 (64%)\ttrain_Loss: 0.005103\tval_Loss: 0.005763\n",
      "Train Epoch: 6391/10000 (64%)\ttrain_Loss: 0.005103\tval_Loss: 0.005761\n",
      "Train Epoch: 6392/10000 (64%)\ttrain_Loss: 0.005103\tval_Loss: 0.005761\n",
      "Train Epoch: 6393/10000 (64%)\ttrain_Loss: 0.005103\tval_Loss: 0.005762\n",
      "Train Epoch: 6394/10000 (64%)\ttrain_Loss: 0.005102\tval_Loss: 0.005759\n",
      "Train Epoch: 6395/10000 (64%)\ttrain_Loss: 0.005102\tval_Loss: 0.005759\n",
      "Train Epoch: 6396/10000 (64%)\ttrain_Loss: 0.005102\tval_Loss: 0.005759\n",
      "Train Epoch: 6397/10000 (64%)\ttrain_Loss: 0.005102\tval_Loss: 0.005759\n",
      "Train Epoch: 6398/10000 (64%)\ttrain_Loss: 0.005102\tval_Loss: 0.005762\n",
      "Train Epoch: 6399/10000 (64%)\ttrain_Loss: 0.005101\tval_Loss: 0.005760\n",
      "Train Epoch: 6400/10000 (64%)\ttrain_Loss: 0.005101\tval_Loss: 0.005759\n",
      "Train Epoch: 6401/10000 (64%)\ttrain_Loss: 0.005101\tval_Loss: 0.005758\n",
      "Train Epoch: 6402/10000 (64%)\ttrain_Loss: 0.005101\tval_Loss: 0.005758\n",
      "Train Epoch: 6403/10000 (64%)\ttrain_Loss: 0.005101\tval_Loss: 0.005758\n",
      "Train Epoch: 6404/10000 (64%)\ttrain_Loss: 0.005100\tval_Loss: 0.005757\n",
      "Train Epoch: 6405/10000 (64%)\ttrain_Loss: 0.005100\tval_Loss: 0.005756\n",
      "Train Epoch: 6406/10000 (64%)\ttrain_Loss: 0.005100\tval_Loss: 0.005757\n",
      "Train Epoch: 6407/10000 (64%)\ttrain_Loss: 0.005100\tval_Loss: 0.005754\n",
      "Train Epoch: 6408/10000 (64%)\ttrain_Loss: 0.005100\tval_Loss: 0.005756\n",
      "Train Epoch: 6409/10000 (64%)\ttrain_Loss: 0.005099\tval_Loss: 0.005757\n",
      "Train Epoch: 6410/10000 (64%)\ttrain_Loss: 0.005099\tval_Loss: 0.005756\n",
      "Train Epoch: 6411/10000 (64%)\ttrain_Loss: 0.005099\tval_Loss: 0.005757\n",
      "Train Epoch: 6412/10000 (64%)\ttrain_Loss: 0.005099\tval_Loss: 0.005756\n",
      "Train Epoch: 6413/10000 (64%)\ttrain_Loss: 0.005099\tval_Loss: 0.005755\n",
      "Train Epoch: 6414/10000 (64%)\ttrain_Loss: 0.005098\tval_Loss: 0.005754\n",
      "Train Epoch: 6415/10000 (64%)\ttrain_Loss: 0.005098\tval_Loss: 0.005753\n",
      "Train Epoch: 6416/10000 (64%)\ttrain_Loss: 0.005098\tval_Loss: 0.005754\n",
      "Train Epoch: 6417/10000 (64%)\ttrain_Loss: 0.005098\tval_Loss: 0.005754\n",
      "Train Epoch: 6418/10000 (64%)\ttrain_Loss: 0.005098\tval_Loss: 0.005754\n",
      "Train Epoch: 6419/10000 (64%)\ttrain_Loss: 0.005097\tval_Loss: 0.005754\n",
      "Train Epoch: 6420/10000 (64%)\ttrain_Loss: 0.005097\tval_Loss: 0.005753\n",
      "Train Epoch: 6421/10000 (64%)\ttrain_Loss: 0.005097\tval_Loss: 0.005752\n",
      "Train Epoch: 6422/10000 (64%)\ttrain_Loss: 0.005097\tval_Loss: 0.005751\n",
      "Train Epoch: 6423/10000 (64%)\ttrain_Loss: 0.005097\tval_Loss: 0.005750\n",
      "Train Epoch: 6424/10000 (64%)\ttrain_Loss: 0.005096\tval_Loss: 0.005751\n",
      "Train Epoch: 6425/10000 (64%)\ttrain_Loss: 0.005096\tval_Loss: 0.005750\n",
      "Train Epoch: 6426/10000 (64%)\ttrain_Loss: 0.005096\tval_Loss: 0.005752\n",
      "Train Epoch: 6427/10000 (64%)\ttrain_Loss: 0.005096\tval_Loss: 0.005751\n",
      "Train Epoch: 6428/10000 (64%)\ttrain_Loss: 0.005096\tval_Loss: 0.005750\n",
      "Train Epoch: 6429/10000 (64%)\ttrain_Loss: 0.005095\tval_Loss: 0.005750\n",
      "Train Epoch: 6430/10000 (64%)\ttrain_Loss: 0.005095\tval_Loss: 0.005748\n",
      "Train Epoch: 6431/10000 (64%)\ttrain_Loss: 0.005095\tval_Loss: 0.005746\n",
      "Train Epoch: 6432/10000 (64%)\ttrain_Loss: 0.005095\tval_Loss: 0.005749\n",
      "Train Epoch: 6433/10000 (64%)\ttrain_Loss: 0.005095\tval_Loss: 0.005748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6434/10000 (64%)\ttrain_Loss: 0.005094\tval_Loss: 0.005749\n",
      "Train Epoch: 6435/10000 (64%)\ttrain_Loss: 0.005094\tval_Loss: 0.005750\n",
      "Train Epoch: 6436/10000 (64%)\ttrain_Loss: 0.005094\tval_Loss: 0.005748\n",
      "Train Epoch: 6437/10000 (64%)\ttrain_Loss: 0.005094\tval_Loss: 0.005747\n",
      "Train Epoch: 6438/10000 (64%)\ttrain_Loss: 0.005093\tval_Loss: 0.005744\n",
      "Train Epoch: 6439/10000 (64%)\ttrain_Loss: 0.005093\tval_Loss: 0.005746\n",
      "Train Epoch: 6440/10000 (64%)\ttrain_Loss: 0.005093\tval_Loss: 0.005747\n",
      "Train Epoch: 6441/10000 (64%)\ttrain_Loss: 0.005093\tval_Loss: 0.005744\n",
      "Train Epoch: 6442/10000 (64%)\ttrain_Loss: 0.005093\tval_Loss: 0.005746\n",
      "Train Epoch: 6443/10000 (64%)\ttrain_Loss: 0.005092\tval_Loss: 0.005745\n",
      "Train Epoch: 6444/10000 (64%)\ttrain_Loss: 0.005092\tval_Loss: 0.005745\n",
      "Train Epoch: 6445/10000 (64%)\ttrain_Loss: 0.005092\tval_Loss: 0.005744\n",
      "Train Epoch: 6446/10000 (64%)\ttrain_Loss: 0.005092\tval_Loss: 0.005746\n",
      "Train Epoch: 6447/10000 (64%)\ttrain_Loss: 0.005091\tval_Loss: 0.005743\n",
      "Train Epoch: 6448/10000 (64%)\ttrain_Loss: 0.005091\tval_Loss: 0.005745\n",
      "Train Epoch: 6449/10000 (64%)\ttrain_Loss: 0.005091\tval_Loss: 0.005743\n",
      "Train Epoch: 6450/10000 (64%)\ttrain_Loss: 0.005091\tval_Loss: 0.005741\n",
      "Train Epoch: 6451/10000 (64%)\ttrain_Loss: 0.005091\tval_Loss: 0.005741\n",
      "Train Epoch: 6452/10000 (65%)\ttrain_Loss: 0.005090\tval_Loss: 0.005741\n",
      "Train Epoch: 6453/10000 (65%)\ttrain_Loss: 0.005090\tval_Loss: 0.005742\n",
      "Train Epoch: 6454/10000 (65%)\ttrain_Loss: 0.005090\tval_Loss: 0.005741\n",
      "Train Epoch: 6455/10000 (65%)\ttrain_Loss: 0.005090\tval_Loss: 0.005741\n",
      "Train Epoch: 6456/10000 (65%)\ttrain_Loss: 0.005089\tval_Loss: 0.005741\n",
      "Train Epoch: 6457/10000 (65%)\ttrain_Loss: 0.005089\tval_Loss: 0.005738\n",
      "Train Epoch: 6458/10000 (65%)\ttrain_Loss: 0.005089\tval_Loss: 0.005740\n",
      "Train Epoch: 6459/10000 (65%)\ttrain_Loss: 0.005089\tval_Loss: 0.005740\n",
      "Train Epoch: 6460/10000 (65%)\ttrain_Loss: 0.005089\tval_Loss: 0.005741\n",
      "Train Epoch: 6461/10000 (65%)\ttrain_Loss: 0.005088\tval_Loss: 0.005740\n",
      "Train Epoch: 6462/10000 (65%)\ttrain_Loss: 0.005088\tval_Loss: 0.005739\n",
      "Train Epoch: 6463/10000 (65%)\ttrain_Loss: 0.005088\tval_Loss: 0.005739\n",
      "Train Epoch: 6464/10000 (65%)\ttrain_Loss: 0.005088\tval_Loss: 0.005737\n",
      "Train Epoch: 6465/10000 (65%)\ttrain_Loss: 0.005087\tval_Loss: 0.005736\n",
      "Train Epoch: 6466/10000 (65%)\ttrain_Loss: 0.005087\tval_Loss: 0.005738\n",
      "Train Epoch: 6467/10000 (65%)\ttrain_Loss: 0.005087\tval_Loss: 0.005737\n",
      "Train Epoch: 6468/10000 (65%)\ttrain_Loss: 0.005087\tval_Loss: 0.005739\n",
      "Train Epoch: 6469/10000 (65%)\ttrain_Loss: 0.005086\tval_Loss: 0.005737\n",
      "Train Epoch: 6470/10000 (65%)\ttrain_Loss: 0.005086\tval_Loss: 0.005736\n",
      "Train Epoch: 6471/10000 (65%)\ttrain_Loss: 0.005086\tval_Loss: 0.005735\n",
      "Train Epoch: 6472/10000 (65%)\ttrain_Loss: 0.005086\tval_Loss: 0.005732\n",
      "Train Epoch: 6473/10000 (65%)\ttrain_Loss: 0.005085\tval_Loss: 0.005736\n",
      "Train Epoch: 6474/10000 (65%)\ttrain_Loss: 0.005085\tval_Loss: 0.005731\n",
      "Train Epoch: 6475/10000 (65%)\ttrain_Loss: 0.005085\tval_Loss: 0.005735\n",
      "Train Epoch: 6476/10000 (65%)\ttrain_Loss: 0.005085\tval_Loss: 0.005732\n",
      "Train Epoch: 6477/10000 (65%)\ttrain_Loss: 0.005085\tval_Loss: 0.005734\n",
      "Train Epoch: 6478/10000 (65%)\ttrain_Loss: 0.005084\tval_Loss: 0.005734\n",
      "Train Epoch: 6479/10000 (65%)\ttrain_Loss: 0.005084\tval_Loss: 0.005730\n",
      "Train Epoch: 6480/10000 (65%)\ttrain_Loss: 0.005084\tval_Loss: 0.005733\n",
      "Train Epoch: 6481/10000 (65%)\ttrain_Loss: 0.005084\tval_Loss: 0.005734\n",
      "Train Epoch: 6482/10000 (65%)\ttrain_Loss: 0.005083\tval_Loss: 0.005733\n",
      "Train Epoch: 6483/10000 (65%)\ttrain_Loss: 0.005083\tval_Loss: 0.005732\n",
      "Train Epoch: 6484/10000 (65%)\ttrain_Loss: 0.005083\tval_Loss: 0.005730\n",
      "Train Epoch: 6485/10000 (65%)\ttrain_Loss: 0.005083\tval_Loss: 0.005728\n",
      "Train Epoch: 6486/10000 (65%)\ttrain_Loss: 0.005082\tval_Loss: 0.005731\n",
      "Train Epoch: 6487/10000 (65%)\ttrain_Loss: 0.005082\tval_Loss: 0.005726\n",
      "Train Epoch: 6488/10000 (65%)\ttrain_Loss: 0.005082\tval_Loss: 0.005732\n",
      "Train Epoch: 6489/10000 (65%)\ttrain_Loss: 0.005082\tval_Loss: 0.005728\n",
      "Train Epoch: 6490/10000 (65%)\ttrain_Loss: 0.005081\tval_Loss: 0.005727\n",
      "Train Epoch: 6491/10000 (65%)\ttrain_Loss: 0.005081\tval_Loss: 0.005734\n",
      "Train Epoch: 6492/10000 (65%)\ttrain_Loss: 0.005081\tval_Loss: 0.005724\n",
      "Train Epoch: 6493/10000 (65%)\ttrain_Loss: 0.005081\tval_Loss: 0.005733\n",
      "Train Epoch: 6494/10000 (65%)\ttrain_Loss: 0.005081\tval_Loss: 0.005726\n",
      "Train Epoch: 6495/10000 (65%)\ttrain_Loss: 0.005080\tval_Loss: 0.005724\n",
      "Train Epoch: 6496/10000 (65%)\ttrain_Loss: 0.005080\tval_Loss: 0.005729\n",
      "Train Epoch: 6497/10000 (65%)\ttrain_Loss: 0.005080\tval_Loss: 0.005722\n",
      "Train Epoch: 6498/10000 (65%)\ttrain_Loss: 0.005080\tval_Loss: 0.005728\n",
      "Train Epoch: 6499/10000 (65%)\ttrain_Loss: 0.005079\tval_Loss: 0.005726\n",
      "Train Epoch: 6500/10000 (65%)\ttrain_Loss: 0.005079\tval_Loss: 0.005724\n",
      "Train Epoch: 6501/10000 (65%)\ttrain_Loss: 0.005079\tval_Loss: 0.005727\n",
      "Train Epoch: 6502/10000 (65%)\ttrain_Loss: 0.005079\tval_Loss: 0.005720\n",
      "Train Epoch: 6503/10000 (65%)\ttrain_Loss: 0.005078\tval_Loss: 0.005725\n",
      "Train Epoch: 6504/10000 (65%)\ttrain_Loss: 0.005078\tval_Loss: 0.005722\n",
      "Train Epoch: 6505/10000 (65%)\ttrain_Loss: 0.005078\tval_Loss: 0.005723\n",
      "Train Epoch: 6506/10000 (65%)\ttrain_Loss: 0.005077\tval_Loss: 0.005726\n",
      "Train Epoch: 6507/10000 (65%)\ttrain_Loss: 0.005077\tval_Loss: 0.005722\n",
      "Train Epoch: 6508/10000 (65%)\ttrain_Loss: 0.005077\tval_Loss: 0.005724\n",
      "Train Epoch: 6509/10000 (65%)\ttrain_Loss: 0.005077\tval_Loss: 0.005719\n",
      "Train Epoch: 6510/10000 (65%)\ttrain_Loss: 0.005076\tval_Loss: 0.005717\n",
      "Train Epoch: 6511/10000 (65%)\ttrain_Loss: 0.005076\tval_Loss: 0.005721\n",
      "Train Epoch: 6512/10000 (65%)\ttrain_Loss: 0.005076\tval_Loss: 0.005718\n",
      "Train Epoch: 6513/10000 (65%)\ttrain_Loss: 0.005076\tval_Loss: 0.005725\n",
      "Train Epoch: 6514/10000 (65%)\ttrain_Loss: 0.005076\tval_Loss: 0.005721\n",
      "Train Epoch: 6515/10000 (65%)\ttrain_Loss: 0.005075\tval_Loss: 0.005722\n",
      "Train Epoch: 6516/10000 (65%)\ttrain_Loss: 0.005075\tval_Loss: 0.005721\n",
      "Train Epoch: 6517/10000 (65%)\ttrain_Loss: 0.005075\tval_Loss: 0.005715\n",
      "Train Epoch: 6518/10000 (65%)\ttrain_Loss: 0.005074\tval_Loss: 0.005717\n",
      "Train Epoch: 6519/10000 (65%)\ttrain_Loss: 0.005074\tval_Loss: 0.005716\n",
      "Train Epoch: 6520/10000 (65%)\ttrain_Loss: 0.005074\tval_Loss: 0.005718\n",
      "Train Epoch: 6521/10000 (65%)\ttrain_Loss: 0.005074\tval_Loss: 0.005716\n",
      "Train Epoch: 6522/10000 (65%)\ttrain_Loss: 0.005073\tval_Loss: 0.005714\n",
      "Train Epoch: 6523/10000 (65%)\ttrain_Loss: 0.005073\tval_Loss: 0.005718\n",
      "Train Epoch: 6524/10000 (65%)\ttrain_Loss: 0.005073\tval_Loss: 0.005710\n",
      "Train Epoch: 6525/10000 (65%)\ttrain_Loss: 0.005073\tval_Loss: 0.005721\n",
      "Train Epoch: 6526/10000 (65%)\ttrain_Loss: 0.005073\tval_Loss: 0.005713\n",
      "Train Epoch: 6527/10000 (65%)\ttrain_Loss: 0.005072\tval_Loss: 0.005717\n",
      "Train Epoch: 6528/10000 (65%)\ttrain_Loss: 0.005072\tval_Loss: 0.005715\n",
      "Train Epoch: 6529/10000 (65%)\ttrain_Loss: 0.005072\tval_Loss: 0.005708\n",
      "Train Epoch: 6530/10000 (65%)\ttrain_Loss: 0.005071\tval_Loss: 0.005718\n",
      "Train Epoch: 6531/10000 (65%)\ttrain_Loss: 0.005071\tval_Loss: 0.005709\n",
      "Train Epoch: 6532/10000 (65%)\ttrain_Loss: 0.005071\tval_Loss: 0.005718\n",
      "Train Epoch: 6533/10000 (65%)\ttrain_Loss: 0.005071\tval_Loss: 0.005711\n",
      "Train Epoch: 6534/10000 (65%)\ttrain_Loss: 0.005070\tval_Loss: 0.005712\n",
      "Train Epoch: 6535/10000 (65%)\ttrain_Loss: 0.005070\tval_Loss: 0.005715\n",
      "Train Epoch: 6536/10000 (65%)\ttrain_Loss: 0.005070\tval_Loss: 0.005707\n",
      "Train Epoch: 6537/10000 (65%)\ttrain_Loss: 0.005069\tval_Loss: 0.005712\n",
      "Train Epoch: 6538/10000 (65%)\ttrain_Loss: 0.005069\tval_Loss: 0.005708\n",
      "Train Epoch: 6539/10000 (65%)\ttrain_Loss: 0.005069\tval_Loss: 0.005709\n",
      "Train Epoch: 6540/10000 (65%)\ttrain_Loss: 0.005069\tval_Loss: 0.005714\n",
      "Train Epoch: 6541/10000 (65%)\ttrain_Loss: 0.005068\tval_Loss: 0.005706\n",
      "Train Epoch: 6542/10000 (65%)\ttrain_Loss: 0.005068\tval_Loss: 0.005713\n",
      "Train Epoch: 6543/10000 (65%)\ttrain_Loss: 0.005068\tval_Loss: 0.005705\n",
      "Train Epoch: 6544/10000 (65%)\ttrain_Loss: 0.005068\tval_Loss: 0.005708\n",
      "Train Epoch: 6545/10000 (65%)\ttrain_Loss: 0.005067\tval_Loss: 0.005707\n",
      "Train Epoch: 6546/10000 (65%)\ttrain_Loss: 0.005067\tval_Loss: 0.005708\n",
      "Train Epoch: 6547/10000 (65%)\ttrain_Loss: 0.005067\tval_Loss: 0.005711\n",
      "Train Epoch: 6548/10000 (65%)\ttrain_Loss: 0.005066\tval_Loss: 0.005703\n",
      "Train Epoch: 6549/10000 (65%)\ttrain_Loss: 0.005066\tval_Loss: 0.005707\n",
      "Train Epoch: 6550/10000 (65%)\ttrain_Loss: 0.005066\tval_Loss: 0.005699\n",
      "Train Epoch: 6551/10000 (66%)\ttrain_Loss: 0.005066\tval_Loss: 0.005706\n",
      "Train Epoch: 6552/10000 (66%)\ttrain_Loss: 0.005065\tval_Loss: 0.005704\n",
      "Train Epoch: 6553/10000 (66%)\ttrain_Loss: 0.005065\tval_Loss: 0.005704\n",
      "Train Epoch: 6554/10000 (66%)\ttrain_Loss: 0.005065\tval_Loss: 0.005707\n",
      "Train Epoch: 6555/10000 (66%)\ttrain_Loss: 0.005065\tval_Loss: 0.005699\n",
      "Train Epoch: 6556/10000 (66%)\ttrain_Loss: 0.005064\tval_Loss: 0.005709\n",
      "Train Epoch: 6557/10000 (66%)\ttrain_Loss: 0.005064\tval_Loss: 0.005695\n",
      "Train Epoch: 6558/10000 (66%)\ttrain_Loss: 0.005064\tval_Loss: 0.005709\n",
      "Train Epoch: 6559/10000 (66%)\ttrain_Loss: 0.005064\tval_Loss: 0.005695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6560/10000 (66%)\ttrain_Loss: 0.005063\tval_Loss: 0.005704\n",
      "Train Epoch: 6561/10000 (66%)\ttrain_Loss: 0.005063\tval_Loss: 0.005700\n",
      "Train Epoch: 6562/10000 (66%)\ttrain_Loss: 0.005063\tval_Loss: 0.005701\n",
      "Train Epoch: 6563/10000 (66%)\ttrain_Loss: 0.005062\tval_Loss: 0.005705\n",
      "Train Epoch: 6564/10000 (66%)\ttrain_Loss: 0.005062\tval_Loss: 0.005694\n",
      "Train Epoch: 6565/10000 (66%)\ttrain_Loss: 0.005062\tval_Loss: 0.005705\n",
      "Train Epoch: 6566/10000 (66%)\ttrain_Loss: 0.005062\tval_Loss: 0.005691\n",
      "Train Epoch: 6567/10000 (66%)\ttrain_Loss: 0.005062\tval_Loss: 0.005706\n",
      "Train Epoch: 6568/10000 (66%)\ttrain_Loss: 0.005061\tval_Loss: 0.005690\n",
      "Train Epoch: 6569/10000 (66%)\ttrain_Loss: 0.005061\tval_Loss: 0.005706\n",
      "Train Epoch: 6570/10000 (66%)\ttrain_Loss: 0.005061\tval_Loss: 0.005693\n",
      "Train Epoch: 6571/10000 (66%)\ttrain_Loss: 0.005060\tval_Loss: 0.005700\n",
      "Train Epoch: 6572/10000 (66%)\ttrain_Loss: 0.005060\tval_Loss: 0.005695\n",
      "Train Epoch: 6573/10000 (66%)\ttrain_Loss: 0.005059\tval_Loss: 0.005694\n",
      "Train Epoch: 6574/10000 (66%)\ttrain_Loss: 0.005059\tval_Loss: 0.005700\n",
      "Train Epoch: 6575/10000 (66%)\ttrain_Loss: 0.005059\tval_Loss: 0.005689\n",
      "Train Epoch: 6576/10000 (66%)\ttrain_Loss: 0.005059\tval_Loss: 0.005701\n",
      "Train Epoch: 6577/10000 (66%)\ttrain_Loss: 0.005059\tval_Loss: 0.005688\n",
      "Train Epoch: 6578/10000 (66%)\ttrain_Loss: 0.005059\tval_Loss: 0.005704\n",
      "Train Epoch: 6579/10000 (66%)\ttrain_Loss: 0.005058\tval_Loss: 0.005686\n",
      "Train Epoch: 6580/10000 (66%)\ttrain_Loss: 0.005058\tval_Loss: 0.005702\n",
      "Train Epoch: 6581/10000 (66%)\ttrain_Loss: 0.005058\tval_Loss: 0.005685\n",
      "Train Epoch: 6582/10000 (66%)\ttrain_Loss: 0.005057\tval_Loss: 0.005694\n",
      "Train Epoch: 6583/10000 (66%)\ttrain_Loss: 0.005057\tval_Loss: 0.005688\n",
      "Train Epoch: 6584/10000 (66%)\ttrain_Loss: 0.005056\tval_Loss: 0.005693\n",
      "Train Epoch: 6585/10000 (66%)\ttrain_Loss: 0.005056\tval_Loss: 0.005690\n",
      "Train Epoch: 6586/10000 (66%)\ttrain_Loss: 0.005056\tval_Loss: 0.005693\n",
      "Train Epoch: 6587/10000 (66%)\ttrain_Loss: 0.005056\tval_Loss: 0.005688\n",
      "Train Epoch: 6588/10000 (66%)\ttrain_Loss: 0.005055\tval_Loss: 0.005690\n",
      "Train Epoch: 6589/10000 (66%)\ttrain_Loss: 0.005055\tval_Loss: 0.005687\n",
      "Train Epoch: 6590/10000 (66%)\ttrain_Loss: 0.005055\tval_Loss: 0.005691\n",
      "Train Epoch: 6591/10000 (66%)\ttrain_Loss: 0.005054\tval_Loss: 0.005689\n",
      "Train Epoch: 6592/10000 (66%)\ttrain_Loss: 0.005054\tval_Loss: 0.005688\n",
      "Train Epoch: 6593/10000 (66%)\ttrain_Loss: 0.005054\tval_Loss: 0.005687\n",
      "Train Epoch: 6594/10000 (66%)\ttrain_Loss: 0.005054\tval_Loss: 0.005683\n",
      "Train Epoch: 6595/10000 (66%)\ttrain_Loss: 0.005053\tval_Loss: 0.005692\n",
      "Train Epoch: 6596/10000 (66%)\ttrain_Loss: 0.005053\tval_Loss: 0.005678\n",
      "Train Epoch: 6597/10000 (66%)\ttrain_Loss: 0.005053\tval_Loss: 0.005700\n",
      "Train Epoch: 6598/10000 (66%)\ttrain_Loss: 0.005054\tval_Loss: 0.005675\n",
      "Train Epoch: 6599/10000 (66%)\ttrain_Loss: 0.005054\tval_Loss: 0.005700\n",
      "Train Epoch: 6600/10000 (66%)\ttrain_Loss: 0.005054\tval_Loss: 0.005672\n",
      "Train Epoch: 6601/10000 (66%)\ttrain_Loss: 0.005053\tval_Loss: 0.005697\n",
      "Train Epoch: 6602/10000 (66%)\ttrain_Loss: 0.005053\tval_Loss: 0.005672\n",
      "Train Epoch: 6603/10000 (66%)\ttrain_Loss: 0.005052\tval_Loss: 0.005696\n",
      "Train Epoch: 6604/10000 (66%)\ttrain_Loss: 0.005052\tval_Loss: 0.005675\n",
      "Train Epoch: 6605/10000 (66%)\ttrain_Loss: 0.005051\tval_Loss: 0.005693\n",
      "Train Epoch: 6606/10000 (66%)\ttrain_Loss: 0.005051\tval_Loss: 0.005676\n",
      "Train Epoch: 6607/10000 (66%)\ttrain_Loss: 0.005050\tval_Loss: 0.005685\n",
      "Train Epoch: 6608/10000 (66%)\ttrain_Loss: 0.005050\tval_Loss: 0.005680\n",
      "Train Epoch: 6609/10000 (66%)\ttrain_Loss: 0.005049\tval_Loss: 0.005678\n",
      "Train Epoch: 6610/10000 (66%)\ttrain_Loss: 0.005049\tval_Loss: 0.005687\n",
      "Train Epoch: 6611/10000 (66%)\ttrain_Loss: 0.005049\tval_Loss: 0.005673\n",
      "Train Epoch: 6612/10000 (66%)\ttrain_Loss: 0.005049\tval_Loss: 0.005692\n",
      "Train Epoch: 6613/10000 (66%)\ttrain_Loss: 0.005049\tval_Loss: 0.005671\n",
      "Train Epoch: 6614/10000 (66%)\ttrain_Loss: 0.005049\tval_Loss: 0.005694\n",
      "Train Epoch: 6615/10000 (66%)\ttrain_Loss: 0.005049\tval_Loss: 0.005669\n",
      "Train Epoch: 6616/10000 (66%)\ttrain_Loss: 0.005049\tval_Loss: 0.005687\n",
      "Train Epoch: 6617/10000 (66%)\ttrain_Loss: 0.005048\tval_Loss: 0.005670\n",
      "Train Epoch: 6618/10000 (66%)\ttrain_Loss: 0.005047\tval_Loss: 0.005680\n",
      "Train Epoch: 6619/10000 (66%)\ttrain_Loss: 0.005047\tval_Loss: 0.005673\n",
      "Train Epoch: 6620/10000 (66%)\ttrain_Loss: 0.005047\tval_Loss: 0.005677\n",
      "Train Epoch: 6621/10000 (66%)\ttrain_Loss: 0.005046\tval_Loss: 0.005676\n",
      "Train Epoch: 6622/10000 (66%)\ttrain_Loss: 0.005046\tval_Loss: 0.005673\n",
      "Train Epoch: 6623/10000 (66%)\ttrain_Loss: 0.005046\tval_Loss: 0.005681\n",
      "Train Epoch: 6624/10000 (66%)\ttrain_Loss: 0.005045\tval_Loss: 0.005671\n",
      "Train Epoch: 6625/10000 (66%)\ttrain_Loss: 0.005045\tval_Loss: 0.005685\n",
      "Train Epoch: 6626/10000 (66%)\ttrain_Loss: 0.005045\tval_Loss: 0.005669\n",
      "Train Epoch: 6627/10000 (66%)\ttrain_Loss: 0.005045\tval_Loss: 0.005684\n",
      "Train Epoch: 6628/10000 (66%)\ttrain_Loss: 0.005045\tval_Loss: 0.005664\n",
      "Train Epoch: 6629/10000 (66%)\ttrain_Loss: 0.005045\tval_Loss: 0.005681\n",
      "Train Epoch: 6630/10000 (66%)\ttrain_Loss: 0.005044\tval_Loss: 0.005660\n",
      "Train Epoch: 6631/10000 (66%)\ttrain_Loss: 0.005044\tval_Loss: 0.005683\n",
      "Train Epoch: 6632/10000 (66%)\ttrain_Loss: 0.005044\tval_Loss: 0.005662\n",
      "Train Epoch: 6633/10000 (66%)\ttrain_Loss: 0.005044\tval_Loss: 0.005686\n",
      "Train Epoch: 6634/10000 (66%)\ttrain_Loss: 0.005044\tval_Loss: 0.005661\n",
      "Train Epoch: 6635/10000 (66%)\ttrain_Loss: 0.005044\tval_Loss: 0.005684\n",
      "Train Epoch: 6636/10000 (66%)\ttrain_Loss: 0.005043\tval_Loss: 0.005659\n",
      "Train Epoch: 6637/10000 (66%)\ttrain_Loss: 0.005043\tval_Loss: 0.005680\n",
      "Train Epoch: 6638/10000 (66%)\ttrain_Loss: 0.005042\tval_Loss: 0.005659\n",
      "Train Epoch: 6639/10000 (66%)\ttrain_Loss: 0.005042\tval_Loss: 0.005680\n",
      "Train Epoch: 6640/10000 (66%)\ttrain_Loss: 0.005042\tval_Loss: 0.005663\n",
      "Train Epoch: 6641/10000 (66%)\ttrain_Loss: 0.005041\tval_Loss: 0.005679\n",
      "Train Epoch: 6642/10000 (66%)\ttrain_Loss: 0.005041\tval_Loss: 0.005661\n",
      "Train Epoch: 6643/10000 (66%)\ttrain_Loss: 0.005041\tval_Loss: 0.005675\n",
      "Train Epoch: 6644/10000 (66%)\ttrain_Loss: 0.005040\tval_Loss: 0.005661\n",
      "Train Epoch: 6645/10000 (66%)\ttrain_Loss: 0.005040\tval_Loss: 0.005673\n",
      "Train Epoch: 6646/10000 (66%)\ttrain_Loss: 0.005040\tval_Loss: 0.005660\n",
      "Train Epoch: 6647/10000 (66%)\ttrain_Loss: 0.005039\tval_Loss: 0.005669\n",
      "Train Epoch: 6648/10000 (66%)\ttrain_Loss: 0.005039\tval_Loss: 0.005661\n",
      "Train Epoch: 6649/10000 (66%)\ttrain_Loss: 0.005038\tval_Loss: 0.005666\n",
      "Train Epoch: 6650/10000 (66%)\ttrain_Loss: 0.005038\tval_Loss: 0.005665\n",
      "Train Epoch: 6651/10000 (66%)\ttrain_Loss: 0.005038\tval_Loss: 0.005663\n",
      "Train Epoch: 6652/10000 (67%)\ttrain_Loss: 0.005038\tval_Loss: 0.005668\n",
      "Train Epoch: 6653/10000 (67%)\ttrain_Loss: 0.005037\tval_Loss: 0.005661\n",
      "Train Epoch: 6654/10000 (67%)\ttrain_Loss: 0.005037\tval_Loss: 0.005670\n",
      "Train Epoch: 6655/10000 (67%)\ttrain_Loss: 0.005037\tval_Loss: 0.005655\n",
      "Train Epoch: 6656/10000 (67%)\ttrain_Loss: 0.005037\tval_Loss: 0.005675\n",
      "Train Epoch: 6657/10000 (67%)\ttrain_Loss: 0.005037\tval_Loss: 0.005647\n",
      "Train Epoch: 6658/10000 (67%)\ttrain_Loss: 0.005038\tval_Loss: 0.005686\n",
      "Train Epoch: 6659/10000 (67%)\ttrain_Loss: 0.005040\tval_Loss: 0.005640\n",
      "Train Epoch: 6660/10000 (67%)\ttrain_Loss: 0.005042\tval_Loss: 0.005703\n",
      "Train Epoch: 6661/10000 (67%)\ttrain_Loss: 0.005045\tval_Loss: 0.005634\n",
      "Train Epoch: 6662/10000 (67%)\ttrain_Loss: 0.005048\tval_Loss: 0.005717\n",
      "Train Epoch: 6663/10000 (67%)\ttrain_Loss: 0.005050\tval_Loss: 0.005632\n",
      "Train Epoch: 6664/10000 (67%)\ttrain_Loss: 0.005049\tval_Loss: 0.005711\n",
      "Train Epoch: 6665/10000 (67%)\ttrain_Loss: 0.005046\tval_Loss: 0.005638\n",
      "Train Epoch: 6666/10000 (67%)\ttrain_Loss: 0.005040\tval_Loss: 0.005676\n",
      "Train Epoch: 6667/10000 (67%)\ttrain_Loss: 0.005035\tval_Loss: 0.005659\n",
      "Train Epoch: 6668/10000 (67%)\ttrain_Loss: 0.005034\tval_Loss: 0.005646\n",
      "Train Epoch: 6669/10000 (67%)\ttrain_Loss: 0.005035\tval_Loss: 0.005685\n",
      "Train Epoch: 6670/10000 (67%)\ttrain_Loss: 0.005037\tval_Loss: 0.005635\n",
      "Train Epoch: 6671/10000 (67%)\ttrain_Loss: 0.005039\tval_Loss: 0.005690\n",
      "Train Epoch: 6672/10000 (67%)\ttrain_Loss: 0.005039\tval_Loss: 0.005635\n",
      "Train Epoch: 6673/10000 (67%)\ttrain_Loss: 0.005037\tval_Loss: 0.005671\n",
      "Train Epoch: 6674/10000 (67%)\ttrain_Loss: 0.005034\tval_Loss: 0.005651\n",
      "Train Epoch: 6675/10000 (67%)\ttrain_Loss: 0.005032\tval_Loss: 0.005650\n",
      "Train Epoch: 6676/10000 (67%)\ttrain_Loss: 0.005032\tval_Loss: 0.005674\n",
      "Train Epoch: 6677/10000 (67%)\ttrain_Loss: 0.005033\tval_Loss: 0.005640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6678/10000 (67%)\ttrain_Loss: 0.005035\tval_Loss: 0.005681\n",
      "Train Epoch: 6679/10000 (67%)\ttrain_Loss: 0.005035\tval_Loss: 0.005639\n",
      "Train Epoch: 6680/10000 (67%)\ttrain_Loss: 0.005034\tval_Loss: 0.005668\n",
      "Train Epoch: 6681/10000 (67%)\ttrain_Loss: 0.005032\tval_Loss: 0.005649\n",
      "Train Epoch: 6682/10000 (67%)\ttrain_Loss: 0.005031\tval_Loss: 0.005652\n",
      "Train Epoch: 6683/10000 (67%)\ttrain_Loss: 0.005030\tval_Loss: 0.005662\n",
      "Train Epoch: 6684/10000 (67%)\ttrain_Loss: 0.005030\tval_Loss: 0.005643\n",
      "Train Epoch: 6685/10000 (67%)\ttrain_Loss: 0.005031\tval_Loss: 0.005668\n",
      "Train Epoch: 6686/10000 (67%)\ttrain_Loss: 0.005031\tval_Loss: 0.005642\n",
      "Train Epoch: 6687/10000 (67%)\ttrain_Loss: 0.005031\tval_Loss: 0.005665\n",
      "Train Epoch: 6688/10000 (67%)\ttrain_Loss: 0.005030\tval_Loss: 0.005646\n",
      "Train Epoch: 6689/10000 (67%)\ttrain_Loss: 0.005029\tval_Loss: 0.005657\n",
      "Train Epoch: 6690/10000 (67%)\ttrain_Loss: 0.005029\tval_Loss: 0.005654\n",
      "Train Epoch: 6691/10000 (67%)\ttrain_Loss: 0.005029\tval_Loss: 0.005650\n",
      "Train Epoch: 6692/10000 (67%)\ttrain_Loss: 0.005028\tval_Loss: 0.005660\n",
      "Train Epoch: 6693/10000 (67%)\ttrain_Loss: 0.005028\tval_Loss: 0.005643\n",
      "Train Epoch: 6694/10000 (67%)\ttrain_Loss: 0.005028\tval_Loss: 0.005658\n",
      "Train Epoch: 6695/10000 (67%)\ttrain_Loss: 0.005028\tval_Loss: 0.005639\n",
      "Train Epoch: 6696/10000 (67%)\ttrain_Loss: 0.005028\tval_Loss: 0.005655\n",
      "Train Epoch: 6697/10000 (67%)\ttrain_Loss: 0.005028\tval_Loss: 0.005644\n",
      "Train Epoch: 6698/10000 (67%)\ttrain_Loss: 0.005027\tval_Loss: 0.005654\n",
      "Train Epoch: 6699/10000 (67%)\ttrain_Loss: 0.005027\tval_Loss: 0.005651\n",
      "Train Epoch: 6700/10000 (67%)\ttrain_Loss: 0.005026\tval_Loss: 0.005649\n",
      "Train Epoch: 6701/10000 (67%)\ttrain_Loss: 0.005026\tval_Loss: 0.005652\n",
      "Train Epoch: 6702/10000 (67%)\ttrain_Loss: 0.005026\tval_Loss: 0.005642\n",
      "Train Epoch: 6703/10000 (67%)\ttrain_Loss: 0.005026\tval_Loss: 0.005654\n",
      "Train Epoch: 6704/10000 (67%)\ttrain_Loss: 0.005026\tval_Loss: 0.005638\n",
      "Train Epoch: 6705/10000 (67%)\ttrain_Loss: 0.005026\tval_Loss: 0.005658\n",
      "Train Epoch: 6706/10000 (67%)\ttrain_Loss: 0.005026\tval_Loss: 0.005637\n",
      "Train Epoch: 6707/10000 (67%)\ttrain_Loss: 0.005026\tval_Loss: 0.005657\n",
      "Train Epoch: 6708/10000 (67%)\ttrain_Loss: 0.005025\tval_Loss: 0.005638\n",
      "Train Epoch: 6709/10000 (67%)\ttrain_Loss: 0.005025\tval_Loss: 0.005656\n",
      "Train Epoch: 6710/10000 (67%)\ttrain_Loss: 0.005025\tval_Loss: 0.005638\n",
      "Train Epoch: 6711/10000 (67%)\ttrain_Loss: 0.005024\tval_Loss: 0.005653\n",
      "Train Epoch: 6712/10000 (67%)\ttrain_Loss: 0.005024\tval_Loss: 0.005637\n",
      "Train Epoch: 6713/10000 (67%)\ttrain_Loss: 0.005024\tval_Loss: 0.005649\n",
      "Train Epoch: 6714/10000 (67%)\ttrain_Loss: 0.005023\tval_Loss: 0.005640\n",
      "Train Epoch: 6715/10000 (67%)\ttrain_Loss: 0.005023\tval_Loss: 0.005647\n",
      "Train Epoch: 6716/10000 (67%)\ttrain_Loss: 0.005023\tval_Loss: 0.005646\n",
      "Train Epoch: 6717/10000 (67%)\ttrain_Loss: 0.005023\tval_Loss: 0.005644\n",
      "Train Epoch: 6718/10000 (67%)\ttrain_Loss: 0.005022\tval_Loss: 0.005645\n",
      "Train Epoch: 6719/10000 (67%)\ttrain_Loss: 0.005022\tval_Loss: 0.005639\n",
      "Train Epoch: 6720/10000 (67%)\ttrain_Loss: 0.005022\tval_Loss: 0.005643\n",
      "Train Epoch: 6721/10000 (67%)\ttrain_Loss: 0.005022\tval_Loss: 0.005640\n",
      "Train Epoch: 6722/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005647\n",
      "Train Epoch: 6723/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005639\n",
      "Train Epoch: 6724/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005647\n",
      "Train Epoch: 6725/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005635\n",
      "Train Epoch: 6726/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005645\n",
      "Train Epoch: 6727/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005631\n",
      "Train Epoch: 6728/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005650\n",
      "Train Epoch: 6729/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005629\n",
      "Train Epoch: 6730/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005659\n",
      "Train Epoch: 6731/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005624\n",
      "Train Epoch: 6732/10000 (67%)\ttrain_Loss: 0.005022\tval_Loss: 0.005667\n",
      "Train Epoch: 6733/10000 (67%)\ttrain_Loss: 0.005023\tval_Loss: 0.005615\n",
      "Train Epoch: 6734/10000 (67%)\ttrain_Loss: 0.005025\tval_Loss: 0.005679\n",
      "Train Epoch: 6735/10000 (67%)\ttrain_Loss: 0.005027\tval_Loss: 0.005610\n",
      "Train Epoch: 6736/10000 (67%)\ttrain_Loss: 0.005029\tval_Loss: 0.005691\n",
      "Train Epoch: 6737/10000 (67%)\ttrain_Loss: 0.005031\tval_Loss: 0.005610\n",
      "Train Epoch: 6738/10000 (67%)\ttrain_Loss: 0.005033\tval_Loss: 0.005697\n",
      "Train Epoch: 6739/10000 (67%)\ttrain_Loss: 0.005032\tval_Loss: 0.005611\n",
      "Train Epoch: 6740/10000 (67%)\ttrain_Loss: 0.005030\tval_Loss: 0.005680\n",
      "Train Epoch: 6741/10000 (67%)\ttrain_Loss: 0.005026\tval_Loss: 0.005614\n",
      "Train Epoch: 6742/10000 (67%)\ttrain_Loss: 0.005022\tval_Loss: 0.005652\n",
      "Train Epoch: 6743/10000 (67%)\ttrain_Loss: 0.005019\tval_Loss: 0.005628\n",
      "Train Epoch: 6744/10000 (67%)\ttrain_Loss: 0.005017\tval_Loss: 0.005628\n",
      "Train Epoch: 6745/10000 (67%)\ttrain_Loss: 0.005017\tval_Loss: 0.005648\n",
      "Train Epoch: 6746/10000 (67%)\ttrain_Loss: 0.005018\tval_Loss: 0.005616\n",
      "Train Epoch: 6747/10000 (67%)\ttrain_Loss: 0.005019\tval_Loss: 0.005665\n",
      "Train Epoch: 6748/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005614\n",
      "Train Epoch: 6749/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005667\n",
      "Train Epoch: 6750/10000 (67%)\ttrain_Loss: 0.005021\tval_Loss: 0.005615\n",
      "Train Epoch: 6751/10000 (68%)\ttrain_Loss: 0.005020\tval_Loss: 0.005656\n",
      "Train Epoch: 6752/10000 (68%)\ttrain_Loss: 0.005018\tval_Loss: 0.005621\n",
      "Train Epoch: 6753/10000 (68%)\ttrain_Loss: 0.005016\tval_Loss: 0.005640\n",
      "Train Epoch: 6754/10000 (68%)\ttrain_Loss: 0.005015\tval_Loss: 0.005633\n",
      "Train Epoch: 6755/10000 (68%)\ttrain_Loss: 0.005014\tval_Loss: 0.005626\n",
      "Train Epoch: 6756/10000 (68%)\ttrain_Loss: 0.005014\tval_Loss: 0.005644\n",
      "Train Epoch: 6757/10000 (68%)\ttrain_Loss: 0.005015\tval_Loss: 0.005618\n",
      "Train Epoch: 6758/10000 (68%)\ttrain_Loss: 0.005015\tval_Loss: 0.005649\n",
      "Train Epoch: 6759/10000 (68%)\ttrain_Loss: 0.005016\tval_Loss: 0.005614\n",
      "Train Epoch: 6760/10000 (68%)\ttrain_Loss: 0.005016\tval_Loss: 0.005650\n",
      "Train Epoch: 6761/10000 (68%)\ttrain_Loss: 0.005015\tval_Loss: 0.005617\n",
      "Train Epoch: 6762/10000 (68%)\ttrain_Loss: 0.005015\tval_Loss: 0.005644\n",
      "Train Epoch: 6763/10000 (68%)\ttrain_Loss: 0.005014\tval_Loss: 0.005620\n",
      "Train Epoch: 6764/10000 (68%)\ttrain_Loss: 0.005013\tval_Loss: 0.005638\n",
      "Train Epoch: 6765/10000 (68%)\ttrain_Loss: 0.005013\tval_Loss: 0.005625\n",
      "Train Epoch: 6766/10000 (68%)\ttrain_Loss: 0.005012\tval_Loss: 0.005633\n",
      "Train Epoch: 6767/10000 (68%)\ttrain_Loss: 0.005012\tval_Loss: 0.005631\n",
      "Train Epoch: 6768/10000 (68%)\ttrain_Loss: 0.005012\tval_Loss: 0.005626\n",
      "Train Epoch: 6769/10000 (68%)\ttrain_Loss: 0.005012\tval_Loss: 0.005634\n",
      "Train Epoch: 6770/10000 (68%)\ttrain_Loss: 0.005011\tval_Loss: 0.005620\n",
      "Train Epoch: 6771/10000 (68%)\ttrain_Loss: 0.005012\tval_Loss: 0.005638\n",
      "Train Epoch: 6772/10000 (68%)\ttrain_Loss: 0.005012\tval_Loss: 0.005616\n",
      "Train Epoch: 6773/10000 (68%)\ttrain_Loss: 0.005012\tval_Loss: 0.005643\n",
      "Train Epoch: 6774/10000 (68%)\ttrain_Loss: 0.005012\tval_Loss: 0.005612\n",
      "Train Epoch: 6775/10000 (68%)\ttrain_Loss: 0.005012\tval_Loss: 0.005647\n",
      "Train Epoch: 6776/10000 (68%)\ttrain_Loss: 0.005013\tval_Loss: 0.005610\n",
      "Train Epoch: 6777/10000 (68%)\ttrain_Loss: 0.005013\tval_Loss: 0.005651\n",
      "Train Epoch: 6778/10000 (68%)\ttrain_Loss: 0.005013\tval_Loss: 0.005609\n",
      "Train Epoch: 6779/10000 (68%)\ttrain_Loss: 0.005013\tval_Loss: 0.005650\n",
      "Train Epoch: 6780/10000 (68%)\ttrain_Loss: 0.005013\tval_Loss: 0.005608\n",
      "Train Epoch: 6781/10000 (68%)\ttrain_Loss: 0.005012\tval_Loss: 0.005646\n",
      "Train Epoch: 6782/10000 (68%)\ttrain_Loss: 0.005012\tval_Loss: 0.005608\n",
      "Train Epoch: 6783/10000 (68%)\ttrain_Loss: 0.005011\tval_Loss: 0.005643\n",
      "Train Epoch: 6784/10000 (68%)\ttrain_Loss: 0.005011\tval_Loss: 0.005610\n",
      "Train Epoch: 6785/10000 (68%)\ttrain_Loss: 0.005010\tval_Loss: 0.005643\n",
      "Train Epoch: 6786/10000 (68%)\ttrain_Loss: 0.005010\tval_Loss: 0.005610\n",
      "Train Epoch: 6787/10000 (68%)\ttrain_Loss: 0.005010\tval_Loss: 0.005641\n",
      "Train Epoch: 6788/10000 (68%)\ttrain_Loss: 0.005009\tval_Loss: 0.005609\n",
      "Train Epoch: 6789/10000 (68%)\ttrain_Loss: 0.005009\tval_Loss: 0.005637\n",
      "Train Epoch: 6790/10000 (68%)\ttrain_Loss: 0.005009\tval_Loss: 0.005609\n",
      "Train Epoch: 6791/10000 (68%)\ttrain_Loss: 0.005009\tval_Loss: 0.005638\n",
      "Train Epoch: 6792/10000 (68%)\ttrain_Loss: 0.005008\tval_Loss: 0.005610\n",
      "Train Epoch: 6793/10000 (68%)\ttrain_Loss: 0.005008\tval_Loss: 0.005641\n",
      "Train Epoch: 6794/10000 (68%)\ttrain_Loss: 0.005008\tval_Loss: 0.005608\n",
      "Train Epoch: 6795/10000 (68%)\ttrain_Loss: 0.005008\tval_Loss: 0.005640\n",
      "Train Epoch: 6796/10000 (68%)\ttrain_Loss: 0.005008\tval_Loss: 0.005604\n",
      "Train Epoch: 6797/10000 (68%)\ttrain_Loss: 0.005008\tval_Loss: 0.005640\n",
      "Train Epoch: 6798/10000 (68%)\ttrain_Loss: 0.005008\tval_Loss: 0.005602\n",
      "Train Epoch: 6799/10000 (68%)\ttrain_Loss: 0.005008\tval_Loss: 0.005644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6800/10000 (68%)\ttrain_Loss: 0.005008\tval_Loss: 0.005603\n",
      "Train Epoch: 6801/10000 (68%)\ttrain_Loss: 0.005009\tval_Loss: 0.005648\n",
      "Train Epoch: 6802/10000 (68%)\ttrain_Loss: 0.005009\tval_Loss: 0.005599\n",
      "Train Epoch: 6803/10000 (68%)\ttrain_Loss: 0.005009\tval_Loss: 0.005650\n",
      "Train Epoch: 6804/10000 (68%)\ttrain_Loss: 0.005010\tval_Loss: 0.005596\n",
      "Train Epoch: 6805/10000 (68%)\ttrain_Loss: 0.005010\tval_Loss: 0.005652\n",
      "Train Epoch: 6806/10000 (68%)\ttrain_Loss: 0.005010\tval_Loss: 0.005595\n",
      "Train Epoch: 6807/10000 (68%)\ttrain_Loss: 0.005011\tval_Loss: 0.005657\n",
      "Train Epoch: 6808/10000 (68%)\ttrain_Loss: 0.005011\tval_Loss: 0.005594\n",
      "Train Epoch: 6809/10000 (68%)\ttrain_Loss: 0.005011\tval_Loss: 0.005659\n",
      "Train Epoch: 6810/10000 (68%)\ttrain_Loss: 0.005011\tval_Loss: 0.005592\n",
      "Train Epoch: 6811/10000 (68%)\ttrain_Loss: 0.005011\tval_Loss: 0.005657\n",
      "Train Epoch: 6812/10000 (68%)\ttrain_Loss: 0.005011\tval_Loss: 0.005593\n",
      "Train Epoch: 6813/10000 (68%)\ttrain_Loss: 0.005010\tval_Loss: 0.005650\n",
      "Train Epoch: 6814/10000 (68%)\ttrain_Loss: 0.005008\tval_Loss: 0.005596\n",
      "Train Epoch: 6815/10000 (68%)\ttrain_Loss: 0.005007\tval_Loss: 0.005639\n",
      "Train Epoch: 6816/10000 (68%)\ttrain_Loss: 0.005005\tval_Loss: 0.005601\n",
      "Train Epoch: 6817/10000 (68%)\ttrain_Loss: 0.005004\tval_Loss: 0.005628\n",
      "Train Epoch: 6818/10000 (68%)\ttrain_Loss: 0.005003\tval_Loss: 0.005607\n",
      "Train Epoch: 6819/10000 (68%)\ttrain_Loss: 0.005002\tval_Loss: 0.005619\n",
      "Train Epoch: 6820/10000 (68%)\ttrain_Loss: 0.005001\tval_Loss: 0.005612\n",
      "Train Epoch: 6821/10000 (68%)\ttrain_Loss: 0.005001\tval_Loss: 0.005614\n",
      "Train Epoch: 6822/10000 (68%)\ttrain_Loss: 0.005001\tval_Loss: 0.005615\n",
      "Train Epoch: 6823/10000 (68%)\ttrain_Loss: 0.005001\tval_Loss: 0.005610\n",
      "Train Epoch: 6824/10000 (68%)\ttrain_Loss: 0.005001\tval_Loss: 0.005618\n",
      "Train Epoch: 6825/10000 (68%)\ttrain_Loss: 0.005001\tval_Loss: 0.005606\n",
      "Train Epoch: 6826/10000 (68%)\ttrain_Loss: 0.005001\tval_Loss: 0.005623\n",
      "Train Epoch: 6827/10000 (68%)\ttrain_Loss: 0.005001\tval_Loss: 0.005602\n",
      "Train Epoch: 6828/10000 (68%)\ttrain_Loss: 0.005001\tval_Loss: 0.005630\n",
      "Train Epoch: 6829/10000 (68%)\ttrain_Loss: 0.005001\tval_Loss: 0.005597\n",
      "Train Epoch: 6830/10000 (68%)\ttrain_Loss: 0.005002\tval_Loss: 0.005637\n",
      "Train Epoch: 6831/10000 (68%)\ttrain_Loss: 0.005003\tval_Loss: 0.005591\n",
      "Train Epoch: 6832/10000 (68%)\ttrain_Loss: 0.005004\tval_Loss: 0.005643\n",
      "Train Epoch: 6833/10000 (68%)\ttrain_Loss: 0.005004\tval_Loss: 0.005587\n",
      "Train Epoch: 6834/10000 (68%)\ttrain_Loss: 0.005006\tval_Loss: 0.005651\n",
      "Train Epoch: 6835/10000 (68%)\ttrain_Loss: 0.005007\tval_Loss: 0.005585\n",
      "Train Epoch: 6836/10000 (68%)\ttrain_Loss: 0.005008\tval_Loss: 0.005661\n",
      "Train Epoch: 6837/10000 (68%)\ttrain_Loss: 0.005009\tval_Loss: 0.005583\n",
      "Train Epoch: 6838/10000 (68%)\ttrain_Loss: 0.005010\tval_Loss: 0.005664\n",
      "Train Epoch: 6839/10000 (68%)\ttrain_Loss: 0.005011\tval_Loss: 0.005579\n",
      "Train Epoch: 6840/10000 (68%)\ttrain_Loss: 0.005010\tval_Loss: 0.005659\n",
      "Train Epoch: 6841/10000 (68%)\ttrain_Loss: 0.005009\tval_Loss: 0.005581\n",
      "Train Epoch: 6842/10000 (68%)\ttrain_Loss: 0.005007\tval_Loss: 0.005648\n",
      "Train Epoch: 6843/10000 (68%)\ttrain_Loss: 0.005004\tval_Loss: 0.005589\n",
      "Train Epoch: 6844/10000 (68%)\ttrain_Loss: 0.005001\tval_Loss: 0.005629\n",
      "Train Epoch: 6845/10000 (68%)\ttrain_Loss: 0.004999\tval_Loss: 0.005600\n",
      "Train Epoch: 6846/10000 (68%)\ttrain_Loss: 0.004997\tval_Loss: 0.005610\n",
      "Train Epoch: 6847/10000 (68%)\ttrain_Loss: 0.004996\tval_Loss: 0.005613\n",
      "Train Epoch: 6848/10000 (68%)\ttrain_Loss: 0.004996\tval_Loss: 0.005595\n",
      "Train Epoch: 6849/10000 (68%)\ttrain_Loss: 0.004997\tval_Loss: 0.005625\n",
      "Train Epoch: 6850/10000 (68%)\ttrain_Loss: 0.004998\tval_Loss: 0.005588\n",
      "Train Epoch: 6851/10000 (68%)\ttrain_Loss: 0.004999\tval_Loss: 0.005636\n",
      "Train Epoch: 6852/10000 (69%)\ttrain_Loss: 0.005000\tval_Loss: 0.005586\n",
      "Train Epoch: 6853/10000 (69%)\ttrain_Loss: 0.005000\tval_Loss: 0.005639\n",
      "Train Epoch: 6854/10000 (69%)\ttrain_Loss: 0.005000\tval_Loss: 0.005586\n",
      "Train Epoch: 6855/10000 (69%)\ttrain_Loss: 0.005000\tval_Loss: 0.005633\n",
      "Train Epoch: 6856/10000 (69%)\ttrain_Loss: 0.004999\tval_Loss: 0.005588\n",
      "Train Epoch: 6857/10000 (69%)\ttrain_Loss: 0.004998\tval_Loss: 0.005626\n",
      "Train Epoch: 6858/10000 (69%)\ttrain_Loss: 0.004997\tval_Loss: 0.005591\n",
      "Train Epoch: 6859/10000 (69%)\ttrain_Loss: 0.004996\tval_Loss: 0.005615\n",
      "Train Epoch: 6860/10000 (69%)\ttrain_Loss: 0.004995\tval_Loss: 0.005595\n",
      "Train Epoch: 6861/10000 (69%)\ttrain_Loss: 0.004994\tval_Loss: 0.005607\n",
      "Train Epoch: 6862/10000 (69%)\ttrain_Loss: 0.004994\tval_Loss: 0.005604\n",
      "Train Epoch: 6863/10000 (69%)\ttrain_Loss: 0.004993\tval_Loss: 0.005601\n",
      "Train Epoch: 6864/10000 (69%)\ttrain_Loss: 0.004993\tval_Loss: 0.005611\n",
      "Train Epoch: 6865/10000 (69%)\ttrain_Loss: 0.004993\tval_Loss: 0.005595\n",
      "Train Epoch: 6866/10000 (69%)\ttrain_Loss: 0.004993\tval_Loss: 0.005616\n",
      "Train Epoch: 6867/10000 (69%)\ttrain_Loss: 0.004994\tval_Loss: 0.005588\n",
      "Train Epoch: 6868/10000 (69%)\ttrain_Loss: 0.004994\tval_Loss: 0.005619\n",
      "Train Epoch: 6869/10000 (69%)\ttrain_Loss: 0.004994\tval_Loss: 0.005584\n",
      "Train Epoch: 6870/10000 (69%)\ttrain_Loss: 0.004995\tval_Loss: 0.005626\n",
      "Train Epoch: 6871/10000 (69%)\ttrain_Loss: 0.004995\tval_Loss: 0.005583\n",
      "Train Epoch: 6872/10000 (69%)\ttrain_Loss: 0.004996\tval_Loss: 0.005635\n",
      "Train Epoch: 6873/10000 (69%)\ttrain_Loss: 0.004997\tval_Loss: 0.005580\n",
      "Train Epoch: 6874/10000 (69%)\ttrain_Loss: 0.004998\tval_Loss: 0.005641\n",
      "Train Epoch: 6875/10000 (69%)\ttrain_Loss: 0.004999\tval_Loss: 0.005575\n",
      "Train Epoch: 6876/10000 (69%)\ttrain_Loss: 0.004999\tval_Loss: 0.005643\n",
      "Train Epoch: 6877/10000 (69%)\ttrain_Loss: 0.005000\tval_Loss: 0.005572\n",
      "Train Epoch: 6878/10000 (69%)\ttrain_Loss: 0.005000\tval_Loss: 0.005642\n",
      "Train Epoch: 6879/10000 (69%)\ttrain_Loss: 0.004999\tval_Loss: 0.005573\n",
      "Train Epoch: 6880/10000 (69%)\ttrain_Loss: 0.004999\tval_Loss: 0.005641\n",
      "Train Epoch: 6881/10000 (69%)\ttrain_Loss: 0.004998\tval_Loss: 0.005577\n",
      "Train Epoch: 6882/10000 (69%)\ttrain_Loss: 0.004997\tval_Loss: 0.005633\n",
      "Train Epoch: 6883/10000 (69%)\ttrain_Loss: 0.004995\tval_Loss: 0.005579\n",
      "Train Epoch: 6884/10000 (69%)\ttrain_Loss: 0.004994\tval_Loss: 0.005622\n",
      "Train Epoch: 6885/10000 (69%)\ttrain_Loss: 0.004993\tval_Loss: 0.005582\n",
      "Train Epoch: 6886/10000 (69%)\ttrain_Loss: 0.004991\tval_Loss: 0.005610\n",
      "Train Epoch: 6887/10000 (69%)\ttrain_Loss: 0.004990\tval_Loss: 0.005589\n",
      "Train Epoch: 6888/10000 (69%)\ttrain_Loss: 0.004989\tval_Loss: 0.005601\n",
      "Train Epoch: 6889/10000 (69%)\ttrain_Loss: 0.004989\tval_Loss: 0.005597\n",
      "Train Epoch: 6890/10000 (69%)\ttrain_Loss: 0.004989\tval_Loss: 0.005596\n",
      "Train Epoch: 6891/10000 (69%)\ttrain_Loss: 0.004988\tval_Loss: 0.005603\n",
      "Train Epoch: 6892/10000 (69%)\ttrain_Loss: 0.004988\tval_Loss: 0.005590\n",
      "Train Epoch: 6893/10000 (69%)\ttrain_Loss: 0.004988\tval_Loss: 0.005606\n",
      "Train Epoch: 6894/10000 (69%)\ttrain_Loss: 0.004988\tval_Loss: 0.005585\n",
      "Train Epoch: 6895/10000 (69%)\ttrain_Loss: 0.004989\tval_Loss: 0.005609\n",
      "Train Epoch: 6896/10000 (69%)\ttrain_Loss: 0.004989\tval_Loss: 0.005583\n",
      "Train Epoch: 6897/10000 (69%)\ttrain_Loss: 0.004989\tval_Loss: 0.005613\n",
      "Train Epoch: 6898/10000 (69%)\ttrain_Loss: 0.004989\tval_Loss: 0.005581\n",
      "Train Epoch: 6899/10000 (69%)\ttrain_Loss: 0.004989\tval_Loss: 0.005616\n",
      "Train Epoch: 6900/10000 (69%)\ttrain_Loss: 0.004989\tval_Loss: 0.005577\n",
      "Train Epoch: 6901/10000 (69%)\ttrain_Loss: 0.004990\tval_Loss: 0.005618\n",
      "Train Epoch: 6902/10000 (69%)\ttrain_Loss: 0.004990\tval_Loss: 0.005573\n",
      "Train Epoch: 6903/10000 (69%)\ttrain_Loss: 0.004990\tval_Loss: 0.005624\n",
      "Train Epoch: 6904/10000 (69%)\ttrain_Loss: 0.004991\tval_Loss: 0.005569\n",
      "Train Epoch: 6905/10000 (69%)\ttrain_Loss: 0.004992\tval_Loss: 0.005635\n",
      "Train Epoch: 6906/10000 (69%)\ttrain_Loss: 0.004994\tval_Loss: 0.005567\n",
      "Train Epoch: 6907/10000 (69%)\ttrain_Loss: 0.004995\tval_Loss: 0.005646\n",
      "Train Epoch: 6908/10000 (69%)\ttrain_Loss: 0.004997\tval_Loss: 0.005564\n",
      "Train Epoch: 6909/10000 (69%)\ttrain_Loss: 0.004999\tval_Loss: 0.005653\n",
      "Train Epoch: 6910/10000 (69%)\ttrain_Loss: 0.005000\tval_Loss: 0.005562\n",
      "Train Epoch: 6911/10000 (69%)\ttrain_Loss: 0.005001\tval_Loss: 0.005655\n",
      "Train Epoch: 6912/10000 (69%)\ttrain_Loss: 0.005001\tval_Loss: 0.005561\n",
      "Train Epoch: 6913/10000 (69%)\ttrain_Loss: 0.005000\tval_Loss: 0.005646\n",
      "Train Epoch: 6914/10000 (69%)\ttrain_Loss: 0.004997\tval_Loss: 0.005564\n",
      "Train Epoch: 6915/10000 (69%)\ttrain_Loss: 0.004994\tval_Loss: 0.005625\n",
      "Train Epoch: 6916/10000 (69%)\ttrain_Loss: 0.004990\tval_Loss: 0.005572\n",
      "Train Epoch: 6917/10000 (69%)\ttrain_Loss: 0.004987\tval_Loss: 0.005601\n",
      "Train Epoch: 6918/10000 (69%)\ttrain_Loss: 0.004984\tval_Loss: 0.005587\n",
      "Train Epoch: 6919/10000 (69%)\ttrain_Loss: 0.004983\tval_Loss: 0.005584\n",
      "Train Epoch: 6920/10000 (69%)\ttrain_Loss: 0.004983\tval_Loss: 0.005605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6921/10000 (69%)\ttrain_Loss: 0.004984\tval_Loss: 0.005574\n",
      "Train Epoch: 6922/10000 (69%)\ttrain_Loss: 0.004986\tval_Loss: 0.005620\n",
      "Train Epoch: 6923/10000 (69%)\ttrain_Loss: 0.004987\tval_Loss: 0.005568\n",
      "Train Epoch: 6924/10000 (69%)\ttrain_Loss: 0.004988\tval_Loss: 0.005625\n",
      "Train Epoch: 6925/10000 (69%)\ttrain_Loss: 0.004989\tval_Loss: 0.005565\n",
      "Train Epoch: 6926/10000 (69%)\ttrain_Loss: 0.004988\tval_Loss: 0.005621\n",
      "Train Epoch: 6927/10000 (69%)\ttrain_Loss: 0.004988\tval_Loss: 0.005566\n",
      "Train Epoch: 6928/10000 (69%)\ttrain_Loss: 0.004987\tval_Loss: 0.005614\n",
      "Train Epoch: 6929/10000 (69%)\ttrain_Loss: 0.004985\tval_Loss: 0.005572\n",
      "Train Epoch: 6930/10000 (69%)\ttrain_Loss: 0.004984\tval_Loss: 0.005605\n",
      "Train Epoch: 6931/10000 (69%)\ttrain_Loss: 0.004983\tval_Loss: 0.005578\n",
      "Train Epoch: 6932/10000 (69%)\ttrain_Loss: 0.004982\tval_Loss: 0.005596\n",
      "Train Epoch: 6933/10000 (69%)\ttrain_Loss: 0.004981\tval_Loss: 0.005583\n",
      "Train Epoch: 6934/10000 (69%)\ttrain_Loss: 0.004981\tval_Loss: 0.005585\n",
      "Train Epoch: 6935/10000 (69%)\ttrain_Loss: 0.004981\tval_Loss: 0.005588\n",
      "Train Epoch: 6936/10000 (69%)\ttrain_Loss: 0.004981\tval_Loss: 0.005579\n",
      "Train Epoch: 6937/10000 (69%)\ttrain_Loss: 0.004981\tval_Loss: 0.005595\n",
      "Train Epoch: 6938/10000 (69%)\ttrain_Loss: 0.004981\tval_Loss: 0.005576\n",
      "Train Epoch: 6939/10000 (69%)\ttrain_Loss: 0.004981\tval_Loss: 0.005601\n",
      "Train Epoch: 6940/10000 (69%)\ttrain_Loss: 0.004981\tval_Loss: 0.005573\n",
      "Train Epoch: 6941/10000 (69%)\ttrain_Loss: 0.004981\tval_Loss: 0.005604\n",
      "Train Epoch: 6942/10000 (69%)\ttrain_Loss: 0.004981\tval_Loss: 0.005570\n",
      "Train Epoch: 6943/10000 (69%)\ttrain_Loss: 0.004982\tval_Loss: 0.005606\n",
      "Train Epoch: 6944/10000 (69%)\ttrain_Loss: 0.004982\tval_Loss: 0.005567\n",
      "Train Epoch: 6945/10000 (69%)\ttrain_Loss: 0.004982\tval_Loss: 0.005609\n",
      "Train Epoch: 6946/10000 (69%)\ttrain_Loss: 0.004982\tval_Loss: 0.005565\n",
      "Train Epoch: 6947/10000 (69%)\ttrain_Loss: 0.004982\tval_Loss: 0.005612\n",
      "Train Epoch: 6948/10000 (69%)\ttrain_Loss: 0.004983\tval_Loss: 0.005562\n",
      "Train Epoch: 6949/10000 (69%)\ttrain_Loss: 0.004983\tval_Loss: 0.005616\n",
      "Train Epoch: 6950/10000 (69%)\ttrain_Loss: 0.004984\tval_Loss: 0.005560\n",
      "Train Epoch: 6951/10000 (70%)\ttrain_Loss: 0.004984\tval_Loss: 0.005621\n",
      "Train Epoch: 6952/10000 (70%)\ttrain_Loss: 0.004985\tval_Loss: 0.005558\n",
      "Train Epoch: 6953/10000 (70%)\ttrain_Loss: 0.004985\tval_Loss: 0.005624\n",
      "Train Epoch: 6954/10000 (70%)\ttrain_Loss: 0.004986\tval_Loss: 0.005556\n",
      "Train Epoch: 6955/10000 (70%)\ttrain_Loss: 0.004986\tval_Loss: 0.005625\n",
      "Train Epoch: 6956/10000 (70%)\ttrain_Loss: 0.004986\tval_Loss: 0.005556\n",
      "Train Epoch: 6957/10000 (70%)\ttrain_Loss: 0.004986\tval_Loss: 0.005624\n",
      "Train Epoch: 6958/10000 (70%)\ttrain_Loss: 0.004985\tval_Loss: 0.005556\n",
      "Train Epoch: 6959/10000 (70%)\ttrain_Loss: 0.004985\tval_Loss: 0.005621\n",
      "Train Epoch: 6960/10000 (70%)\ttrain_Loss: 0.004984\tval_Loss: 0.005556\n",
      "Train Epoch: 6961/10000 (70%)\ttrain_Loss: 0.004983\tval_Loss: 0.005615\n",
      "Train Epoch: 6962/10000 (70%)\ttrain_Loss: 0.004982\tval_Loss: 0.005559\n",
      "Train Epoch: 6963/10000 (70%)\ttrain_Loss: 0.004981\tval_Loss: 0.005606\n",
      "Train Epoch: 6964/10000 (70%)\ttrain_Loss: 0.004979\tval_Loss: 0.005565\n",
      "Train Epoch: 6965/10000 (70%)\ttrain_Loss: 0.004978\tval_Loss: 0.005595\n",
      "Train Epoch: 6966/10000 (70%)\ttrain_Loss: 0.004977\tval_Loss: 0.005570\n",
      "Train Epoch: 6967/10000 (70%)\ttrain_Loss: 0.004976\tval_Loss: 0.005587\n",
      "Train Epoch: 6968/10000 (70%)\ttrain_Loss: 0.004975\tval_Loss: 0.005575\n",
      "Train Epoch: 6969/10000 (70%)\ttrain_Loss: 0.004975\tval_Loss: 0.005581\n",
      "Train Epoch: 6970/10000 (70%)\ttrain_Loss: 0.004975\tval_Loss: 0.005578\n",
      "Train Epoch: 6971/10000 (70%)\ttrain_Loss: 0.004975\tval_Loss: 0.005576\n",
      "Train Epoch: 6972/10000 (70%)\ttrain_Loss: 0.004974\tval_Loss: 0.005580\n",
      "Train Epoch: 6973/10000 (70%)\ttrain_Loss: 0.004974\tval_Loss: 0.005574\n",
      "Train Epoch: 6974/10000 (70%)\ttrain_Loss: 0.004974\tval_Loss: 0.005584\n",
      "Train Epoch: 6975/10000 (70%)\ttrain_Loss: 0.004974\tval_Loss: 0.005573\n",
      "Train Epoch: 6976/10000 (70%)\ttrain_Loss: 0.004974\tval_Loss: 0.005587\n",
      "Train Epoch: 6977/10000 (70%)\ttrain_Loss: 0.004974\tval_Loss: 0.005569\n",
      "Train Epoch: 6978/10000 (70%)\ttrain_Loss: 0.004974\tval_Loss: 0.005588\n",
      "Train Epoch: 6979/10000 (70%)\ttrain_Loss: 0.004974\tval_Loss: 0.005564\n",
      "Train Epoch: 6980/10000 (70%)\ttrain_Loss: 0.004974\tval_Loss: 0.005593\n",
      "Train Epoch: 6981/10000 (70%)\ttrain_Loss: 0.004975\tval_Loss: 0.005559\n",
      "Train Epoch: 6982/10000 (70%)\ttrain_Loss: 0.004975\tval_Loss: 0.005604\n",
      "Train Epoch: 6983/10000 (70%)\ttrain_Loss: 0.004976\tval_Loss: 0.005554\n",
      "Train Epoch: 6984/10000 (70%)\ttrain_Loss: 0.004978\tval_Loss: 0.005619\n",
      "Train Epoch: 6985/10000 (70%)\ttrain_Loss: 0.004981\tval_Loss: 0.005547\n",
      "Train Epoch: 6986/10000 (70%)\ttrain_Loss: 0.004984\tval_Loss: 0.005639\n",
      "Train Epoch: 6987/10000 (70%)\ttrain_Loss: 0.004988\tval_Loss: 0.005541\n",
      "Train Epoch: 6988/10000 (70%)\ttrain_Loss: 0.004994\tval_Loss: 0.005665\n",
      "Train Epoch: 6989/10000 (70%)\ttrain_Loss: 0.005000\tval_Loss: 0.005539\n",
      "Train Epoch: 6990/10000 (70%)\ttrain_Loss: 0.005006\tval_Loss: 0.005684\n",
      "Train Epoch: 6991/10000 (70%)\ttrain_Loss: 0.005009\tval_Loss: 0.005539\n",
      "Train Epoch: 6992/10000 (70%)\ttrain_Loss: 0.005007\tval_Loss: 0.005667\n",
      "Train Epoch: 6993/10000 (70%)\ttrain_Loss: 0.005000\tval_Loss: 0.005542\n",
      "Train Epoch: 6994/10000 (70%)\ttrain_Loss: 0.004989\tval_Loss: 0.005613\n",
      "Train Epoch: 6995/10000 (70%)\ttrain_Loss: 0.004978\tval_Loss: 0.005562\n",
      "Train Epoch: 6996/10000 (70%)\ttrain_Loss: 0.004971\tval_Loss: 0.005564\n",
      "Train Epoch: 6997/10000 (70%)\ttrain_Loss: 0.004971\tval_Loss: 0.005601\n",
      "Train Epoch: 6998/10000 (70%)\ttrain_Loss: 0.004975\tval_Loss: 0.005543\n",
      "Train Epoch: 6999/10000 (70%)\ttrain_Loss: 0.004980\tval_Loss: 0.005627\n",
      "Train Epoch: 7000/10000 (70%)\ttrain_Loss: 0.004983\tval_Loss: 0.005540\n",
      "Train Epoch: 7001/10000 (70%)\ttrain_Loss: 0.004984\tval_Loss: 0.005622\n",
      "Train Epoch: 7002/10000 (70%)\ttrain_Loss: 0.004981\tval_Loss: 0.005549\n",
      "Train Epoch: 7003/10000 (70%)\ttrain_Loss: 0.004976\tval_Loss: 0.005592\n",
      "Train Epoch: 7004/10000 (70%)\ttrain_Loss: 0.004971\tval_Loss: 0.005570\n",
      "Train Epoch: 7005/10000 (70%)\ttrain_Loss: 0.004969\tval_Loss: 0.005564\n",
      "Train Epoch: 7006/10000 (70%)\ttrain_Loss: 0.004970\tval_Loss: 0.005594\n",
      "Train Epoch: 7007/10000 (70%)\ttrain_Loss: 0.004972\tval_Loss: 0.005549\n",
      "Train Epoch: 7008/10000 (70%)\ttrain_Loss: 0.004974\tval_Loss: 0.005605\n",
      "Train Epoch: 7009/10000 (70%)\ttrain_Loss: 0.004975\tval_Loss: 0.005547\n",
      "Train Epoch: 7010/10000 (70%)\ttrain_Loss: 0.004974\tval_Loss: 0.005598\n",
      "Train Epoch: 7011/10000 (70%)\ttrain_Loss: 0.004972\tval_Loss: 0.005554\n",
      "Train Epoch: 7012/10000 (70%)\ttrain_Loss: 0.004970\tval_Loss: 0.005580\n",
      "Train Epoch: 7013/10000 (70%)\ttrain_Loss: 0.004969\tval_Loss: 0.005567\n",
      "Train Epoch: 7014/10000 (70%)\ttrain_Loss: 0.004968\tval_Loss: 0.005564\n",
      "Train Epoch: 7015/10000 (70%)\ttrain_Loss: 0.004968\tval_Loss: 0.005581\n",
      "Train Epoch: 7016/10000 (70%)\ttrain_Loss: 0.004968\tval_Loss: 0.005555\n",
      "Train Epoch: 7017/10000 (70%)\ttrain_Loss: 0.004969\tval_Loss: 0.005588\n",
      "Train Epoch: 7018/10000 (70%)\ttrain_Loss: 0.004969\tval_Loss: 0.005553\n",
      "Train Epoch: 7019/10000 (70%)\ttrain_Loss: 0.004969\tval_Loss: 0.005587\n",
      "Train Epoch: 7020/10000 (70%)\ttrain_Loss: 0.004969\tval_Loss: 0.005556\n",
      "Train Epoch: 7021/10000 (70%)\ttrain_Loss: 0.004968\tval_Loss: 0.005580\n",
      "Train Epoch: 7022/10000 (70%)\ttrain_Loss: 0.004967\tval_Loss: 0.005561\n",
      "Train Epoch: 7023/10000 (70%)\ttrain_Loss: 0.004967\tval_Loss: 0.005570\n",
      "Train Epoch: 7024/10000 (70%)\ttrain_Loss: 0.004966\tval_Loss: 0.005568\n",
      "Train Epoch: 7025/10000 (70%)\ttrain_Loss: 0.004966\tval_Loss: 0.005561\n",
      "Train Epoch: 7026/10000 (70%)\ttrain_Loss: 0.004966\tval_Loss: 0.005575\n",
      "Train Epoch: 7027/10000 (70%)\ttrain_Loss: 0.004966\tval_Loss: 0.005556\n",
      "Train Epoch: 7028/10000 (70%)\ttrain_Loss: 0.004966\tval_Loss: 0.005581\n",
      "Train Epoch: 7029/10000 (70%)\ttrain_Loss: 0.004967\tval_Loss: 0.005554\n",
      "Train Epoch: 7030/10000 (70%)\ttrain_Loss: 0.004967\tval_Loss: 0.005584\n",
      "Train Epoch: 7031/10000 (70%)\ttrain_Loss: 0.004967\tval_Loss: 0.005554\n",
      "Train Epoch: 7032/10000 (70%)\ttrain_Loss: 0.004966\tval_Loss: 0.005580\n",
      "Train Epoch: 7033/10000 (70%)\ttrain_Loss: 0.004966\tval_Loss: 0.005555\n",
      "Train Epoch: 7034/10000 (70%)\ttrain_Loss: 0.004965\tval_Loss: 0.005573\n",
      "Train Epoch: 7035/10000 (70%)\ttrain_Loss: 0.004965\tval_Loss: 0.005557\n",
      "Train Epoch: 7036/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005566\n",
      "Train Epoch: 7037/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005563\n",
      "Train Epoch: 7038/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005563\n",
      "Train Epoch: 7039/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005568\n",
      "Train Epoch: 7040/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005560\n",
      "Train Epoch: 7041/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7042/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005555\n",
      "Train Epoch: 7043/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005573\n",
      "Train Epoch: 7044/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005552\n",
      "Train Epoch: 7045/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005576\n",
      "Train Epoch: 7046/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005550\n",
      "Train Epoch: 7047/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005579\n",
      "Train Epoch: 7048/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005549\n",
      "Train Epoch: 7049/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005581\n",
      "Train Epoch: 7050/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005547\n",
      "Train Epoch: 7051/10000 (70%)\ttrain_Loss: 0.004964\tval_Loss: 0.005581\n",
      "Train Epoch: 7052/10000 (71%)\ttrain_Loss: 0.004964\tval_Loss: 0.005543\n",
      "Train Epoch: 7053/10000 (71%)\ttrain_Loss: 0.004964\tval_Loss: 0.005583\n",
      "Train Epoch: 7054/10000 (71%)\ttrain_Loss: 0.004964\tval_Loss: 0.005540\n",
      "Train Epoch: 7055/10000 (71%)\ttrain_Loss: 0.004965\tval_Loss: 0.005588\n",
      "Train Epoch: 7056/10000 (71%)\ttrain_Loss: 0.004965\tval_Loss: 0.005539\n",
      "Train Epoch: 7057/10000 (71%)\ttrain_Loss: 0.004966\tval_Loss: 0.005597\n",
      "Train Epoch: 7058/10000 (71%)\ttrain_Loss: 0.004967\tval_Loss: 0.005535\n",
      "Train Epoch: 7059/10000 (71%)\ttrain_Loss: 0.004968\tval_Loss: 0.005608\n",
      "Train Epoch: 7060/10000 (71%)\ttrain_Loss: 0.004970\tval_Loss: 0.005530\n",
      "Train Epoch: 7061/10000 (71%)\ttrain_Loss: 0.004973\tval_Loss: 0.005620\n",
      "Train Epoch: 7062/10000 (71%)\ttrain_Loss: 0.004975\tval_Loss: 0.005526\n",
      "Train Epoch: 7063/10000 (71%)\ttrain_Loss: 0.004977\tval_Loss: 0.005630\n",
      "Train Epoch: 7064/10000 (71%)\ttrain_Loss: 0.004979\tval_Loss: 0.005524\n",
      "Train Epoch: 7065/10000 (71%)\ttrain_Loss: 0.004980\tval_Loss: 0.005633\n",
      "Train Epoch: 7066/10000 (71%)\ttrain_Loss: 0.004980\tval_Loss: 0.005525\n",
      "Train Epoch: 7067/10000 (71%)\ttrain_Loss: 0.004978\tval_Loss: 0.005621\n",
      "Train Epoch: 7068/10000 (71%)\ttrain_Loss: 0.004975\tval_Loss: 0.005529\n",
      "Train Epoch: 7069/10000 (71%)\ttrain_Loss: 0.004971\tval_Loss: 0.005596\n",
      "Train Epoch: 7070/10000 (71%)\ttrain_Loss: 0.004966\tval_Loss: 0.005537\n",
      "Train Epoch: 7071/10000 (71%)\ttrain_Loss: 0.004962\tval_Loss: 0.005569\n",
      "Train Epoch: 7072/10000 (71%)\ttrain_Loss: 0.004959\tval_Loss: 0.005554\n",
      "Train Epoch: 7073/10000 (71%)\ttrain_Loss: 0.004958\tval_Loss: 0.005549\n",
      "Train Epoch: 7074/10000 (71%)\ttrain_Loss: 0.004959\tval_Loss: 0.005573\n",
      "Train Epoch: 7075/10000 (71%)\ttrain_Loss: 0.004960\tval_Loss: 0.005538\n",
      "Train Epoch: 7076/10000 (71%)\ttrain_Loss: 0.004961\tval_Loss: 0.005590\n",
      "Train Epoch: 7077/10000 (71%)\ttrain_Loss: 0.004963\tval_Loss: 0.005533\n",
      "Train Epoch: 7078/10000 (71%)\ttrain_Loss: 0.004964\tval_Loss: 0.005597\n",
      "Train Epoch: 7079/10000 (71%)\ttrain_Loss: 0.004965\tval_Loss: 0.005530\n",
      "Train Epoch: 7080/10000 (71%)\ttrain_Loss: 0.004965\tval_Loss: 0.005594\n",
      "Train Epoch: 7081/10000 (71%)\ttrain_Loss: 0.004964\tval_Loss: 0.005532\n",
      "Train Epoch: 7082/10000 (71%)\ttrain_Loss: 0.004963\tval_Loss: 0.005581\n",
      "Train Epoch: 7083/10000 (71%)\ttrain_Loss: 0.004961\tval_Loss: 0.005538\n",
      "Train Epoch: 7084/10000 (71%)\ttrain_Loss: 0.004959\tval_Loss: 0.005567\n",
      "Train Epoch: 7085/10000 (71%)\ttrain_Loss: 0.004957\tval_Loss: 0.005548\n",
      "Train Epoch: 7086/10000 (71%)\ttrain_Loss: 0.004957\tval_Loss: 0.005554\n",
      "Train Epoch: 7087/10000 (71%)\ttrain_Loss: 0.004956\tval_Loss: 0.005558\n",
      "Train Epoch: 7088/10000 (71%)\ttrain_Loss: 0.004956\tval_Loss: 0.005545\n",
      "Train Epoch: 7089/10000 (71%)\ttrain_Loss: 0.004956\tval_Loss: 0.005567\n",
      "Train Epoch: 7090/10000 (71%)\ttrain_Loss: 0.004957\tval_Loss: 0.005538\n",
      "Train Epoch: 7091/10000 (71%)\ttrain_Loss: 0.004957\tval_Loss: 0.005574\n",
      "Train Epoch: 7092/10000 (71%)\ttrain_Loss: 0.004958\tval_Loss: 0.005534\n",
      "Train Epoch: 7093/10000 (71%)\ttrain_Loss: 0.004958\tval_Loss: 0.005578\n",
      "Train Epoch: 7094/10000 (71%)\ttrain_Loss: 0.004959\tval_Loss: 0.005533\n",
      "Train Epoch: 7095/10000 (71%)\ttrain_Loss: 0.004959\tval_Loss: 0.005579\n",
      "Train Epoch: 7096/10000 (71%)\ttrain_Loss: 0.004958\tval_Loss: 0.005533\n",
      "Train Epoch: 7097/10000 (71%)\ttrain_Loss: 0.004958\tval_Loss: 0.005578\n",
      "Train Epoch: 7098/10000 (71%)\ttrain_Loss: 0.004958\tval_Loss: 0.005532\n",
      "Train Epoch: 7099/10000 (71%)\ttrain_Loss: 0.004958\tval_Loss: 0.005575\n",
      "Train Epoch: 7100/10000 (71%)\ttrain_Loss: 0.004957\tval_Loss: 0.005532\n",
      "Train Epoch: 7101/10000 (71%)\ttrain_Loss: 0.004957\tval_Loss: 0.005573\n",
      "Train Epoch: 7102/10000 (71%)\ttrain_Loss: 0.004957\tval_Loss: 0.005534\n",
      "Train Epoch: 7103/10000 (71%)\ttrain_Loss: 0.004956\tval_Loss: 0.005571\n",
      "Train Epoch: 7104/10000 (71%)\ttrain_Loss: 0.004956\tval_Loss: 0.005535\n",
      "Train Epoch: 7105/10000 (71%)\ttrain_Loss: 0.004955\tval_Loss: 0.005568\n",
      "Train Epoch: 7106/10000 (71%)\ttrain_Loss: 0.004955\tval_Loss: 0.005536\n",
      "Train Epoch: 7107/10000 (71%)\ttrain_Loss: 0.004955\tval_Loss: 0.005567\n",
      "Train Epoch: 7108/10000 (71%)\ttrain_Loss: 0.004955\tval_Loss: 0.005535\n",
      "Train Epoch: 7109/10000 (71%)\ttrain_Loss: 0.004955\tval_Loss: 0.005568\n",
      "Train Epoch: 7110/10000 (71%)\ttrain_Loss: 0.004954\tval_Loss: 0.005533\n",
      "Train Epoch: 7111/10000 (71%)\ttrain_Loss: 0.004954\tval_Loss: 0.005568\n",
      "Train Epoch: 7112/10000 (71%)\ttrain_Loss: 0.004955\tval_Loss: 0.005531\n",
      "Train Epoch: 7113/10000 (71%)\ttrain_Loss: 0.004955\tval_Loss: 0.005573\n",
      "Train Epoch: 7114/10000 (71%)\ttrain_Loss: 0.004955\tval_Loss: 0.005528\n",
      "Train Epoch: 7115/10000 (71%)\ttrain_Loss: 0.004956\tval_Loss: 0.005580\n",
      "Train Epoch: 7116/10000 (71%)\ttrain_Loss: 0.004957\tval_Loss: 0.005524\n",
      "Train Epoch: 7117/10000 (71%)\ttrain_Loss: 0.004958\tval_Loss: 0.005589\n",
      "Train Epoch: 7118/10000 (71%)\ttrain_Loss: 0.004959\tval_Loss: 0.005519\n",
      "Train Epoch: 7119/10000 (71%)\ttrain_Loss: 0.004961\tval_Loss: 0.005601\n",
      "Train Epoch: 7120/10000 (71%)\ttrain_Loss: 0.004964\tval_Loss: 0.005515\n",
      "Train Epoch: 7121/10000 (71%)\ttrain_Loss: 0.004966\tval_Loss: 0.005615\n",
      "Train Epoch: 7122/10000 (71%)\ttrain_Loss: 0.004969\tval_Loss: 0.005512\n",
      "Train Epoch: 7123/10000 (71%)\ttrain_Loss: 0.004972\tval_Loss: 0.005626\n",
      "Train Epoch: 7124/10000 (71%)\ttrain_Loss: 0.004974\tval_Loss: 0.005511\n",
      "Train Epoch: 7125/10000 (71%)\ttrain_Loss: 0.004975\tval_Loss: 0.005626\n",
      "Train Epoch: 7126/10000 (71%)\ttrain_Loss: 0.004973\tval_Loss: 0.005512\n",
      "Train Epoch: 7127/10000 (71%)\ttrain_Loss: 0.004970\tval_Loss: 0.005606\n",
      "Train Epoch: 7128/10000 (71%)\ttrain_Loss: 0.004965\tval_Loss: 0.005518\n",
      "Train Epoch: 7129/10000 (71%)\ttrain_Loss: 0.004959\tval_Loss: 0.005573\n",
      "Train Epoch: 7130/10000 (71%)\ttrain_Loss: 0.004954\tval_Loss: 0.005533\n",
      "Train Epoch: 7131/10000 (71%)\ttrain_Loss: 0.004950\tval_Loss: 0.005543\n",
      "Train Epoch: 7132/10000 (71%)\ttrain_Loss: 0.004949\tval_Loss: 0.005557\n",
      "Train Epoch: 7133/10000 (71%)\ttrain_Loss: 0.004950\tval_Loss: 0.005526\n",
      "Train Epoch: 7134/10000 (71%)\ttrain_Loss: 0.004952\tval_Loss: 0.005577\n",
      "Train Epoch: 7135/10000 (71%)\ttrain_Loss: 0.004954\tval_Loss: 0.005518\n",
      "Train Epoch: 7136/10000 (71%)\ttrain_Loss: 0.004956\tval_Loss: 0.005587\n",
      "Train Epoch: 7137/10000 (71%)\ttrain_Loss: 0.004957\tval_Loss: 0.005516\n",
      "Train Epoch: 7138/10000 (71%)\ttrain_Loss: 0.004957\tval_Loss: 0.005583\n",
      "Train Epoch: 7139/10000 (71%)\ttrain_Loss: 0.004956\tval_Loss: 0.005519\n",
      "Train Epoch: 7140/10000 (71%)\ttrain_Loss: 0.004954\tval_Loss: 0.005570\n",
      "Train Epoch: 7141/10000 (71%)\ttrain_Loss: 0.004952\tval_Loss: 0.005526\n",
      "Train Epoch: 7142/10000 (71%)\ttrain_Loss: 0.004950\tval_Loss: 0.005553\n",
      "Train Epoch: 7143/10000 (71%)\ttrain_Loss: 0.004948\tval_Loss: 0.005538\n",
      "Train Epoch: 7144/10000 (71%)\ttrain_Loss: 0.004947\tval_Loss: 0.005538\n",
      "Train Epoch: 7145/10000 (71%)\ttrain_Loss: 0.004947\tval_Loss: 0.005550\n",
      "Train Epoch: 7146/10000 (71%)\ttrain_Loss: 0.004948\tval_Loss: 0.005529\n",
      "Train Epoch: 7147/10000 (71%)\ttrain_Loss: 0.004948\tval_Loss: 0.005560\n",
      "Train Epoch: 7148/10000 (71%)\ttrain_Loss: 0.004949\tval_Loss: 0.005524\n",
      "Train Epoch: 7149/10000 (71%)\ttrain_Loss: 0.004949\tval_Loss: 0.005565\n",
      "Train Epoch: 7150/10000 (71%)\ttrain_Loss: 0.004950\tval_Loss: 0.005522\n",
      "Train Epoch: 7151/10000 (72%)\ttrain_Loss: 0.004950\tval_Loss: 0.005565\n",
      "Train Epoch: 7152/10000 (72%)\ttrain_Loss: 0.004949\tval_Loss: 0.005523\n",
      "Train Epoch: 7153/10000 (72%)\ttrain_Loss: 0.004949\tval_Loss: 0.005561\n",
      "Train Epoch: 7154/10000 (72%)\ttrain_Loss: 0.004948\tval_Loss: 0.005525\n",
      "Train Epoch: 7155/10000 (72%)\ttrain_Loss: 0.004948\tval_Loss: 0.005554\n",
      "Train Epoch: 7156/10000 (72%)\ttrain_Loss: 0.004947\tval_Loss: 0.005529\n",
      "Train Epoch: 7157/10000 (72%)\ttrain_Loss: 0.004946\tval_Loss: 0.005547\n",
      "Train Epoch: 7158/10000 (72%)\ttrain_Loss: 0.004946\tval_Loss: 0.005533\n",
      "Train Epoch: 7159/10000 (72%)\ttrain_Loss: 0.004945\tval_Loss: 0.005542\n",
      "Train Epoch: 7160/10000 (72%)\ttrain_Loss: 0.004945\tval_Loss: 0.005536\n",
      "Train Epoch: 7161/10000 (72%)\ttrain_Loss: 0.004945\tval_Loss: 0.005539\n",
      "Train Epoch: 7162/10000 (72%)\ttrain_Loss: 0.004945\tval_Loss: 0.005538\n",
      "Train Epoch: 7163/10000 (72%)\ttrain_Loss: 0.004944\tval_Loss: 0.005536\n",
      "Train Epoch: 7164/10000 (72%)\ttrain_Loss: 0.004944\tval_Loss: 0.005540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7165/10000 (72%)\ttrain_Loss: 0.004944\tval_Loss: 0.005533\n",
      "Train Epoch: 7166/10000 (72%)\ttrain_Loss: 0.004944\tval_Loss: 0.005541\n",
      "Train Epoch: 7167/10000 (72%)\ttrain_Loss: 0.004944\tval_Loss: 0.005530\n",
      "Train Epoch: 7168/10000 (72%)\ttrain_Loss: 0.004944\tval_Loss: 0.005545\n",
      "Train Epoch: 7169/10000 (72%)\ttrain_Loss: 0.004944\tval_Loss: 0.005527\n",
      "Train Epoch: 7170/10000 (72%)\ttrain_Loss: 0.004944\tval_Loss: 0.005549\n",
      "Train Epoch: 7171/10000 (72%)\ttrain_Loss: 0.004944\tval_Loss: 0.005523\n",
      "Train Epoch: 7172/10000 (72%)\ttrain_Loss: 0.004945\tval_Loss: 0.005555\n",
      "Train Epoch: 7173/10000 (72%)\ttrain_Loss: 0.004945\tval_Loss: 0.005517\n",
      "Train Epoch: 7174/10000 (72%)\ttrain_Loss: 0.004946\tval_Loss: 0.005565\n",
      "Train Epoch: 7175/10000 (72%)\ttrain_Loss: 0.004947\tval_Loss: 0.005511\n",
      "Train Epoch: 7176/10000 (72%)\ttrain_Loss: 0.004949\tval_Loss: 0.005578\n",
      "Train Epoch: 7177/10000 (72%)\ttrain_Loss: 0.004951\tval_Loss: 0.005504\n",
      "Train Epoch: 7178/10000 (72%)\ttrain_Loss: 0.004955\tval_Loss: 0.005602\n",
      "Train Epoch: 7179/10000 (72%)\ttrain_Loss: 0.004959\tval_Loss: 0.005500\n",
      "Train Epoch: 7180/10000 (72%)\ttrain_Loss: 0.004966\tval_Loss: 0.005633\n",
      "Train Epoch: 7181/10000 (72%)\ttrain_Loss: 0.004973\tval_Loss: 0.005499\n",
      "Train Epoch: 7182/10000 (72%)\ttrain_Loss: 0.004981\tval_Loss: 0.005660\n",
      "Train Epoch: 7183/10000 (72%)\ttrain_Loss: 0.004987\tval_Loss: 0.005498\n",
      "Train Epoch: 7184/10000 (72%)\ttrain_Loss: 0.004988\tval_Loss: 0.005649\n",
      "Train Epoch: 7185/10000 (72%)\ttrain_Loss: 0.004982\tval_Loss: 0.005497\n",
      "Train Epoch: 7186/10000 (72%)\ttrain_Loss: 0.004969\tval_Loss: 0.005590\n",
      "Train Epoch: 7187/10000 (72%)\ttrain_Loss: 0.004955\tval_Loss: 0.005513\n",
      "Train Epoch: 7188/10000 (72%)\ttrain_Loss: 0.004944\tval_Loss: 0.005530\n",
      "Train Epoch: 7189/10000 (72%)\ttrain_Loss: 0.004941\tval_Loss: 0.005558\n",
      "Train Epoch: 7190/10000 (72%)\ttrain_Loss: 0.004944\tval_Loss: 0.005503\n",
      "Train Epoch: 7191/10000 (72%)\ttrain_Loss: 0.004951\tval_Loss: 0.005597\n",
      "Train Epoch: 7192/10000 (72%)\ttrain_Loss: 0.004957\tval_Loss: 0.005498\n",
      "Train Epoch: 7193/10000 (72%)\ttrain_Loss: 0.004959\tval_Loss: 0.005593\n",
      "Train Epoch: 7194/10000 (72%)\ttrain_Loss: 0.004955\tval_Loss: 0.005503\n",
      "Train Epoch: 7195/10000 (72%)\ttrain_Loss: 0.004949\tval_Loss: 0.005554\n",
      "Train Epoch: 7196/10000 (72%)\ttrain_Loss: 0.004943\tval_Loss: 0.005526\n",
      "Train Epoch: 7197/10000 (72%)\ttrain_Loss: 0.004940\tval_Loss: 0.005521\n",
      "Train Epoch: 7198/10000 (72%)\ttrain_Loss: 0.004940\tval_Loss: 0.005558\n",
      "Train Epoch: 7199/10000 (72%)\ttrain_Loss: 0.004943\tval_Loss: 0.005507\n",
      "Train Epoch: 7200/10000 (72%)\ttrain_Loss: 0.004946\tval_Loss: 0.005572\n",
      "Train Epoch: 7201/10000 (72%)\ttrain_Loss: 0.004947\tval_Loss: 0.005506\n",
      "Train Epoch: 7202/10000 (72%)\ttrain_Loss: 0.004946\tval_Loss: 0.005559\n",
      "Train Epoch: 7203/10000 (72%)\ttrain_Loss: 0.004943\tval_Loss: 0.005515\n",
      "Train Epoch: 7204/10000 (72%)\ttrain_Loss: 0.004940\tval_Loss: 0.005536\n",
      "Train Epoch: 7205/10000 (72%)\ttrain_Loss: 0.004939\tval_Loss: 0.005533\n",
      "Train Epoch: 7206/10000 (72%)\ttrain_Loss: 0.004938\tval_Loss: 0.005517\n",
      "Train Epoch: 7207/10000 (72%)\ttrain_Loss: 0.004939\tval_Loss: 0.005550\n",
      "Train Epoch: 7208/10000 (72%)\ttrain_Loss: 0.004940\tval_Loss: 0.005509\n",
      "Train Epoch: 7209/10000 (72%)\ttrain_Loss: 0.004941\tval_Loss: 0.005556\n",
      "Train Epoch: 7210/10000 (72%)\ttrain_Loss: 0.004941\tval_Loss: 0.005510\n",
      "Train Epoch: 7211/10000 (72%)\ttrain_Loss: 0.004941\tval_Loss: 0.005548\n",
      "Train Epoch: 7212/10000 (72%)\ttrain_Loss: 0.004939\tval_Loss: 0.005517\n",
      "Train Epoch: 7213/10000 (72%)\ttrain_Loss: 0.004938\tval_Loss: 0.005533\n",
      "Train Epoch: 7214/10000 (72%)\ttrain_Loss: 0.004937\tval_Loss: 0.005529\n",
      "Train Epoch: 7215/10000 (72%)\ttrain_Loss: 0.004937\tval_Loss: 0.005520\n",
      "Train Epoch: 7216/10000 (72%)\ttrain_Loss: 0.004937\tval_Loss: 0.005540\n",
      "Train Epoch: 7217/10000 (72%)\ttrain_Loss: 0.004938\tval_Loss: 0.005513\n",
      "Train Epoch: 7218/10000 (72%)\ttrain_Loss: 0.004938\tval_Loss: 0.005544\n",
      "Train Epoch: 7219/10000 (72%)\ttrain_Loss: 0.004938\tval_Loss: 0.005512\n",
      "Train Epoch: 7220/10000 (72%)\ttrain_Loss: 0.004938\tval_Loss: 0.005541\n",
      "Train Epoch: 7221/10000 (72%)\ttrain_Loss: 0.004937\tval_Loss: 0.005516\n",
      "Train Epoch: 7222/10000 (72%)\ttrain_Loss: 0.004937\tval_Loss: 0.005535\n",
      "Train Epoch: 7223/10000 (72%)\ttrain_Loss: 0.004936\tval_Loss: 0.005521\n",
      "Train Epoch: 7224/10000 (72%)\ttrain_Loss: 0.004936\tval_Loss: 0.005528\n",
      "Train Epoch: 7225/10000 (72%)\ttrain_Loss: 0.004935\tval_Loss: 0.005527\n",
      "Train Epoch: 7226/10000 (72%)\ttrain_Loss: 0.004935\tval_Loss: 0.005521\n",
      "Train Epoch: 7227/10000 (72%)\ttrain_Loss: 0.004935\tval_Loss: 0.005531\n",
      "Train Epoch: 7228/10000 (72%)\ttrain_Loss: 0.004935\tval_Loss: 0.005518\n",
      "Train Epoch: 7229/10000 (72%)\ttrain_Loss: 0.004935\tval_Loss: 0.005534\n",
      "Train Epoch: 7230/10000 (72%)\ttrain_Loss: 0.004935\tval_Loss: 0.005516\n",
      "Train Epoch: 7231/10000 (72%)\ttrain_Loss: 0.004935\tval_Loss: 0.005534\n",
      "Train Epoch: 7232/10000 (72%)\ttrain_Loss: 0.004935\tval_Loss: 0.005516\n",
      "Train Epoch: 7233/10000 (72%)\ttrain_Loss: 0.004935\tval_Loss: 0.005531\n",
      "Train Epoch: 7234/10000 (72%)\ttrain_Loss: 0.004934\tval_Loss: 0.005517\n",
      "Train Epoch: 7235/10000 (72%)\ttrain_Loss: 0.004934\tval_Loss: 0.005528\n",
      "Train Epoch: 7236/10000 (72%)\ttrain_Loss: 0.004934\tval_Loss: 0.005520\n",
      "Train Epoch: 7237/10000 (72%)\ttrain_Loss: 0.004934\tval_Loss: 0.005524\n",
      "Train Epoch: 7238/10000 (72%)\ttrain_Loss: 0.004933\tval_Loss: 0.005522\n",
      "Train Epoch: 7239/10000 (72%)\ttrain_Loss: 0.004933\tval_Loss: 0.005521\n",
      "Train Epoch: 7240/10000 (72%)\ttrain_Loss: 0.004933\tval_Loss: 0.005524\n",
      "Train Epoch: 7241/10000 (72%)\ttrain_Loss: 0.004933\tval_Loss: 0.005519\n",
      "Train Epoch: 7242/10000 (72%)\ttrain_Loss: 0.004933\tval_Loss: 0.005525\n",
      "Train Epoch: 7243/10000 (72%)\ttrain_Loss: 0.004933\tval_Loss: 0.005516\n",
      "Train Epoch: 7244/10000 (72%)\ttrain_Loss: 0.004933\tval_Loss: 0.005527\n",
      "Train Epoch: 7245/10000 (72%)\ttrain_Loss: 0.004932\tval_Loss: 0.005515\n",
      "Train Epoch: 7246/10000 (72%)\ttrain_Loss: 0.004932\tval_Loss: 0.005530\n",
      "Train Epoch: 7247/10000 (72%)\ttrain_Loss: 0.004932\tval_Loss: 0.005513\n",
      "Train Epoch: 7248/10000 (72%)\ttrain_Loss: 0.004932\tval_Loss: 0.005532\n",
      "Train Epoch: 7249/10000 (72%)\ttrain_Loss: 0.004932\tval_Loss: 0.005510\n",
      "Train Epoch: 7250/10000 (72%)\ttrain_Loss: 0.004932\tval_Loss: 0.005534\n",
      "Train Epoch: 7251/10000 (72%)\ttrain_Loss: 0.004932\tval_Loss: 0.005507\n",
      "Train Epoch: 7252/10000 (73%)\ttrain_Loss: 0.004933\tval_Loss: 0.005537\n",
      "Train Epoch: 7253/10000 (73%)\ttrain_Loss: 0.004933\tval_Loss: 0.005503\n",
      "Train Epoch: 7254/10000 (73%)\ttrain_Loss: 0.004933\tval_Loss: 0.005542\n",
      "Train Epoch: 7255/10000 (73%)\ttrain_Loss: 0.004934\tval_Loss: 0.005498\n",
      "Train Epoch: 7256/10000 (73%)\ttrain_Loss: 0.004935\tval_Loss: 0.005552\n",
      "Train Epoch: 7257/10000 (73%)\ttrain_Loss: 0.004936\tval_Loss: 0.005493\n",
      "Train Epoch: 7258/10000 (73%)\ttrain_Loss: 0.004938\tval_Loss: 0.005567\n",
      "Train Epoch: 7259/10000 (73%)\ttrain_Loss: 0.004940\tval_Loss: 0.005487\n",
      "Train Epoch: 7260/10000 (73%)\ttrain_Loss: 0.004944\tval_Loss: 0.005591\n",
      "Train Epoch: 7261/10000 (73%)\ttrain_Loss: 0.004950\tval_Loss: 0.005483\n",
      "Train Epoch: 7262/10000 (73%)\ttrain_Loss: 0.004956\tval_Loss: 0.005622\n",
      "Train Epoch: 7263/10000 (73%)\ttrain_Loss: 0.004964\tval_Loss: 0.005482\n",
      "Train Epoch: 7264/10000 (73%)\ttrain_Loss: 0.004971\tval_Loss: 0.005645\n",
      "Train Epoch: 7265/10000 (73%)\ttrain_Loss: 0.004975\tval_Loss: 0.005482\n",
      "Train Epoch: 7266/10000 (73%)\ttrain_Loss: 0.004976\tval_Loss: 0.005632\n",
      "Train Epoch: 7267/10000 (73%)\ttrain_Loss: 0.004968\tval_Loss: 0.005481\n",
      "Train Epoch: 7268/10000 (73%)\ttrain_Loss: 0.004956\tval_Loss: 0.005572\n",
      "Train Epoch: 7269/10000 (73%)\ttrain_Loss: 0.004942\tval_Loss: 0.005497\n",
      "Train Epoch: 7270/10000 (73%)\ttrain_Loss: 0.004931\tval_Loss: 0.005512\n",
      "Train Epoch: 7271/10000 (73%)\ttrain_Loss: 0.004928\tval_Loss: 0.005541\n",
      "Train Epoch: 7272/10000 (73%)\ttrain_Loss: 0.004932\tval_Loss: 0.005485\n",
      "Train Epoch: 7273/10000 (73%)\ttrain_Loss: 0.004939\tval_Loss: 0.005579\n",
      "Train Epoch: 7274/10000 (73%)\ttrain_Loss: 0.004945\tval_Loss: 0.005481\n",
      "Train Epoch: 7275/10000 (73%)\ttrain_Loss: 0.004946\tval_Loss: 0.005577\n",
      "Train Epoch: 7276/10000 (73%)\ttrain_Loss: 0.004943\tval_Loss: 0.005487\n",
      "Train Epoch: 7277/10000 (73%)\ttrain_Loss: 0.004937\tval_Loss: 0.005541\n",
      "Train Epoch: 7278/10000 (73%)\ttrain_Loss: 0.004931\tval_Loss: 0.005508\n",
      "Train Epoch: 7279/10000 (73%)\ttrain_Loss: 0.004928\tval_Loss: 0.005506\n",
      "Train Epoch: 7280/10000 (73%)\ttrain_Loss: 0.004928\tval_Loss: 0.005538\n",
      "Train Epoch: 7281/10000 (73%)\ttrain_Loss: 0.004930\tval_Loss: 0.005490\n",
      "Train Epoch: 7282/10000 (73%)\ttrain_Loss: 0.004933\tval_Loss: 0.005554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7283/10000 (73%)\ttrain_Loss: 0.004934\tval_Loss: 0.005488\n",
      "Train Epoch: 7284/10000 (73%)\ttrain_Loss: 0.004934\tval_Loss: 0.005545\n",
      "Train Epoch: 7285/10000 (73%)\ttrain_Loss: 0.004932\tval_Loss: 0.005496\n",
      "Train Epoch: 7286/10000 (73%)\ttrain_Loss: 0.004929\tval_Loss: 0.005522\n",
      "Train Epoch: 7287/10000 (73%)\ttrain_Loss: 0.004927\tval_Loss: 0.005513\n",
      "Train Epoch: 7288/10000 (73%)\ttrain_Loss: 0.004926\tval_Loss: 0.005503\n",
      "Train Epoch: 7289/10000 (73%)\ttrain_Loss: 0.004927\tval_Loss: 0.005531\n",
      "Train Epoch: 7290/10000 (73%)\ttrain_Loss: 0.004928\tval_Loss: 0.005493\n",
      "Train Epoch: 7291/10000 (73%)\ttrain_Loss: 0.004929\tval_Loss: 0.005539\n",
      "Train Epoch: 7292/10000 (73%)\ttrain_Loss: 0.004929\tval_Loss: 0.005492\n",
      "Train Epoch: 7293/10000 (73%)\ttrain_Loss: 0.004929\tval_Loss: 0.005533\n",
      "Train Epoch: 7294/10000 (73%)\ttrain_Loss: 0.004928\tval_Loss: 0.005499\n",
      "Train Epoch: 7295/10000 (73%)\ttrain_Loss: 0.004926\tval_Loss: 0.005519\n",
      "Train Epoch: 7296/10000 (73%)\ttrain_Loss: 0.004925\tval_Loss: 0.005510\n",
      "Train Epoch: 7297/10000 (73%)\ttrain_Loss: 0.004925\tval_Loss: 0.005506\n",
      "Train Epoch: 7298/10000 (73%)\ttrain_Loss: 0.004925\tval_Loss: 0.005520\n",
      "Train Epoch: 7299/10000 (73%)\ttrain_Loss: 0.004925\tval_Loss: 0.005498\n",
      "Train Epoch: 7300/10000 (73%)\ttrain_Loss: 0.004925\tval_Loss: 0.005526\n",
      "Train Epoch: 7301/10000 (73%)\ttrain_Loss: 0.004926\tval_Loss: 0.005495\n",
      "Train Epoch: 7302/10000 (73%)\ttrain_Loss: 0.004926\tval_Loss: 0.005527\n",
      "Train Epoch: 7303/10000 (73%)\ttrain_Loss: 0.004926\tval_Loss: 0.005497\n",
      "Train Epoch: 7304/10000 (73%)\ttrain_Loss: 0.004925\tval_Loss: 0.005521\n",
      "Train Epoch: 7305/10000 (73%)\ttrain_Loss: 0.004924\tval_Loss: 0.005501\n",
      "Train Epoch: 7306/10000 (73%)\ttrain_Loss: 0.004924\tval_Loss: 0.005514\n",
      "Train Epoch: 7307/10000 (73%)\ttrain_Loss: 0.004923\tval_Loss: 0.005507\n",
      "Train Epoch: 7308/10000 (73%)\ttrain_Loss: 0.004923\tval_Loss: 0.005507\n",
      "Train Epoch: 7309/10000 (73%)\ttrain_Loss: 0.004923\tval_Loss: 0.005514\n",
      "Train Epoch: 7310/10000 (73%)\ttrain_Loss: 0.004923\tval_Loss: 0.005500\n",
      "Train Epoch: 7311/10000 (73%)\ttrain_Loss: 0.004923\tval_Loss: 0.005519\n",
      "Train Epoch: 7312/10000 (73%)\ttrain_Loss: 0.004923\tval_Loss: 0.005496\n",
      "Train Epoch: 7313/10000 (73%)\ttrain_Loss: 0.004924\tval_Loss: 0.005523\n",
      "Train Epoch: 7314/10000 (73%)\ttrain_Loss: 0.004924\tval_Loss: 0.005494\n",
      "Train Epoch: 7315/10000 (73%)\ttrain_Loss: 0.004924\tval_Loss: 0.005524\n",
      "Train Epoch: 7316/10000 (73%)\ttrain_Loss: 0.004924\tval_Loss: 0.005494\n",
      "Train Epoch: 7317/10000 (73%)\ttrain_Loss: 0.004923\tval_Loss: 0.005522\n",
      "Train Epoch: 7318/10000 (73%)\ttrain_Loss: 0.004923\tval_Loss: 0.005494\n",
      "Train Epoch: 7319/10000 (73%)\ttrain_Loss: 0.004923\tval_Loss: 0.005517\n",
      "Train Epoch: 7320/10000 (73%)\ttrain_Loss: 0.004922\tval_Loss: 0.005495\n",
      "Train Epoch: 7321/10000 (73%)\ttrain_Loss: 0.004922\tval_Loss: 0.005514\n",
      "Train Epoch: 7322/10000 (73%)\ttrain_Loss: 0.004922\tval_Loss: 0.005498\n",
      "Train Epoch: 7323/10000 (73%)\ttrain_Loss: 0.004921\tval_Loss: 0.005513\n",
      "Train Epoch: 7324/10000 (73%)\ttrain_Loss: 0.004921\tval_Loss: 0.005500\n",
      "Train Epoch: 7325/10000 (73%)\ttrain_Loss: 0.004921\tval_Loss: 0.005511\n",
      "Train Epoch: 7326/10000 (73%)\ttrain_Loss: 0.004921\tval_Loss: 0.005500\n",
      "Train Epoch: 7327/10000 (73%)\ttrain_Loss: 0.004921\tval_Loss: 0.005509\n",
      "Train Epoch: 7328/10000 (73%)\ttrain_Loss: 0.004920\tval_Loss: 0.005500\n",
      "Train Epoch: 7329/10000 (73%)\ttrain_Loss: 0.004920\tval_Loss: 0.005507\n",
      "Train Epoch: 7330/10000 (73%)\ttrain_Loss: 0.004920\tval_Loss: 0.005500\n",
      "Train Epoch: 7331/10000 (73%)\ttrain_Loss: 0.004920\tval_Loss: 0.005507\n",
      "Train Epoch: 7332/10000 (73%)\ttrain_Loss: 0.004920\tval_Loss: 0.005499\n",
      "Train Epoch: 7333/10000 (73%)\ttrain_Loss: 0.004920\tval_Loss: 0.005508\n",
      "Train Epoch: 7334/10000 (73%)\ttrain_Loss: 0.004919\tval_Loss: 0.005499\n",
      "Train Epoch: 7335/10000 (73%)\ttrain_Loss: 0.004919\tval_Loss: 0.005509\n",
      "Train Epoch: 7336/10000 (73%)\ttrain_Loss: 0.004919\tval_Loss: 0.005496\n",
      "Train Epoch: 7337/10000 (73%)\ttrain_Loss: 0.004919\tval_Loss: 0.005511\n",
      "Train Epoch: 7338/10000 (73%)\ttrain_Loss: 0.004919\tval_Loss: 0.005492\n",
      "Train Epoch: 7339/10000 (73%)\ttrain_Loss: 0.004919\tval_Loss: 0.005514\n",
      "Train Epoch: 7340/10000 (73%)\ttrain_Loss: 0.004919\tval_Loss: 0.005489\n",
      "Train Epoch: 7341/10000 (73%)\ttrain_Loss: 0.004920\tval_Loss: 0.005521\n",
      "Train Epoch: 7342/10000 (73%)\ttrain_Loss: 0.004920\tval_Loss: 0.005483\n",
      "Train Epoch: 7343/10000 (73%)\ttrain_Loss: 0.004921\tval_Loss: 0.005535\n",
      "Train Epoch: 7344/10000 (73%)\ttrain_Loss: 0.004923\tval_Loss: 0.005474\n",
      "Train Epoch: 7345/10000 (73%)\ttrain_Loss: 0.004926\tval_Loss: 0.005560\n",
      "Train Epoch: 7346/10000 (73%)\ttrain_Loss: 0.004932\tval_Loss: 0.005466\n",
      "Train Epoch: 7347/10000 (73%)\ttrain_Loss: 0.004940\tval_Loss: 0.005605\n",
      "Train Epoch: 7348/10000 (73%)\ttrain_Loss: 0.004951\tval_Loss: 0.005465\n",
      "Train Epoch: 7349/10000 (73%)\ttrain_Loss: 0.004968\tval_Loss: 0.005672\n",
      "Train Epoch: 7350/10000 (73%)\ttrain_Loss: 0.004988\tval_Loss: 0.005476\n",
      "Train Epoch: 7351/10000 (74%)\ttrain_Loss: 0.005006\tval_Loss: 0.005711\n",
      "Train Epoch: 7352/10000 (74%)\ttrain_Loss: 0.005010\tval_Loss: 0.005472\n",
      "Train Epoch: 7353/10000 (74%)\ttrain_Loss: 0.004994\tval_Loss: 0.005623\n",
      "Train Epoch: 7354/10000 (74%)\ttrain_Loss: 0.004960\tval_Loss: 0.005469\n",
      "Train Epoch: 7355/10000 (74%)\ttrain_Loss: 0.004928\tval_Loss: 0.005496\n",
      "Train Epoch: 7356/10000 (74%)\ttrain_Loss: 0.004916\tval_Loss: 0.005550\n",
      "Train Epoch: 7357/10000 (74%)\ttrain_Loss: 0.004928\tval_Loss: 0.005461\n",
      "Train Epoch: 7358/10000 (74%)\ttrain_Loss: 0.004947\tval_Loss: 0.005614\n",
      "Train Epoch: 7359/10000 (74%)\ttrain_Loss: 0.004956\tval_Loss: 0.005460\n",
      "Train Epoch: 7360/10000 (74%)\ttrain_Loss: 0.004948\tval_Loss: 0.005555\n",
      "Train Epoch: 7361/10000 (74%)\ttrain_Loss: 0.004929\tval_Loss: 0.005486\n",
      "Train Epoch: 7362/10000 (74%)\ttrain_Loss: 0.004916\tval_Loss: 0.005479\n",
      "Train Epoch: 7363/10000 (74%)\ttrain_Loss: 0.004918\tval_Loss: 0.005554\n",
      "Train Epoch: 7364/10000 (74%)\ttrain_Loss: 0.004929\tval_Loss: 0.005462\n",
      "Train Epoch: 7365/10000 (74%)\ttrain_Loss: 0.004936\tval_Loss: 0.005565\n",
      "Train Epoch: 7366/10000 (74%)\ttrain_Loss: 0.004933\tval_Loss: 0.005469\n",
      "Train Epoch: 7367/10000 (74%)\ttrain_Loss: 0.004923\tval_Loss: 0.005506\n",
      "Train Epoch: 7368/10000 (74%)\ttrain_Loss: 0.004915\tval_Loss: 0.005511\n",
      "Train Epoch: 7369/10000 (74%)\ttrain_Loss: 0.004916\tval_Loss: 0.005471\n",
      "Train Epoch: 7370/10000 (74%)\ttrain_Loss: 0.004922\tval_Loss: 0.005548\n",
      "Train Epoch: 7371/10000 (74%)\ttrain_Loss: 0.004926\tval_Loss: 0.005468\n",
      "Train Epoch: 7372/10000 (74%)\ttrain_Loss: 0.004924\tval_Loss: 0.005526\n",
      "Train Epoch: 7373/10000 (74%)\ttrain_Loss: 0.004918\tval_Loss: 0.005489\n",
      "Train Epoch: 7374/10000 (74%)\ttrain_Loss: 0.004914\tval_Loss: 0.005486\n",
      "Train Epoch: 7375/10000 (74%)\ttrain_Loss: 0.004915\tval_Loss: 0.005523\n",
      "Train Epoch: 7376/10000 (74%)\ttrain_Loss: 0.004918\tval_Loss: 0.005471\n",
      "Train Epoch: 7377/10000 (74%)\ttrain_Loss: 0.004920\tval_Loss: 0.005528\n",
      "Train Epoch: 7378/10000 (74%)\ttrain_Loss: 0.004919\tval_Loss: 0.005478\n",
      "Train Epoch: 7379/10000 (74%)\ttrain_Loss: 0.004916\tval_Loss: 0.005501\n",
      "Train Epoch: 7380/10000 (74%)\ttrain_Loss: 0.004913\tval_Loss: 0.005500\n",
      "Train Epoch: 7381/10000 (74%)\ttrain_Loss: 0.004913\tval_Loss: 0.005479\n",
      "Train Epoch: 7382/10000 (74%)\ttrain_Loss: 0.004915\tval_Loss: 0.005519\n",
      "Train Epoch: 7383/10000 (74%)\ttrain_Loss: 0.004916\tval_Loss: 0.005475\n",
      "Train Epoch: 7384/10000 (74%)\ttrain_Loss: 0.004916\tval_Loss: 0.005512\n",
      "Train Epoch: 7385/10000 (74%)\ttrain_Loss: 0.004914\tval_Loss: 0.005485\n",
      "Train Epoch: 7386/10000 (74%)\ttrain_Loss: 0.004913\tval_Loss: 0.005492\n",
      "Train Epoch: 7387/10000 (74%)\ttrain_Loss: 0.004912\tval_Loss: 0.005503\n",
      "Train Epoch: 7388/10000 (74%)\ttrain_Loss: 0.004913\tval_Loss: 0.005479\n",
      "Train Epoch: 7389/10000 (74%)\ttrain_Loss: 0.004913\tval_Loss: 0.005511\n",
      "Train Epoch: 7390/10000 (74%)\ttrain_Loss: 0.004914\tval_Loss: 0.005479\n",
      "Train Epoch: 7391/10000 (74%)\ttrain_Loss: 0.004913\tval_Loss: 0.005503\n",
      "Train Epoch: 7392/10000 (74%)\ttrain_Loss: 0.004912\tval_Loss: 0.005488\n",
      "Train Epoch: 7393/10000 (74%)\ttrain_Loss: 0.004911\tval_Loss: 0.005490\n",
      "Train Epoch: 7394/10000 (74%)\ttrain_Loss: 0.004911\tval_Loss: 0.005500\n",
      "Train Epoch: 7395/10000 (74%)\ttrain_Loss: 0.004911\tval_Loss: 0.005481\n",
      "Train Epoch: 7396/10000 (74%)\ttrain_Loss: 0.004912\tval_Loss: 0.005505\n",
      "Train Epoch: 7397/10000 (74%)\ttrain_Loss: 0.004912\tval_Loss: 0.005481\n",
      "Train Epoch: 7398/10000 (74%)\ttrain_Loss: 0.004911\tval_Loss: 0.005499\n",
      "Train Epoch: 7399/10000 (74%)\ttrain_Loss: 0.004911\tval_Loss: 0.005488\n",
      "Train Epoch: 7400/10000 (74%)\ttrain_Loss: 0.004910\tval_Loss: 0.005489\n",
      "Train Epoch: 7401/10000 (74%)\ttrain_Loss: 0.004910\tval_Loss: 0.005495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7402/10000 (74%)\ttrain_Loss: 0.004910\tval_Loss: 0.005482\n",
      "Train Epoch: 7403/10000 (74%)\ttrain_Loss: 0.004910\tval_Loss: 0.005499\n",
      "Train Epoch: 7404/10000 (74%)\ttrain_Loss: 0.004910\tval_Loss: 0.005481\n",
      "Train Epoch: 7405/10000 (74%)\ttrain_Loss: 0.004910\tval_Loss: 0.005497\n",
      "Train Epoch: 7406/10000 (74%)\ttrain_Loss: 0.004910\tval_Loss: 0.005485\n",
      "Train Epoch: 7407/10000 (74%)\ttrain_Loss: 0.004910\tval_Loss: 0.005491\n",
      "Train Epoch: 7408/10000 (74%)\ttrain_Loss: 0.004909\tval_Loss: 0.005491\n",
      "Train Epoch: 7409/10000 (74%)\ttrain_Loss: 0.004909\tval_Loss: 0.005485\n",
      "Train Epoch: 7410/10000 (74%)\ttrain_Loss: 0.004909\tval_Loss: 0.005494\n",
      "Train Epoch: 7411/10000 (74%)\ttrain_Loss: 0.004909\tval_Loss: 0.005482\n",
      "Train Epoch: 7412/10000 (74%)\ttrain_Loss: 0.004909\tval_Loss: 0.005494\n",
      "Train Epoch: 7413/10000 (74%)\ttrain_Loss: 0.004909\tval_Loss: 0.005482\n",
      "Train Epoch: 7414/10000 (74%)\ttrain_Loss: 0.004909\tval_Loss: 0.005492\n",
      "Train Epoch: 7415/10000 (74%)\ttrain_Loss: 0.004908\tval_Loss: 0.005484\n",
      "Train Epoch: 7416/10000 (74%)\ttrain_Loss: 0.004908\tval_Loss: 0.005489\n",
      "Train Epoch: 7417/10000 (74%)\ttrain_Loss: 0.004908\tval_Loss: 0.005487\n",
      "Train Epoch: 7418/10000 (74%)\ttrain_Loss: 0.004908\tval_Loss: 0.005486\n",
      "Train Epoch: 7419/10000 (74%)\ttrain_Loss: 0.004908\tval_Loss: 0.005488\n",
      "Train Epoch: 7420/10000 (74%)\ttrain_Loss: 0.004908\tval_Loss: 0.005485\n",
      "Train Epoch: 7421/10000 (74%)\ttrain_Loss: 0.004907\tval_Loss: 0.005490\n",
      "Train Epoch: 7422/10000 (74%)\ttrain_Loss: 0.004907\tval_Loss: 0.005483\n",
      "Train Epoch: 7423/10000 (74%)\ttrain_Loss: 0.004907\tval_Loss: 0.005489\n",
      "Train Epoch: 7424/10000 (74%)\ttrain_Loss: 0.004907\tval_Loss: 0.005483\n",
      "Train Epoch: 7425/10000 (74%)\ttrain_Loss: 0.004907\tval_Loss: 0.005488\n",
      "Train Epoch: 7426/10000 (74%)\ttrain_Loss: 0.004907\tval_Loss: 0.005483\n",
      "Train Epoch: 7427/10000 (74%)\ttrain_Loss: 0.004907\tval_Loss: 0.005486\n",
      "Train Epoch: 7428/10000 (74%)\ttrain_Loss: 0.004906\tval_Loss: 0.005484\n",
      "Train Epoch: 7429/10000 (74%)\ttrain_Loss: 0.004906\tval_Loss: 0.005484\n",
      "Train Epoch: 7430/10000 (74%)\ttrain_Loss: 0.004906\tval_Loss: 0.005485\n",
      "Train Epoch: 7431/10000 (74%)\ttrain_Loss: 0.004906\tval_Loss: 0.005482\n",
      "Train Epoch: 7432/10000 (74%)\ttrain_Loss: 0.004906\tval_Loss: 0.005487\n",
      "Train Epoch: 7433/10000 (74%)\ttrain_Loss: 0.004906\tval_Loss: 0.005481\n",
      "Train Epoch: 7434/10000 (74%)\ttrain_Loss: 0.004906\tval_Loss: 0.005488\n",
      "Train Epoch: 7435/10000 (74%)\ttrain_Loss: 0.004905\tval_Loss: 0.005480\n",
      "Train Epoch: 7436/10000 (74%)\ttrain_Loss: 0.004905\tval_Loss: 0.005487\n",
      "Train Epoch: 7437/10000 (74%)\ttrain_Loss: 0.004905\tval_Loss: 0.005479\n",
      "Train Epoch: 7438/10000 (74%)\ttrain_Loss: 0.004905\tval_Loss: 0.005486\n",
      "Train Epoch: 7439/10000 (74%)\ttrain_Loss: 0.004905\tval_Loss: 0.005479\n",
      "Train Epoch: 7440/10000 (74%)\ttrain_Loss: 0.004905\tval_Loss: 0.005485\n",
      "Train Epoch: 7441/10000 (74%)\ttrain_Loss: 0.004905\tval_Loss: 0.005480\n",
      "Train Epoch: 7442/10000 (74%)\ttrain_Loss: 0.004904\tval_Loss: 0.005484\n",
      "Train Epoch: 7443/10000 (74%)\ttrain_Loss: 0.004904\tval_Loss: 0.005480\n",
      "Train Epoch: 7444/10000 (74%)\ttrain_Loss: 0.004904\tval_Loss: 0.005482\n",
      "Train Epoch: 7445/10000 (74%)\ttrain_Loss: 0.004904\tval_Loss: 0.005480\n",
      "Train Epoch: 7446/10000 (74%)\ttrain_Loss: 0.004904\tval_Loss: 0.005481\n",
      "Train Epoch: 7447/10000 (74%)\ttrain_Loss: 0.004904\tval_Loss: 0.005479\n",
      "Train Epoch: 7448/10000 (74%)\ttrain_Loss: 0.004903\tval_Loss: 0.005481\n",
      "Train Epoch: 7449/10000 (74%)\ttrain_Loss: 0.004903\tval_Loss: 0.005480\n",
      "Train Epoch: 7450/10000 (74%)\ttrain_Loss: 0.004903\tval_Loss: 0.005481\n",
      "Train Epoch: 7451/10000 (74%)\ttrain_Loss: 0.004903\tval_Loss: 0.005480\n",
      "Train Epoch: 7452/10000 (75%)\ttrain_Loss: 0.004903\tval_Loss: 0.005480\n",
      "Train Epoch: 7453/10000 (75%)\ttrain_Loss: 0.004903\tval_Loss: 0.005479\n",
      "Train Epoch: 7454/10000 (75%)\ttrain_Loss: 0.004903\tval_Loss: 0.005479\n",
      "Train Epoch: 7455/10000 (75%)\ttrain_Loss: 0.004902\tval_Loss: 0.005478\n",
      "Train Epoch: 7456/10000 (75%)\ttrain_Loss: 0.004902\tval_Loss: 0.005480\n",
      "Train Epoch: 7457/10000 (75%)\ttrain_Loss: 0.004902\tval_Loss: 0.005479\n",
      "Train Epoch: 7458/10000 (75%)\ttrain_Loss: 0.004902\tval_Loss: 0.005479\n",
      "Train Epoch: 7459/10000 (75%)\ttrain_Loss: 0.004902\tval_Loss: 0.005478\n",
      "Train Epoch: 7460/10000 (75%)\ttrain_Loss: 0.004902\tval_Loss: 0.005478\n",
      "Train Epoch: 7461/10000 (75%)\ttrain_Loss: 0.004902\tval_Loss: 0.005477\n",
      "Train Epoch: 7462/10000 (75%)\ttrain_Loss: 0.004901\tval_Loss: 0.005478\n",
      "Train Epoch: 7463/10000 (75%)\ttrain_Loss: 0.004901\tval_Loss: 0.005476\n",
      "Train Epoch: 7464/10000 (75%)\ttrain_Loss: 0.004901\tval_Loss: 0.005478\n",
      "Train Epoch: 7465/10000 (75%)\ttrain_Loss: 0.004901\tval_Loss: 0.005476\n",
      "Train Epoch: 7466/10000 (75%)\ttrain_Loss: 0.004901\tval_Loss: 0.005478\n",
      "Train Epoch: 7467/10000 (75%)\ttrain_Loss: 0.004901\tval_Loss: 0.005475\n",
      "Train Epoch: 7468/10000 (75%)\ttrain_Loss: 0.004900\tval_Loss: 0.005478\n",
      "Train Epoch: 7469/10000 (75%)\ttrain_Loss: 0.004900\tval_Loss: 0.005473\n",
      "Train Epoch: 7470/10000 (75%)\ttrain_Loss: 0.004900\tval_Loss: 0.005478\n",
      "Train Epoch: 7471/10000 (75%)\ttrain_Loss: 0.004900\tval_Loss: 0.005472\n",
      "Train Epoch: 7472/10000 (75%)\ttrain_Loss: 0.004900\tval_Loss: 0.005481\n",
      "Train Epoch: 7473/10000 (75%)\ttrain_Loss: 0.004900\tval_Loss: 0.005469\n",
      "Train Epoch: 7474/10000 (75%)\ttrain_Loss: 0.004900\tval_Loss: 0.005485\n",
      "Train Epoch: 7475/10000 (75%)\ttrain_Loss: 0.004900\tval_Loss: 0.005464\n",
      "Train Epoch: 7476/10000 (75%)\ttrain_Loss: 0.004900\tval_Loss: 0.005494\n",
      "Train Epoch: 7477/10000 (75%)\ttrain_Loss: 0.004901\tval_Loss: 0.005455\n",
      "Train Epoch: 7478/10000 (75%)\ttrain_Loss: 0.004903\tval_Loss: 0.005513\n",
      "Train Epoch: 7479/10000 (75%)\ttrain_Loss: 0.004906\tval_Loss: 0.005443\n",
      "Train Epoch: 7480/10000 (75%)\ttrain_Loss: 0.004912\tval_Loss: 0.005561\n",
      "Train Epoch: 7481/10000 (75%)\ttrain_Loss: 0.004924\tval_Loss: 0.005438\n",
      "Train Epoch: 7482/10000 (75%)\ttrain_Loss: 0.004947\tval_Loss: 0.005672\n",
      "Train Epoch: 7483/10000 (75%)\ttrain_Loss: 0.004984\tval_Loss: 0.005472\n",
      "Train Epoch: 7484/10000 (75%)\ttrain_Loss: 0.005039\tval_Loss: 0.005841\n",
      "Train Epoch: 7485/10000 (75%)\ttrain_Loss: 0.005095\tval_Loss: 0.005511\n",
      "Train Epoch: 7486/10000 (75%)\ttrain_Loss: 0.005111\tval_Loss: 0.005765\n",
      "Train Epoch: 7487/10000 (75%)\ttrain_Loss: 0.005043\tval_Loss: 0.005437\n",
      "Train Epoch: 7488/10000 (75%)\ttrain_Loss: 0.004939\tval_Loss: 0.005465\n",
      "Train Epoch: 7489/10000 (75%)\ttrain_Loss: 0.004898\tval_Loss: 0.005603\n",
      "Train Epoch: 7490/10000 (75%)\ttrain_Loss: 0.004945\tval_Loss: 0.005452\n",
      "Train Epoch: 7491/10000 (75%)\ttrain_Loss: 0.005000\tval_Loss: 0.005668\n",
      "Train Epoch: 7492/10000 (75%)\ttrain_Loss: 0.004982\tval_Loss: 0.005436\n",
      "Train Epoch: 7493/10000 (75%)\ttrain_Loss: 0.004919\tval_Loss: 0.005457\n",
      "Train Epoch: 7494/10000 (75%)\ttrain_Loss: 0.004899\tval_Loss: 0.005585\n",
      "Train Epoch: 7495/10000 (75%)\ttrain_Loss: 0.004935\tval_Loss: 0.005438\n",
      "Train Epoch: 7496/10000 (75%)\ttrain_Loss: 0.004959\tval_Loss: 0.005576\n",
      "Train Epoch: 7497/10000 (75%)\ttrain_Loss: 0.004931\tval_Loss: 0.005457\n",
      "Train Epoch: 7498/10000 (75%)\ttrain_Loss: 0.004898\tval_Loss: 0.005440\n",
      "Train Epoch: 7499/10000 (75%)\ttrain_Loss: 0.004908\tval_Loss: 0.005580\n",
      "Train Epoch: 7500/10000 (75%)\ttrain_Loss: 0.004933\tval_Loss: 0.005433\n",
      "Train Epoch: 7501/10000 (75%)\ttrain_Loss: 0.004927\tval_Loss: 0.005504\n",
      "Train Epoch: 7502/10000 (75%)\ttrain_Loss: 0.004902\tval_Loss: 0.005492\n",
      "Train Epoch: 7503/10000 (75%)\ttrain_Loss: 0.004899\tval_Loss: 0.005436\n",
      "Train Epoch: 7504/10000 (75%)\ttrain_Loss: 0.004915\tval_Loss: 0.005550\n",
      "Train Epoch: 7505/10000 (75%)\ttrain_Loss: 0.004918\tval_Loss: 0.005444\n",
      "Train Epoch: 7506/10000 (75%)\ttrain_Loss: 0.004903\tval_Loss: 0.005465\n",
      "Train Epoch: 7507/10000 (75%)\ttrain_Loss: 0.004896\tval_Loss: 0.005517\n",
      "Train Epoch: 7508/10000 (75%)\ttrain_Loss: 0.004905\tval_Loss: 0.005437\n",
      "Train Epoch: 7509/10000 (75%)\ttrain_Loss: 0.004911\tval_Loss: 0.005510\n",
      "Train Epoch: 7510/10000 (75%)\ttrain_Loss: 0.004903\tval_Loss: 0.005466\n",
      "Train Epoch: 7511/10000 (75%)\ttrain_Loss: 0.004895\tval_Loss: 0.005449\n",
      "Train Epoch: 7512/10000 (75%)\ttrain_Loss: 0.004899\tval_Loss: 0.005516\n",
      "Train Epoch: 7513/10000 (75%)\ttrain_Loss: 0.004905\tval_Loss: 0.005444\n",
      "Train Epoch: 7514/10000 (75%)\ttrain_Loss: 0.004901\tval_Loss: 0.005480\n",
      "Train Epoch: 7515/10000 (75%)\ttrain_Loss: 0.004895\tval_Loss: 0.005484\n",
      "Train Epoch: 7516/10000 (75%)\ttrain_Loss: 0.004896\tval_Loss: 0.005445\n",
      "Train Epoch: 7517/10000 (75%)\ttrain_Loss: 0.004900\tval_Loss: 0.005502\n",
      "Train Epoch: 7518/10000 (75%)\ttrain_Loss: 0.004900\tval_Loss: 0.005456\n",
      "Train Epoch: 7519/10000 (75%)\ttrain_Loss: 0.004895\tval_Loss: 0.005463\n",
      "Train Epoch: 7520/10000 (75%)\ttrain_Loss: 0.004894\tval_Loss: 0.005491\n",
      "Train Epoch: 7521/10000 (75%)\ttrain_Loss: 0.004897\tval_Loss: 0.005447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7522/10000 (75%)\ttrain_Loss: 0.004898\tval_Loss: 0.005485\n",
      "Train Epoch: 7523/10000 (75%)\ttrain_Loss: 0.004895\tval_Loss: 0.005466\n",
      "Train Epoch: 7524/10000 (75%)\ttrain_Loss: 0.004893\tval_Loss: 0.005456\n",
      "Train Epoch: 7525/10000 (75%)\ttrain_Loss: 0.004894\tval_Loss: 0.005488\n",
      "Train Epoch: 7526/10000 (75%)\ttrain_Loss: 0.004896\tval_Loss: 0.005452\n",
      "Train Epoch: 7527/10000 (75%)\ttrain_Loss: 0.004895\tval_Loss: 0.005473\n",
      "Train Epoch: 7528/10000 (75%)\ttrain_Loss: 0.004893\tval_Loss: 0.005472\n",
      "Train Epoch: 7529/10000 (75%)\ttrain_Loss: 0.004893\tval_Loss: 0.005455\n",
      "Train Epoch: 7530/10000 (75%)\ttrain_Loss: 0.004894\tval_Loss: 0.005482\n",
      "Train Epoch: 7531/10000 (75%)\ttrain_Loss: 0.004894\tval_Loss: 0.005457\n",
      "Train Epoch: 7532/10000 (75%)\ttrain_Loss: 0.004893\tval_Loss: 0.005466\n",
      "Train Epoch: 7533/10000 (75%)\ttrain_Loss: 0.004892\tval_Loss: 0.005474\n",
      "Train Epoch: 7534/10000 (75%)\ttrain_Loss: 0.004892\tval_Loss: 0.005454\n",
      "Train Epoch: 7535/10000 (75%)\ttrain_Loss: 0.004893\tval_Loss: 0.005475\n",
      "Train Epoch: 7536/10000 (75%)\ttrain_Loss: 0.004892\tval_Loss: 0.005460\n",
      "Train Epoch: 7537/10000 (75%)\ttrain_Loss: 0.004892\tval_Loss: 0.005462\n",
      "Train Epoch: 7538/10000 (75%)\ttrain_Loss: 0.004891\tval_Loss: 0.005473\n",
      "Train Epoch: 7539/10000 (75%)\ttrain_Loss: 0.004892\tval_Loss: 0.005456\n",
      "Train Epoch: 7540/10000 (75%)\ttrain_Loss: 0.004892\tval_Loss: 0.005471\n",
      "Train Epoch: 7541/10000 (75%)\ttrain_Loss: 0.004891\tval_Loss: 0.005463\n",
      "Train Epoch: 7542/10000 (75%)\ttrain_Loss: 0.004891\tval_Loss: 0.005461\n",
      "Train Epoch: 7543/10000 (75%)\ttrain_Loss: 0.004891\tval_Loss: 0.005471\n",
      "Train Epoch: 7544/10000 (75%)\ttrain_Loss: 0.004891\tval_Loss: 0.005457\n",
      "Train Epoch: 7545/10000 (75%)\ttrain_Loss: 0.004891\tval_Loss: 0.005467\n",
      "Train Epoch: 7546/10000 (75%)\ttrain_Loss: 0.004890\tval_Loss: 0.005463\n",
      "Train Epoch: 7547/10000 (75%)\ttrain_Loss: 0.004890\tval_Loss: 0.005458\n",
      "Train Epoch: 7548/10000 (75%)\ttrain_Loss: 0.004890\tval_Loss: 0.005468\n",
      "Train Epoch: 7549/10000 (75%)\ttrain_Loss: 0.004890\tval_Loss: 0.005457\n",
      "Train Epoch: 7550/10000 (75%)\ttrain_Loss: 0.004890\tval_Loss: 0.005465\n",
      "Train Epoch: 7551/10000 (76%)\ttrain_Loss: 0.004889\tval_Loss: 0.005463\n",
      "Train Epoch: 7552/10000 (76%)\ttrain_Loss: 0.004889\tval_Loss: 0.005459\n",
      "Train Epoch: 7553/10000 (76%)\ttrain_Loss: 0.004889\tval_Loss: 0.005467\n",
      "Train Epoch: 7554/10000 (76%)\ttrain_Loss: 0.004889\tval_Loss: 0.005458\n",
      "Train Epoch: 7555/10000 (76%)\ttrain_Loss: 0.004889\tval_Loss: 0.005464\n",
      "Train Epoch: 7556/10000 (76%)\ttrain_Loss: 0.004889\tval_Loss: 0.005462\n",
      "Train Epoch: 7557/10000 (76%)\ttrain_Loss: 0.004888\tval_Loss: 0.005459\n",
      "Train Epoch: 7558/10000 (76%)\ttrain_Loss: 0.004888\tval_Loss: 0.005464\n",
      "Train Epoch: 7559/10000 (76%)\ttrain_Loss: 0.004888\tval_Loss: 0.005457\n",
      "Train Epoch: 7560/10000 (76%)\ttrain_Loss: 0.004888\tval_Loss: 0.005462\n",
      "Train Epoch: 7561/10000 (76%)\ttrain_Loss: 0.004888\tval_Loss: 0.005459\n",
      "Train Epoch: 7562/10000 (76%)\ttrain_Loss: 0.004888\tval_Loss: 0.005459\n",
      "Train Epoch: 7563/10000 (76%)\ttrain_Loss: 0.004888\tval_Loss: 0.005462\n",
      "Train Epoch: 7564/10000 (76%)\ttrain_Loss: 0.004887\tval_Loss: 0.005458\n",
      "Train Epoch: 7565/10000 (76%)\ttrain_Loss: 0.004887\tval_Loss: 0.005462\n",
      "Train Epoch: 7566/10000 (76%)\ttrain_Loss: 0.004887\tval_Loss: 0.005458\n",
      "Train Epoch: 7567/10000 (76%)\ttrain_Loss: 0.004887\tval_Loss: 0.005460\n",
      "Train Epoch: 7568/10000 (76%)\ttrain_Loss: 0.004887\tval_Loss: 0.005459\n",
      "Train Epoch: 7569/10000 (76%)\ttrain_Loss: 0.004887\tval_Loss: 0.005457\n",
      "Train Epoch: 7570/10000 (76%)\ttrain_Loss: 0.004886\tval_Loss: 0.005459\n",
      "Train Epoch: 7571/10000 (76%)\ttrain_Loss: 0.004886\tval_Loss: 0.005456\n",
      "Train Epoch: 7572/10000 (76%)\ttrain_Loss: 0.004886\tval_Loss: 0.005458\n",
      "Train Epoch: 7573/10000 (76%)\ttrain_Loss: 0.004886\tval_Loss: 0.005457\n",
      "Train Epoch: 7574/10000 (76%)\ttrain_Loss: 0.004886\tval_Loss: 0.005458\n",
      "Train Epoch: 7575/10000 (76%)\ttrain_Loss: 0.004886\tval_Loss: 0.005459\n",
      "Train Epoch: 7576/10000 (76%)\ttrain_Loss: 0.004886\tval_Loss: 0.005456\n",
      "Train Epoch: 7577/10000 (76%)\ttrain_Loss: 0.004885\tval_Loss: 0.005458\n",
      "Train Epoch: 7578/10000 (76%)\ttrain_Loss: 0.004885\tval_Loss: 0.005456\n",
      "Train Epoch: 7579/10000 (76%)\ttrain_Loss: 0.004885\tval_Loss: 0.005456\n",
      "Train Epoch: 7580/10000 (76%)\ttrain_Loss: 0.004885\tval_Loss: 0.005455\n",
      "Train Epoch: 7581/10000 (76%)\ttrain_Loss: 0.004885\tval_Loss: 0.005455\n",
      "Train Epoch: 7582/10000 (76%)\ttrain_Loss: 0.004885\tval_Loss: 0.005456\n",
      "Train Epoch: 7583/10000 (76%)\ttrain_Loss: 0.004885\tval_Loss: 0.005456\n",
      "Train Epoch: 7584/10000 (76%)\ttrain_Loss: 0.004884\tval_Loss: 0.005456\n",
      "Train Epoch: 7585/10000 (76%)\ttrain_Loss: 0.004884\tval_Loss: 0.005456\n",
      "Train Epoch: 7586/10000 (76%)\ttrain_Loss: 0.004884\tval_Loss: 0.005455\n",
      "Train Epoch: 7587/10000 (76%)\ttrain_Loss: 0.004884\tval_Loss: 0.005455\n",
      "Train Epoch: 7588/10000 (76%)\ttrain_Loss: 0.004884\tval_Loss: 0.005454\n",
      "Train Epoch: 7589/10000 (76%)\ttrain_Loss: 0.004884\tval_Loss: 0.005454\n",
      "Train Epoch: 7590/10000 (76%)\ttrain_Loss: 0.004883\tval_Loss: 0.005454\n",
      "Train Epoch: 7591/10000 (76%)\ttrain_Loss: 0.004883\tval_Loss: 0.005454\n",
      "Train Epoch: 7592/10000 (76%)\ttrain_Loss: 0.004883\tval_Loss: 0.005453\n",
      "Train Epoch: 7593/10000 (76%)\ttrain_Loss: 0.004883\tval_Loss: 0.005454\n",
      "Train Epoch: 7594/10000 (76%)\ttrain_Loss: 0.004883\tval_Loss: 0.005454\n",
      "Train Epoch: 7595/10000 (76%)\ttrain_Loss: 0.004883\tval_Loss: 0.005454\n",
      "Train Epoch: 7596/10000 (76%)\ttrain_Loss: 0.004883\tval_Loss: 0.005453\n",
      "Train Epoch: 7597/10000 (76%)\ttrain_Loss: 0.004882\tval_Loss: 0.005452\n",
      "Train Epoch: 7598/10000 (76%)\ttrain_Loss: 0.004882\tval_Loss: 0.005452\n",
      "Train Epoch: 7599/10000 (76%)\ttrain_Loss: 0.004882\tval_Loss: 0.005452\n",
      "Train Epoch: 7600/10000 (76%)\ttrain_Loss: 0.004882\tval_Loss: 0.005453\n",
      "Train Epoch: 7601/10000 (76%)\ttrain_Loss: 0.004882\tval_Loss: 0.005452\n",
      "Train Epoch: 7602/10000 (76%)\ttrain_Loss: 0.004882\tval_Loss: 0.005452\n",
      "Train Epoch: 7603/10000 (76%)\ttrain_Loss: 0.004882\tval_Loss: 0.005453\n",
      "Train Epoch: 7604/10000 (76%)\ttrain_Loss: 0.004881\tval_Loss: 0.005451\n",
      "Train Epoch: 7605/10000 (76%)\ttrain_Loss: 0.004881\tval_Loss: 0.005452\n",
      "Train Epoch: 7606/10000 (76%)\ttrain_Loss: 0.004881\tval_Loss: 0.005451\n",
      "Train Epoch: 7607/10000 (76%)\ttrain_Loss: 0.004881\tval_Loss: 0.005452\n",
      "Train Epoch: 7608/10000 (76%)\ttrain_Loss: 0.004881\tval_Loss: 0.005451\n",
      "Train Epoch: 7609/10000 (76%)\ttrain_Loss: 0.004881\tval_Loss: 0.005451\n",
      "Train Epoch: 7610/10000 (76%)\ttrain_Loss: 0.004880\tval_Loss: 0.005451\n",
      "Train Epoch: 7611/10000 (76%)\ttrain_Loss: 0.004880\tval_Loss: 0.005450\n",
      "Train Epoch: 7612/10000 (76%)\ttrain_Loss: 0.004880\tval_Loss: 0.005451\n",
      "Train Epoch: 7613/10000 (76%)\ttrain_Loss: 0.004880\tval_Loss: 0.005450\n",
      "Train Epoch: 7614/10000 (76%)\ttrain_Loss: 0.004880\tval_Loss: 0.005450\n",
      "Train Epoch: 7615/10000 (76%)\ttrain_Loss: 0.004880\tval_Loss: 0.005449\n",
      "Train Epoch: 7616/10000 (76%)\ttrain_Loss: 0.004880\tval_Loss: 0.005451\n",
      "Train Epoch: 7617/10000 (76%)\ttrain_Loss: 0.004879\tval_Loss: 0.005448\n",
      "Train Epoch: 7618/10000 (76%)\ttrain_Loss: 0.004879\tval_Loss: 0.005450\n",
      "Train Epoch: 7619/10000 (76%)\ttrain_Loss: 0.004879\tval_Loss: 0.005448\n",
      "Train Epoch: 7620/10000 (76%)\ttrain_Loss: 0.004879\tval_Loss: 0.005450\n",
      "Train Epoch: 7621/10000 (76%)\ttrain_Loss: 0.004879\tval_Loss: 0.005449\n",
      "Train Epoch: 7622/10000 (76%)\ttrain_Loss: 0.004879\tval_Loss: 0.005449\n",
      "Train Epoch: 7623/10000 (76%)\ttrain_Loss: 0.004878\tval_Loss: 0.005449\n",
      "Train Epoch: 7624/10000 (76%)\ttrain_Loss: 0.004878\tval_Loss: 0.005448\n",
      "Train Epoch: 7625/10000 (76%)\ttrain_Loss: 0.004878\tval_Loss: 0.005449\n",
      "Train Epoch: 7626/10000 (76%)\ttrain_Loss: 0.004878\tval_Loss: 0.005447\n",
      "Train Epoch: 7627/10000 (76%)\ttrain_Loss: 0.004878\tval_Loss: 0.005448\n",
      "Train Epoch: 7628/10000 (76%)\ttrain_Loss: 0.004878\tval_Loss: 0.005446\n",
      "Train Epoch: 7629/10000 (76%)\ttrain_Loss: 0.004878\tval_Loss: 0.005448\n",
      "Train Epoch: 7630/10000 (76%)\ttrain_Loss: 0.004877\tval_Loss: 0.005447\n",
      "Train Epoch: 7631/10000 (76%)\ttrain_Loss: 0.004877\tval_Loss: 0.005447\n",
      "Train Epoch: 7632/10000 (76%)\ttrain_Loss: 0.004877\tval_Loss: 0.005449\n",
      "Train Epoch: 7633/10000 (76%)\ttrain_Loss: 0.004877\tval_Loss: 0.005445\n",
      "Train Epoch: 7634/10000 (76%)\ttrain_Loss: 0.004877\tval_Loss: 0.005448\n",
      "Train Epoch: 7635/10000 (76%)\ttrain_Loss: 0.004877\tval_Loss: 0.005444\n",
      "Train Epoch: 7636/10000 (76%)\ttrain_Loss: 0.004877\tval_Loss: 0.005448\n",
      "Train Epoch: 7637/10000 (76%)\ttrain_Loss: 0.004876\tval_Loss: 0.005444\n",
      "Train Epoch: 7638/10000 (76%)\ttrain_Loss: 0.004876\tval_Loss: 0.005448\n",
      "Train Epoch: 7639/10000 (76%)\ttrain_Loss: 0.004876\tval_Loss: 0.005444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7640/10000 (76%)\ttrain_Loss: 0.004876\tval_Loss: 0.005447\n",
      "Train Epoch: 7641/10000 (76%)\ttrain_Loss: 0.004876\tval_Loss: 0.005443\n",
      "Train Epoch: 7642/10000 (76%)\ttrain_Loss: 0.004876\tval_Loss: 0.005446\n",
      "Train Epoch: 7643/10000 (76%)\ttrain_Loss: 0.004875\tval_Loss: 0.005444\n",
      "Train Epoch: 7644/10000 (76%)\ttrain_Loss: 0.004875\tval_Loss: 0.005445\n",
      "Train Epoch: 7645/10000 (76%)\ttrain_Loss: 0.004875\tval_Loss: 0.005444\n",
      "Train Epoch: 7646/10000 (76%)\ttrain_Loss: 0.004875\tval_Loss: 0.005443\n",
      "Train Epoch: 7647/10000 (76%)\ttrain_Loss: 0.004875\tval_Loss: 0.005444\n",
      "Train Epoch: 7648/10000 (76%)\ttrain_Loss: 0.004875\tval_Loss: 0.005441\n",
      "Train Epoch: 7649/10000 (76%)\ttrain_Loss: 0.004874\tval_Loss: 0.005446\n",
      "Train Epoch: 7650/10000 (76%)\ttrain_Loss: 0.004874\tval_Loss: 0.005439\n",
      "Train Epoch: 7651/10000 (76%)\ttrain_Loss: 0.004874\tval_Loss: 0.005449\n",
      "Train Epoch: 7652/10000 (77%)\ttrain_Loss: 0.004874\tval_Loss: 0.005438\n",
      "Train Epoch: 7653/10000 (77%)\ttrain_Loss: 0.004874\tval_Loss: 0.005450\n",
      "Train Epoch: 7654/10000 (77%)\ttrain_Loss: 0.004874\tval_Loss: 0.005436\n",
      "Train Epoch: 7655/10000 (77%)\ttrain_Loss: 0.004874\tval_Loss: 0.005451\n",
      "Train Epoch: 7656/10000 (77%)\ttrain_Loss: 0.004874\tval_Loss: 0.005433\n",
      "Train Epoch: 7657/10000 (77%)\ttrain_Loss: 0.004874\tval_Loss: 0.005453\n",
      "Train Epoch: 7658/10000 (77%)\ttrain_Loss: 0.004874\tval_Loss: 0.005430\n",
      "Train Epoch: 7659/10000 (77%)\ttrain_Loss: 0.004874\tval_Loss: 0.005458\n",
      "Train Epoch: 7660/10000 (77%)\ttrain_Loss: 0.004874\tval_Loss: 0.005427\n",
      "Train Epoch: 7661/10000 (77%)\ttrain_Loss: 0.004875\tval_Loss: 0.005465\n",
      "Train Epoch: 7662/10000 (77%)\ttrain_Loss: 0.004875\tval_Loss: 0.005422\n",
      "Train Epoch: 7663/10000 (77%)\ttrain_Loss: 0.004876\tval_Loss: 0.005472\n",
      "Train Epoch: 7664/10000 (77%)\ttrain_Loss: 0.004877\tval_Loss: 0.005416\n",
      "Train Epoch: 7665/10000 (77%)\ttrain_Loss: 0.004878\tval_Loss: 0.005485\n",
      "Train Epoch: 7666/10000 (77%)\ttrain_Loss: 0.004881\tval_Loss: 0.005409\n",
      "Train Epoch: 7667/10000 (77%)\ttrain_Loss: 0.004884\tval_Loss: 0.005508\n",
      "Train Epoch: 7668/10000 (77%)\ttrain_Loss: 0.004889\tval_Loss: 0.005404\n",
      "Train Epoch: 7669/10000 (77%)\ttrain_Loss: 0.004895\tval_Loss: 0.005541\n",
      "Train Epoch: 7670/10000 (77%)\ttrain_Loss: 0.004902\tval_Loss: 0.005404\n",
      "Train Epoch: 7671/10000 (77%)\ttrain_Loss: 0.004911\tval_Loss: 0.005575\n",
      "Train Epoch: 7672/10000 (77%)\ttrain_Loss: 0.004919\tval_Loss: 0.005404\n",
      "Train Epoch: 7673/10000 (77%)\ttrain_Loss: 0.004925\tval_Loss: 0.005584\n",
      "Train Epoch: 7674/10000 (77%)\ttrain_Loss: 0.004925\tval_Loss: 0.005402\n",
      "Train Epoch: 7675/10000 (77%)\ttrain_Loss: 0.004918\tval_Loss: 0.005545\n",
      "Train Epoch: 7676/10000 (77%)\ttrain_Loss: 0.004905\tval_Loss: 0.005404\n",
      "Train Epoch: 7677/10000 (77%)\ttrain_Loss: 0.004889\tval_Loss: 0.005472\n",
      "Train Epoch: 7678/10000 (77%)\ttrain_Loss: 0.004876\tval_Loss: 0.005433\n",
      "Train Epoch: 7679/10000 (77%)\ttrain_Loss: 0.004870\tval_Loss: 0.005422\n",
      "Train Epoch: 7680/10000 (77%)\ttrain_Loss: 0.004872\tval_Loss: 0.005484\n",
      "Train Epoch: 7681/10000 (77%)\ttrain_Loss: 0.004879\tval_Loss: 0.005405\n",
      "Train Epoch: 7682/10000 (77%)\ttrain_Loss: 0.004886\tval_Loss: 0.005514\n",
      "Train Epoch: 7683/10000 (77%)\ttrain_Loss: 0.004890\tval_Loss: 0.005403\n",
      "Train Epoch: 7684/10000 (77%)\ttrain_Loss: 0.004889\tval_Loss: 0.005498\n",
      "Train Epoch: 7685/10000 (77%)\ttrain_Loss: 0.004883\tval_Loss: 0.005410\n",
      "Train Epoch: 7686/10000 (77%)\ttrain_Loss: 0.004876\tval_Loss: 0.005455\n",
      "Train Epoch: 7687/10000 (77%)\ttrain_Loss: 0.004871\tval_Loss: 0.005434\n",
      "Train Epoch: 7688/10000 (77%)\ttrain_Loss: 0.004869\tval_Loss: 0.005422\n",
      "Train Epoch: 7689/10000 (77%)\ttrain_Loss: 0.004870\tval_Loss: 0.005467\n",
      "Train Epoch: 7690/10000 (77%)\ttrain_Loss: 0.004873\tval_Loss: 0.005410\n",
      "Train Epoch: 7691/10000 (77%)\ttrain_Loss: 0.004876\tval_Loss: 0.005482\n",
      "Train Epoch: 7692/10000 (77%)\ttrain_Loss: 0.004877\tval_Loss: 0.005410\n",
      "Train Epoch: 7693/10000 (77%)\ttrain_Loss: 0.004876\tval_Loss: 0.005471\n",
      "Train Epoch: 7694/10000 (77%)\ttrain_Loss: 0.004873\tval_Loss: 0.005417\n",
      "Train Epoch: 7695/10000 (77%)\ttrain_Loss: 0.004871\tval_Loss: 0.005446\n",
      "Train Epoch: 7696/10000 (77%)\ttrain_Loss: 0.004868\tval_Loss: 0.005433\n",
      "Train Epoch: 7697/10000 (77%)\ttrain_Loss: 0.004867\tval_Loss: 0.005425\n",
      "Train Epoch: 7698/10000 (77%)\ttrain_Loss: 0.004868\tval_Loss: 0.005451\n",
      "Train Epoch: 7699/10000 (77%)\ttrain_Loss: 0.004869\tval_Loss: 0.005416\n",
      "Train Epoch: 7700/10000 (77%)\ttrain_Loss: 0.004870\tval_Loss: 0.005461\n",
      "Train Epoch: 7701/10000 (77%)\ttrain_Loss: 0.004870\tval_Loss: 0.005415\n",
      "Train Epoch: 7702/10000 (77%)\ttrain_Loss: 0.004870\tval_Loss: 0.005459\n",
      "Train Epoch: 7703/10000 (77%)\ttrain_Loss: 0.004870\tval_Loss: 0.005418\n",
      "Train Epoch: 7704/10000 (77%)\ttrain_Loss: 0.004869\tval_Loss: 0.005447\n",
      "Train Epoch: 7705/10000 (77%)\ttrain_Loss: 0.004867\tval_Loss: 0.005424\n",
      "Train Epoch: 7706/10000 (77%)\ttrain_Loss: 0.004867\tval_Loss: 0.005434\n",
      "Train Epoch: 7707/10000 (77%)\ttrain_Loss: 0.004866\tval_Loss: 0.005433\n",
      "Train Epoch: 7708/10000 (77%)\ttrain_Loss: 0.004866\tval_Loss: 0.005426\n",
      "Train Epoch: 7709/10000 (77%)\ttrain_Loss: 0.004866\tval_Loss: 0.005443\n",
      "Train Epoch: 7710/10000 (77%)\ttrain_Loss: 0.004866\tval_Loss: 0.005422\n",
      "Train Epoch: 7711/10000 (77%)\ttrain_Loss: 0.004866\tval_Loss: 0.005447\n",
      "Train Epoch: 7712/10000 (77%)\ttrain_Loss: 0.004866\tval_Loss: 0.005419\n",
      "Train Epoch: 7713/10000 (77%)\ttrain_Loss: 0.004866\tval_Loss: 0.005445\n",
      "Train Epoch: 7714/10000 (77%)\ttrain_Loss: 0.004866\tval_Loss: 0.005419\n",
      "Train Epoch: 7715/10000 (77%)\ttrain_Loss: 0.004866\tval_Loss: 0.005441\n",
      "Train Epoch: 7716/10000 (77%)\ttrain_Loss: 0.004865\tval_Loss: 0.005422\n",
      "Train Epoch: 7717/10000 (77%)\ttrain_Loss: 0.004865\tval_Loss: 0.005438\n",
      "Train Epoch: 7718/10000 (77%)\ttrain_Loss: 0.004865\tval_Loss: 0.005427\n",
      "Train Epoch: 7719/10000 (77%)\ttrain_Loss: 0.004864\tval_Loss: 0.005434\n",
      "Train Epoch: 7720/10000 (77%)\ttrain_Loss: 0.004864\tval_Loss: 0.005430\n",
      "Train Epoch: 7721/10000 (77%)\ttrain_Loss: 0.004864\tval_Loss: 0.005429\n",
      "Train Epoch: 7722/10000 (77%)\ttrain_Loss: 0.004864\tval_Loss: 0.005432\n",
      "Train Epoch: 7723/10000 (77%)\ttrain_Loss: 0.004864\tval_Loss: 0.005425\n",
      "Train Epoch: 7724/10000 (77%)\ttrain_Loss: 0.004864\tval_Loss: 0.005434\n",
      "Train Epoch: 7725/10000 (77%)\ttrain_Loss: 0.004863\tval_Loss: 0.005423\n",
      "Train Epoch: 7726/10000 (77%)\ttrain_Loss: 0.004863\tval_Loss: 0.005437\n",
      "Train Epoch: 7727/10000 (77%)\ttrain_Loss: 0.004863\tval_Loss: 0.005422\n",
      "Train Epoch: 7728/10000 (77%)\ttrain_Loss: 0.004863\tval_Loss: 0.005439\n",
      "Train Epoch: 7729/10000 (77%)\ttrain_Loss: 0.004863\tval_Loss: 0.005420\n",
      "Train Epoch: 7730/10000 (77%)\ttrain_Loss: 0.004863\tval_Loss: 0.005440\n",
      "Train Epoch: 7731/10000 (77%)\ttrain_Loss: 0.004863\tval_Loss: 0.005417\n",
      "Train Epoch: 7732/10000 (77%)\ttrain_Loss: 0.004863\tval_Loss: 0.005441\n",
      "Train Epoch: 7733/10000 (77%)\ttrain_Loss: 0.004863\tval_Loss: 0.005414\n",
      "Train Epoch: 7734/10000 (77%)\ttrain_Loss: 0.004863\tval_Loss: 0.005444\n",
      "Train Epoch: 7735/10000 (77%)\ttrain_Loss: 0.004863\tval_Loss: 0.005413\n",
      "Train Epoch: 7736/10000 (77%)\ttrain_Loss: 0.004863\tval_Loss: 0.005448\n",
      "Train Epoch: 7737/10000 (77%)\ttrain_Loss: 0.004864\tval_Loss: 0.005410\n",
      "Train Epoch: 7738/10000 (77%)\ttrain_Loss: 0.004864\tval_Loss: 0.005453\n",
      "Train Epoch: 7739/10000 (77%)\ttrain_Loss: 0.004865\tval_Loss: 0.005405\n",
      "Train Epoch: 7740/10000 (77%)\ttrain_Loss: 0.004865\tval_Loss: 0.005461\n",
      "Train Epoch: 7741/10000 (77%)\ttrain_Loss: 0.004867\tval_Loss: 0.005398\n",
      "Train Epoch: 7742/10000 (77%)\ttrain_Loss: 0.004869\tval_Loss: 0.005476\n",
      "Train Epoch: 7743/10000 (77%)\ttrain_Loss: 0.004871\tval_Loss: 0.005392\n",
      "Train Epoch: 7744/10000 (77%)\ttrain_Loss: 0.004875\tval_Loss: 0.005499\n",
      "Train Epoch: 7745/10000 (77%)\ttrain_Loss: 0.004880\tval_Loss: 0.005389\n",
      "Train Epoch: 7746/10000 (77%)\ttrain_Loss: 0.004886\tval_Loss: 0.005532\n",
      "Train Epoch: 7747/10000 (77%)\ttrain_Loss: 0.004894\tval_Loss: 0.005389\n",
      "Train Epoch: 7748/10000 (77%)\ttrain_Loss: 0.004902\tval_Loss: 0.005563\n",
      "Train Epoch: 7749/10000 (77%)\ttrain_Loss: 0.004909\tval_Loss: 0.005389\n",
      "Train Epoch: 7750/10000 (77%)\ttrain_Loss: 0.004913\tval_Loss: 0.005564\n",
      "Train Epoch: 7751/10000 (78%)\ttrain_Loss: 0.004910\tval_Loss: 0.005386\n",
      "Train Epoch: 7752/10000 (78%)\ttrain_Loss: 0.004901\tval_Loss: 0.005516\n",
      "Train Epoch: 7753/10000 (78%)\ttrain_Loss: 0.004887\tval_Loss: 0.005392\n",
      "Train Epoch: 7754/10000 (78%)\ttrain_Loss: 0.004872\tval_Loss: 0.005448\n",
      "Train Epoch: 7755/10000 (78%)\ttrain_Loss: 0.004862\tval_Loss: 0.005426\n",
      "Train Epoch: 7756/10000 (78%)\ttrain_Loss: 0.004859\tval_Loss: 0.005405\n",
      "Train Epoch: 7757/10000 (78%)\ttrain_Loss: 0.004862\tval_Loss: 0.005473\n",
      "Train Epoch: 7758/10000 (78%)\ttrain_Loss: 0.004869\tval_Loss: 0.005389\n",
      "Train Epoch: 7759/10000 (78%)\ttrain_Loss: 0.004875\tval_Loss: 0.005495\n",
      "Train Epoch: 7760/10000 (78%)\ttrain_Loss: 0.004877\tval_Loss: 0.005387\n",
      "Train Epoch: 7761/10000 (78%)\ttrain_Loss: 0.004876\tval_Loss: 0.005477\n",
      "Train Epoch: 7762/10000 (78%)\ttrain_Loss: 0.004871\tval_Loss: 0.005396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7763/10000 (78%)\ttrain_Loss: 0.004864\tval_Loss: 0.005440\n",
      "Train Epoch: 7764/10000 (78%)\ttrain_Loss: 0.004860\tval_Loss: 0.005419\n",
      "Train Epoch: 7765/10000 (78%)\ttrain_Loss: 0.004858\tval_Loss: 0.005411\n",
      "Train Epoch: 7766/10000 (78%)\ttrain_Loss: 0.004858\tval_Loss: 0.005447\n",
      "Train Epoch: 7767/10000 (78%)\ttrain_Loss: 0.004861\tval_Loss: 0.005397\n",
      "Train Epoch: 7768/10000 (78%)\ttrain_Loss: 0.004863\tval_Loss: 0.005463\n",
      "Train Epoch: 7769/10000 (78%)\ttrain_Loss: 0.004865\tval_Loss: 0.005395\n",
      "Train Epoch: 7770/10000 (78%)\ttrain_Loss: 0.004864\tval_Loss: 0.005457\n",
      "Train Epoch: 7771/10000 (78%)\ttrain_Loss: 0.004863\tval_Loss: 0.005400\n",
      "Train Epoch: 7772/10000 (78%)\ttrain_Loss: 0.004860\tval_Loss: 0.005436\n",
      "Train Epoch: 7773/10000 (78%)\ttrain_Loss: 0.004858\tval_Loss: 0.005414\n",
      "Train Epoch: 7774/10000 (78%)\ttrain_Loss: 0.004857\tval_Loss: 0.005416\n",
      "Train Epoch: 7775/10000 (78%)\ttrain_Loss: 0.004856\tval_Loss: 0.005431\n",
      "Train Epoch: 7776/10000 (78%)\ttrain_Loss: 0.004857\tval_Loss: 0.005404\n",
      "Train Epoch: 7777/10000 (78%)\ttrain_Loss: 0.004858\tval_Loss: 0.005444\n",
      "Train Epoch: 7778/10000 (78%)\ttrain_Loss: 0.004859\tval_Loss: 0.005399\n",
      "Train Epoch: 7779/10000 (78%)\ttrain_Loss: 0.004859\tval_Loss: 0.005447\n",
      "Train Epoch: 7780/10000 (78%)\ttrain_Loss: 0.004859\tval_Loss: 0.005400\n",
      "Train Epoch: 7781/10000 (78%)\ttrain_Loss: 0.004859\tval_Loss: 0.005441\n",
      "Train Epoch: 7782/10000 (78%)\ttrain_Loss: 0.004858\tval_Loss: 0.005404\n",
      "Train Epoch: 7783/10000 (78%)\ttrain_Loss: 0.004857\tval_Loss: 0.005429\n",
      "Train Epoch: 7784/10000 (78%)\ttrain_Loss: 0.004856\tval_Loss: 0.005413\n",
      "Train Epoch: 7785/10000 (78%)\ttrain_Loss: 0.004855\tval_Loss: 0.005418\n",
      "Train Epoch: 7786/10000 (78%)\ttrain_Loss: 0.004855\tval_Loss: 0.005422\n",
      "Train Epoch: 7787/10000 (78%)\ttrain_Loss: 0.004855\tval_Loss: 0.005410\n",
      "Train Epoch: 7788/10000 (78%)\ttrain_Loss: 0.004855\tval_Loss: 0.005429\n",
      "Train Epoch: 7789/10000 (78%)\ttrain_Loss: 0.004855\tval_Loss: 0.005404\n",
      "Train Epoch: 7790/10000 (78%)\ttrain_Loss: 0.004855\tval_Loss: 0.005433\n",
      "Train Epoch: 7791/10000 (78%)\ttrain_Loss: 0.004855\tval_Loss: 0.005403\n",
      "Train Epoch: 7792/10000 (78%)\ttrain_Loss: 0.004855\tval_Loss: 0.005435\n",
      "Train Epoch: 7793/10000 (78%)\ttrain_Loss: 0.004855\tval_Loss: 0.005403\n",
      "Train Epoch: 7794/10000 (78%)\ttrain_Loss: 0.004855\tval_Loss: 0.005432\n",
      "Train Epoch: 7795/10000 (78%)\ttrain_Loss: 0.004855\tval_Loss: 0.005404\n",
      "Train Epoch: 7796/10000 (78%)\ttrain_Loss: 0.004854\tval_Loss: 0.005428\n",
      "Train Epoch: 7797/10000 (78%)\ttrain_Loss: 0.004854\tval_Loss: 0.005406\n",
      "Train Epoch: 7798/10000 (78%)\ttrain_Loss: 0.004854\tval_Loss: 0.005424\n",
      "Train Epoch: 7799/10000 (78%)\ttrain_Loss: 0.004853\tval_Loss: 0.005408\n",
      "Train Epoch: 7800/10000 (78%)\ttrain_Loss: 0.004853\tval_Loss: 0.005419\n",
      "Train Epoch: 7801/10000 (78%)\ttrain_Loss: 0.004853\tval_Loss: 0.005411\n",
      "Train Epoch: 7802/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005415\n",
      "Train Epoch: 7803/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005414\n",
      "Train Epoch: 7804/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005412\n",
      "Train Epoch: 7805/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005418\n",
      "Train Epoch: 7806/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005410\n",
      "Train Epoch: 7807/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005420\n",
      "Train Epoch: 7808/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005407\n",
      "Train Epoch: 7809/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005423\n",
      "Train Epoch: 7810/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005404\n",
      "Train Epoch: 7811/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005425\n",
      "Train Epoch: 7812/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005400\n",
      "Train Epoch: 7813/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005429\n",
      "Train Epoch: 7814/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005396\n",
      "Train Epoch: 7815/10000 (78%)\ttrain_Loss: 0.004853\tval_Loss: 0.005438\n",
      "Train Epoch: 7816/10000 (78%)\ttrain_Loss: 0.004854\tval_Loss: 0.005390\n",
      "Train Epoch: 7817/10000 (78%)\ttrain_Loss: 0.004855\tval_Loss: 0.005451\n",
      "Train Epoch: 7818/10000 (78%)\ttrain_Loss: 0.004857\tval_Loss: 0.005383\n",
      "Train Epoch: 7819/10000 (78%)\ttrain_Loss: 0.004860\tval_Loss: 0.005474\n",
      "Train Epoch: 7820/10000 (78%)\ttrain_Loss: 0.004864\tval_Loss: 0.005376\n",
      "Train Epoch: 7821/10000 (78%)\ttrain_Loss: 0.004870\tval_Loss: 0.005507\n",
      "Train Epoch: 7822/10000 (78%)\ttrain_Loss: 0.004878\tval_Loss: 0.005373\n",
      "Train Epoch: 7823/10000 (78%)\ttrain_Loss: 0.004889\tval_Loss: 0.005552\n",
      "Train Epoch: 7824/10000 (78%)\ttrain_Loss: 0.004901\tval_Loss: 0.005375\n",
      "Train Epoch: 7825/10000 (78%)\ttrain_Loss: 0.004913\tval_Loss: 0.005587\n",
      "Train Epoch: 7826/10000 (78%)\ttrain_Loss: 0.004920\tval_Loss: 0.005377\n",
      "Train Epoch: 7827/10000 (78%)\ttrain_Loss: 0.004919\tval_Loss: 0.005563\n",
      "Train Epoch: 7828/10000 (78%)\ttrain_Loss: 0.004907\tval_Loss: 0.005372\n",
      "Train Epoch: 7829/10000 (78%)\ttrain_Loss: 0.004885\tval_Loss: 0.005472\n",
      "Train Epoch: 7830/10000 (78%)\ttrain_Loss: 0.004863\tval_Loss: 0.005395\n",
      "Train Epoch: 7831/10000 (78%)\ttrain_Loss: 0.004850\tval_Loss: 0.005396\n",
      "Train Epoch: 7832/10000 (78%)\ttrain_Loss: 0.004850\tval_Loss: 0.005459\n",
      "Train Epoch: 7833/10000 (78%)\ttrain_Loss: 0.004858\tval_Loss: 0.005373\n",
      "Train Epoch: 7834/10000 (78%)\ttrain_Loss: 0.004869\tval_Loss: 0.005502\n",
      "Train Epoch: 7835/10000 (78%)\ttrain_Loss: 0.004875\tval_Loss: 0.005371\n",
      "Train Epoch: 7836/10000 (78%)\ttrain_Loss: 0.004873\tval_Loss: 0.005476\n",
      "Train Epoch: 7837/10000 (78%)\ttrain_Loss: 0.004865\tval_Loss: 0.005382\n",
      "Train Epoch: 7838/10000 (78%)\ttrain_Loss: 0.004854\tval_Loss: 0.005419\n",
      "Train Epoch: 7839/10000 (78%)\ttrain_Loss: 0.004848\tval_Loss: 0.005419\n",
      "Train Epoch: 7840/10000 (78%)\ttrain_Loss: 0.004848\tval_Loss: 0.005384\n",
      "Train Epoch: 7841/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005458\n",
      "Train Epoch: 7842/10000 (78%)\ttrain_Loss: 0.004857\tval_Loss: 0.005375\n",
      "Train Epoch: 7843/10000 (78%)\ttrain_Loss: 0.004860\tval_Loss: 0.005460\n",
      "Train Epoch: 7844/10000 (78%)\ttrain_Loss: 0.004858\tval_Loss: 0.005381\n",
      "Train Epoch: 7845/10000 (78%)\ttrain_Loss: 0.004854\tval_Loss: 0.005430\n",
      "Train Epoch: 7846/10000 (78%)\ttrain_Loss: 0.004849\tval_Loss: 0.005402\n",
      "Train Epoch: 7847/10000 (78%)\ttrain_Loss: 0.004846\tval_Loss: 0.005398\n",
      "Train Epoch: 7848/10000 (78%)\ttrain_Loss: 0.004847\tval_Loss: 0.005429\n",
      "Train Epoch: 7849/10000 (78%)\ttrain_Loss: 0.004849\tval_Loss: 0.005383\n",
      "Train Epoch: 7850/10000 (78%)\ttrain_Loss: 0.004851\tval_Loss: 0.005442\n",
      "Train Epoch: 7851/10000 (78%)\ttrain_Loss: 0.004852\tval_Loss: 0.005382\n",
      "Train Epoch: 7852/10000 (79%)\ttrain_Loss: 0.004851\tval_Loss: 0.005432\n",
      "Train Epoch: 7853/10000 (79%)\ttrain_Loss: 0.004849\tval_Loss: 0.005391\n",
      "Train Epoch: 7854/10000 (79%)\ttrain_Loss: 0.004847\tval_Loss: 0.005412\n",
      "Train Epoch: 7855/10000 (79%)\ttrain_Loss: 0.004845\tval_Loss: 0.005407\n",
      "Train Epoch: 7856/10000 (79%)\ttrain_Loss: 0.004845\tval_Loss: 0.005395\n",
      "Train Epoch: 7857/10000 (79%)\ttrain_Loss: 0.004846\tval_Loss: 0.005422\n",
      "Train Epoch: 7858/10000 (79%)\ttrain_Loss: 0.004846\tval_Loss: 0.005388\n",
      "Train Epoch: 7859/10000 (79%)\ttrain_Loss: 0.004847\tval_Loss: 0.005426\n",
      "Train Epoch: 7860/10000 (79%)\ttrain_Loss: 0.004847\tval_Loss: 0.005388\n",
      "Train Epoch: 7861/10000 (79%)\ttrain_Loss: 0.004846\tval_Loss: 0.005420\n",
      "Train Epoch: 7862/10000 (79%)\ttrain_Loss: 0.004846\tval_Loss: 0.005394\n",
      "Train Epoch: 7863/10000 (79%)\ttrain_Loss: 0.004845\tval_Loss: 0.005409\n",
      "Train Epoch: 7864/10000 (79%)\ttrain_Loss: 0.004844\tval_Loss: 0.005403\n",
      "Train Epoch: 7865/10000 (79%)\ttrain_Loss: 0.004844\tval_Loss: 0.005399\n",
      "Train Epoch: 7866/10000 (79%)\ttrain_Loss: 0.004844\tval_Loss: 0.005411\n",
      "Train Epoch: 7867/10000 (79%)\ttrain_Loss: 0.004844\tval_Loss: 0.005393\n",
      "Train Epoch: 7868/10000 (79%)\ttrain_Loss: 0.004844\tval_Loss: 0.005415\n",
      "Train Epoch: 7869/10000 (79%)\ttrain_Loss: 0.004844\tval_Loss: 0.005390\n",
      "Train Epoch: 7870/10000 (79%)\ttrain_Loss: 0.004844\tval_Loss: 0.005415\n",
      "Train Epoch: 7871/10000 (79%)\ttrain_Loss: 0.004844\tval_Loss: 0.005391\n",
      "Train Epoch: 7872/10000 (79%)\ttrain_Loss: 0.004844\tval_Loss: 0.005412\n",
      "Train Epoch: 7873/10000 (79%)\ttrain_Loss: 0.004843\tval_Loss: 0.005394\n",
      "Train Epoch: 7874/10000 (79%)\ttrain_Loss: 0.004843\tval_Loss: 0.005406\n",
      "Train Epoch: 7875/10000 (79%)\ttrain_Loss: 0.004843\tval_Loss: 0.005398\n",
      "Train Epoch: 7876/10000 (79%)\ttrain_Loss: 0.004842\tval_Loss: 0.005401\n",
      "Train Epoch: 7877/10000 (79%)\ttrain_Loss: 0.004842\tval_Loss: 0.005402\n",
      "Train Epoch: 7878/10000 (79%)\ttrain_Loss: 0.004842\tval_Loss: 0.005396\n",
      "Train Epoch: 7879/10000 (79%)\ttrain_Loss: 0.004842\tval_Loss: 0.005407\n",
      "Train Epoch: 7880/10000 (79%)\ttrain_Loss: 0.004842\tval_Loss: 0.005394\n",
      "Train Epoch: 7881/10000 (79%)\ttrain_Loss: 0.004842\tval_Loss: 0.005409\n",
      "Train Epoch: 7882/10000 (79%)\ttrain_Loss: 0.004842\tval_Loss: 0.005393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7883/10000 (79%)\ttrain_Loss: 0.004842\tval_Loss: 0.005408\n",
      "Train Epoch: 7884/10000 (79%)\ttrain_Loss: 0.004842\tval_Loss: 0.005392\n",
      "Train Epoch: 7885/10000 (79%)\ttrain_Loss: 0.004841\tval_Loss: 0.005406\n",
      "Train Epoch: 7886/10000 (79%)\ttrain_Loss: 0.004841\tval_Loss: 0.005392\n",
      "Train Epoch: 7887/10000 (79%)\ttrain_Loss: 0.004841\tval_Loss: 0.005404\n",
      "Train Epoch: 7888/10000 (79%)\ttrain_Loss: 0.004841\tval_Loss: 0.005394\n",
      "Train Epoch: 7889/10000 (79%)\ttrain_Loss: 0.004841\tval_Loss: 0.005403\n",
      "Train Epoch: 7890/10000 (79%)\ttrain_Loss: 0.004840\tval_Loss: 0.005395\n",
      "Train Epoch: 7891/10000 (79%)\ttrain_Loss: 0.004840\tval_Loss: 0.005401\n",
      "Train Epoch: 7892/10000 (79%)\ttrain_Loss: 0.004840\tval_Loss: 0.005395\n",
      "Train Epoch: 7893/10000 (79%)\ttrain_Loss: 0.004840\tval_Loss: 0.005399\n",
      "Train Epoch: 7894/10000 (79%)\ttrain_Loss: 0.004840\tval_Loss: 0.005395\n",
      "Train Epoch: 7895/10000 (79%)\ttrain_Loss: 0.004840\tval_Loss: 0.005398\n",
      "Train Epoch: 7896/10000 (79%)\ttrain_Loss: 0.004839\tval_Loss: 0.005395\n",
      "Train Epoch: 7897/10000 (79%)\ttrain_Loss: 0.004839\tval_Loss: 0.005398\n",
      "Train Epoch: 7898/10000 (79%)\ttrain_Loss: 0.004839\tval_Loss: 0.005396\n",
      "Train Epoch: 7899/10000 (79%)\ttrain_Loss: 0.004839\tval_Loss: 0.005398\n",
      "Train Epoch: 7900/10000 (79%)\ttrain_Loss: 0.004839\tval_Loss: 0.005396\n",
      "Train Epoch: 7901/10000 (79%)\ttrain_Loss: 0.004839\tval_Loss: 0.005396\n",
      "Train Epoch: 7902/10000 (79%)\ttrain_Loss: 0.004839\tval_Loss: 0.005395\n",
      "Train Epoch: 7903/10000 (79%)\ttrain_Loss: 0.004838\tval_Loss: 0.005395\n",
      "Train Epoch: 7904/10000 (79%)\ttrain_Loss: 0.004838\tval_Loss: 0.005394\n",
      "Train Epoch: 7905/10000 (79%)\ttrain_Loss: 0.004838\tval_Loss: 0.005395\n",
      "Train Epoch: 7906/10000 (79%)\ttrain_Loss: 0.004838\tval_Loss: 0.005395\n",
      "Train Epoch: 7907/10000 (79%)\ttrain_Loss: 0.004838\tval_Loss: 0.005395\n",
      "Train Epoch: 7908/10000 (79%)\ttrain_Loss: 0.004838\tval_Loss: 0.005395\n",
      "Train Epoch: 7909/10000 (79%)\ttrain_Loss: 0.004838\tval_Loss: 0.005394\n",
      "Train Epoch: 7910/10000 (79%)\ttrain_Loss: 0.004837\tval_Loss: 0.005394\n",
      "Train Epoch: 7911/10000 (79%)\ttrain_Loss: 0.004837\tval_Loss: 0.005393\n",
      "Train Epoch: 7912/10000 (79%)\ttrain_Loss: 0.004837\tval_Loss: 0.005393\n",
      "Train Epoch: 7913/10000 (79%)\ttrain_Loss: 0.004837\tval_Loss: 0.005392\n",
      "Train Epoch: 7914/10000 (79%)\ttrain_Loss: 0.004837\tval_Loss: 0.005392\n",
      "Train Epoch: 7915/10000 (79%)\ttrain_Loss: 0.004837\tval_Loss: 0.005393\n",
      "Train Epoch: 7916/10000 (79%)\ttrain_Loss: 0.004837\tval_Loss: 0.005393\n",
      "Train Epoch: 7917/10000 (79%)\ttrain_Loss: 0.004836\tval_Loss: 0.005393\n",
      "Train Epoch: 7918/10000 (79%)\ttrain_Loss: 0.004836\tval_Loss: 0.005393\n",
      "Train Epoch: 7919/10000 (79%)\ttrain_Loss: 0.004836\tval_Loss: 0.005392\n",
      "Train Epoch: 7920/10000 (79%)\ttrain_Loss: 0.004836\tval_Loss: 0.005392\n",
      "Train Epoch: 7921/10000 (79%)\ttrain_Loss: 0.004836\tval_Loss: 0.005391\n",
      "Train Epoch: 7922/10000 (79%)\ttrain_Loss: 0.004836\tval_Loss: 0.005391\n",
      "Train Epoch: 7923/10000 (79%)\ttrain_Loss: 0.004836\tval_Loss: 0.005389\n",
      "Train Epoch: 7924/10000 (79%)\ttrain_Loss: 0.004835\tval_Loss: 0.005393\n",
      "Train Epoch: 7925/10000 (79%)\ttrain_Loss: 0.004835\tval_Loss: 0.005388\n",
      "Train Epoch: 7926/10000 (79%)\ttrain_Loss: 0.004835\tval_Loss: 0.005395\n",
      "Train Epoch: 7927/10000 (79%)\ttrain_Loss: 0.004835\tval_Loss: 0.005385\n",
      "Train Epoch: 7928/10000 (79%)\ttrain_Loss: 0.004835\tval_Loss: 0.005398\n",
      "Train Epoch: 7929/10000 (79%)\ttrain_Loss: 0.004835\tval_Loss: 0.005379\n",
      "Train Epoch: 7930/10000 (79%)\ttrain_Loss: 0.004835\tval_Loss: 0.005405\n",
      "Train Epoch: 7931/10000 (79%)\ttrain_Loss: 0.004836\tval_Loss: 0.005370\n",
      "Train Epoch: 7932/10000 (79%)\ttrain_Loss: 0.004837\tval_Loss: 0.005426\n",
      "Train Epoch: 7933/10000 (79%)\ttrain_Loss: 0.004840\tval_Loss: 0.005358\n",
      "Train Epoch: 7934/10000 (79%)\ttrain_Loss: 0.004847\tval_Loss: 0.005480\n",
      "Train Epoch: 7935/10000 (79%)\ttrain_Loss: 0.004860\tval_Loss: 0.005352\n",
      "Train Epoch: 7936/10000 (79%)\ttrain_Loss: 0.004887\tval_Loss: 0.005623\n",
      "Train Epoch: 7937/10000 (79%)\ttrain_Loss: 0.004938\tval_Loss: 0.005408\n",
      "Train Epoch: 7938/10000 (79%)\ttrain_Loss: 0.005024\tval_Loss: 0.005904\n",
      "Train Epoch: 7939/10000 (79%)\ttrain_Loss: 0.005130\tval_Loss: 0.005511\n",
      "Train Epoch: 7940/10000 (79%)\ttrain_Loss: 0.005189\tval_Loss: 0.005856\n",
      "Train Epoch: 7941/10000 (79%)\ttrain_Loss: 0.005094\tval_Loss: 0.005357\n",
      "Train Epoch: 7942/10000 (79%)\ttrain_Loss: 0.004908\tval_Loss: 0.005375\n",
      "Train Epoch: 7943/10000 (79%)\ttrain_Loss: 0.004835\tval_Loss: 0.005612\n",
      "Train Epoch: 7944/10000 (79%)\ttrain_Loss: 0.004930\tval_Loss: 0.005402\n",
      "Train Epoch: 7945/10000 (79%)\ttrain_Loss: 0.005014\tval_Loss: 0.005632\n",
      "Train Epoch: 7946/10000 (79%)\ttrain_Loss: 0.004944\tval_Loss: 0.005360\n",
      "Train Epoch: 7947/10000 (79%)\ttrain_Loss: 0.004840\tval_Loss: 0.005348\n",
      "Train Epoch: 7948/10000 (79%)\ttrain_Loss: 0.004864\tval_Loss: 0.005621\n",
      "Train Epoch: 7949/10000 (79%)\ttrain_Loss: 0.004938\tval_Loss: 0.005355\n",
      "Train Epoch: 7950/10000 (79%)\ttrain_Loss: 0.004914\tval_Loss: 0.005431\n",
      "Train Epoch: 7951/10000 (80%)\ttrain_Loss: 0.004841\tval_Loss: 0.005460\n",
      "Train Epoch: 7952/10000 (80%)\ttrain_Loss: 0.004851\tval_Loss: 0.005352\n",
      "Train Epoch: 7953/10000 (80%)\ttrain_Loss: 0.004901\tval_Loss: 0.005525\n",
      "Train Epoch: 7954/10000 (80%)\ttrain_Loss: 0.004882\tval_Loss: 0.005366\n",
      "Train Epoch: 7955/10000 (80%)\ttrain_Loss: 0.004835\tval_Loss: 0.005350\n",
      "Train Epoch: 7956/10000 (80%)\ttrain_Loss: 0.004848\tval_Loss: 0.005517\n",
      "Train Epoch: 7957/10000 (80%)\ttrain_Loss: 0.004879\tval_Loss: 0.005345\n",
      "Train Epoch: 7958/10000 (80%)\ttrain_Loss: 0.004859\tval_Loss: 0.005391\n",
      "Train Epoch: 7959/10000 (80%)\ttrain_Loss: 0.004832\tval_Loss: 0.005449\n",
      "Train Epoch: 7960/10000 (80%)\ttrain_Loss: 0.004847\tval_Loss: 0.005345\n",
      "Train Epoch: 7961/10000 (80%)\ttrain_Loss: 0.004864\tval_Loss: 0.005444\n",
      "Train Epoch: 7962/10000 (80%)\ttrain_Loss: 0.004845\tval_Loss: 0.005388\n",
      "Train Epoch: 7963/10000 (80%)\ttrain_Loss: 0.004831\tval_Loss: 0.005351\n",
      "Train Epoch: 7964/10000 (80%)\ttrain_Loss: 0.004845\tval_Loss: 0.005461\n",
      "Train Epoch: 7965/10000 (80%)\ttrain_Loss: 0.004851\tval_Loss: 0.005359\n",
      "Train Epoch: 7966/10000 (80%)\ttrain_Loss: 0.004837\tval_Loss: 0.005372\n",
      "Train Epoch: 7967/10000 (80%)\ttrain_Loss: 0.004831\tval_Loss: 0.005437\n",
      "Train Epoch: 7968/10000 (80%)\ttrain_Loss: 0.004842\tval_Loss: 0.005351\n",
      "Train Epoch: 7969/10000 (80%)\ttrain_Loss: 0.004842\tval_Loss: 0.005402\n",
      "Train Epoch: 7970/10000 (80%)\ttrain_Loss: 0.004832\tval_Loss: 0.005400\n",
      "Train Epoch: 7971/10000 (80%)\ttrain_Loss: 0.004831\tval_Loss: 0.005353\n",
      "Train Epoch: 7972/10000 (80%)\ttrain_Loss: 0.004838\tval_Loss: 0.005422\n",
      "Train Epoch: 7973/10000 (80%)\ttrain_Loss: 0.004836\tval_Loss: 0.005372\n",
      "Train Epoch: 7974/10000 (80%)\ttrain_Loss: 0.004830\tval_Loss: 0.005364\n",
      "Train Epoch: 7975/10000 (80%)\ttrain_Loss: 0.004831\tval_Loss: 0.005418\n",
      "Train Epoch: 7976/10000 (80%)\ttrain_Loss: 0.004835\tval_Loss: 0.005358\n",
      "Train Epoch: 7977/10000 (80%)\ttrain_Loss: 0.004832\tval_Loss: 0.005383\n",
      "Train Epoch: 7978/10000 (80%)\ttrain_Loss: 0.004828\tval_Loss: 0.005398\n",
      "Train Epoch: 7979/10000 (80%)\ttrain_Loss: 0.004830\tval_Loss: 0.005356\n",
      "Train Epoch: 7980/10000 (80%)\ttrain_Loss: 0.004832\tval_Loss: 0.005398\n",
      "Train Epoch: 7981/10000 (80%)\ttrain_Loss: 0.004830\tval_Loss: 0.005376\n",
      "Train Epoch: 7982/10000 (80%)\ttrain_Loss: 0.004827\tval_Loss: 0.005362\n",
      "Train Epoch: 7983/10000 (80%)\ttrain_Loss: 0.004828\tval_Loss: 0.005400\n",
      "Train Epoch: 7984/10000 (80%)\ttrain_Loss: 0.004830\tval_Loss: 0.005363\n",
      "Train Epoch: 7985/10000 (80%)\ttrain_Loss: 0.004828\tval_Loss: 0.005375\n",
      "Train Epoch: 7986/10000 (80%)\ttrain_Loss: 0.004826\tval_Loss: 0.005390\n",
      "Train Epoch: 7987/10000 (80%)\ttrain_Loss: 0.004827\tval_Loss: 0.005360\n",
      "Train Epoch: 7988/10000 (80%)\ttrain_Loss: 0.004828\tval_Loss: 0.005386\n",
      "Train Epoch: 7989/10000 (80%)\ttrain_Loss: 0.004827\tval_Loss: 0.005376\n",
      "Train Epoch: 7990/10000 (80%)\ttrain_Loss: 0.004826\tval_Loss: 0.005363\n",
      "Train Epoch: 7991/10000 (80%)\ttrain_Loss: 0.004826\tval_Loss: 0.005389\n",
      "Train Epoch: 7992/10000 (80%)\ttrain_Loss: 0.004827\tval_Loss: 0.005365\n",
      "Train Epoch: 7993/10000 (80%)\ttrain_Loss: 0.004826\tval_Loss: 0.005373\n",
      "Train Epoch: 7994/10000 (80%)\ttrain_Loss: 0.004825\tval_Loss: 0.005383\n",
      "Train Epoch: 7995/10000 (80%)\ttrain_Loss: 0.004825\tval_Loss: 0.005363\n",
      "Train Epoch: 7996/10000 (80%)\ttrain_Loss: 0.004826\tval_Loss: 0.005381\n",
      "Train Epoch: 7997/10000 (80%)\ttrain_Loss: 0.004825\tval_Loss: 0.005374\n",
      "Train Epoch: 7998/10000 (80%)\ttrain_Loss: 0.004825\tval_Loss: 0.005366\n",
      "Train Epoch: 7999/10000 (80%)\ttrain_Loss: 0.004825\tval_Loss: 0.005383\n",
      "Train Epoch: 8000/10000 (80%)\ttrain_Loss: 0.004825\tval_Loss: 0.005367\n",
      "Train Epoch: 8001/10000 (80%)\ttrain_Loss: 0.004824\tval_Loss: 0.005374\n",
      "Train Epoch: 8002/10000 (80%)\ttrain_Loss: 0.004824\tval_Loss: 0.005378\n",
      "Train Epoch: 8003/10000 (80%)\ttrain_Loss: 0.004824\tval_Loss: 0.005366\n",
      "Train Epoch: 8004/10000 (80%)\ttrain_Loss: 0.004824\tval_Loss: 0.005379\n",
      "Train Epoch: 8005/10000 (80%)\ttrain_Loss: 0.004824\tval_Loss: 0.005371\n",
      "Train Epoch: 8006/10000 (80%)\ttrain_Loss: 0.004823\tval_Loss: 0.005370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8007/10000 (80%)\ttrain_Loss: 0.004823\tval_Loss: 0.005378\n",
      "Train Epoch: 8008/10000 (80%)\ttrain_Loss: 0.004823\tval_Loss: 0.005368\n",
      "Train Epoch: 8009/10000 (80%)\ttrain_Loss: 0.004823\tval_Loss: 0.005375\n",
      "Train Epoch: 8010/10000 (80%)\ttrain_Loss: 0.004823\tval_Loss: 0.005374\n",
      "Train Epoch: 8011/10000 (80%)\ttrain_Loss: 0.004823\tval_Loss: 0.005368\n",
      "Train Epoch: 8012/10000 (80%)\ttrain_Loss: 0.004822\tval_Loss: 0.005377\n",
      "Train Epoch: 8013/10000 (80%)\ttrain_Loss: 0.004822\tval_Loss: 0.005369\n",
      "Train Epoch: 8014/10000 (80%)\ttrain_Loss: 0.004822\tval_Loss: 0.005372\n",
      "Train Epoch: 8015/10000 (80%)\ttrain_Loss: 0.004822\tval_Loss: 0.005375\n",
      "Train Epoch: 8016/10000 (80%)\ttrain_Loss: 0.004822\tval_Loss: 0.005368\n",
      "Train Epoch: 8017/10000 (80%)\ttrain_Loss: 0.004822\tval_Loss: 0.005375\n",
      "Train Epoch: 8018/10000 (80%)\ttrain_Loss: 0.004821\tval_Loss: 0.005371\n",
      "Train Epoch: 8019/10000 (80%)\ttrain_Loss: 0.004821\tval_Loss: 0.005371\n",
      "Train Epoch: 8020/10000 (80%)\ttrain_Loss: 0.004821\tval_Loss: 0.005374\n",
      "Train Epoch: 8021/10000 (80%)\ttrain_Loss: 0.004821\tval_Loss: 0.005369\n",
      "Train Epoch: 8022/10000 (80%)\ttrain_Loss: 0.004821\tval_Loss: 0.005373\n",
      "Train Epoch: 8023/10000 (80%)\ttrain_Loss: 0.004821\tval_Loss: 0.005371\n",
      "Train Epoch: 8024/10000 (80%)\ttrain_Loss: 0.004820\tval_Loss: 0.005370\n",
      "Train Epoch: 8025/10000 (80%)\ttrain_Loss: 0.004820\tval_Loss: 0.005373\n",
      "Train Epoch: 8026/10000 (80%)\ttrain_Loss: 0.004820\tval_Loss: 0.005370\n",
      "Train Epoch: 8027/10000 (80%)\ttrain_Loss: 0.004820\tval_Loss: 0.005372\n",
      "Train Epoch: 8028/10000 (80%)\ttrain_Loss: 0.004820\tval_Loss: 0.005372\n",
      "Train Epoch: 8029/10000 (80%)\ttrain_Loss: 0.004820\tval_Loss: 0.005370\n",
      "Train Epoch: 8030/10000 (80%)\ttrain_Loss: 0.004820\tval_Loss: 0.005372\n",
      "Train Epoch: 8031/10000 (80%)\ttrain_Loss: 0.004819\tval_Loss: 0.005370\n",
      "Train Epoch: 8032/10000 (80%)\ttrain_Loss: 0.004819\tval_Loss: 0.005371\n",
      "Train Epoch: 8033/10000 (80%)\ttrain_Loss: 0.004819\tval_Loss: 0.005371\n",
      "Train Epoch: 8034/10000 (80%)\ttrain_Loss: 0.004819\tval_Loss: 0.005369\n",
      "Train Epoch: 8035/10000 (80%)\ttrain_Loss: 0.004819\tval_Loss: 0.005371\n",
      "Train Epoch: 8036/10000 (80%)\ttrain_Loss: 0.004819\tval_Loss: 0.005369\n",
      "Train Epoch: 8037/10000 (80%)\ttrain_Loss: 0.004818\tval_Loss: 0.005369\n",
      "Train Epoch: 8038/10000 (80%)\ttrain_Loss: 0.004818\tval_Loss: 0.005370\n",
      "Train Epoch: 8039/10000 (80%)\ttrain_Loss: 0.004818\tval_Loss: 0.005367\n",
      "Train Epoch: 8040/10000 (80%)\ttrain_Loss: 0.004818\tval_Loss: 0.005371\n",
      "Train Epoch: 8041/10000 (80%)\ttrain_Loss: 0.004818\tval_Loss: 0.005368\n",
      "Train Epoch: 8042/10000 (80%)\ttrain_Loss: 0.004818\tval_Loss: 0.005368\n",
      "Train Epoch: 8043/10000 (80%)\ttrain_Loss: 0.004817\tval_Loss: 0.005370\n",
      "Train Epoch: 8044/10000 (80%)\ttrain_Loss: 0.004817\tval_Loss: 0.005367\n",
      "Train Epoch: 8045/10000 (80%)\ttrain_Loss: 0.004817\tval_Loss: 0.005369\n",
      "Train Epoch: 8046/10000 (80%)\ttrain_Loss: 0.004817\tval_Loss: 0.005367\n",
      "Train Epoch: 8047/10000 (80%)\ttrain_Loss: 0.004817\tval_Loss: 0.005367\n",
      "Train Epoch: 8048/10000 (80%)\ttrain_Loss: 0.004817\tval_Loss: 0.005368\n",
      "Train Epoch: 8049/10000 (80%)\ttrain_Loss: 0.004817\tval_Loss: 0.005368\n",
      "Train Epoch: 8050/10000 (80%)\ttrain_Loss: 0.004816\tval_Loss: 0.005369\n",
      "Train Epoch: 8051/10000 (80%)\ttrain_Loss: 0.004816\tval_Loss: 0.005368\n",
      "Train Epoch: 8052/10000 (81%)\ttrain_Loss: 0.004816\tval_Loss: 0.005369\n",
      "Train Epoch: 8053/10000 (81%)\ttrain_Loss: 0.004816\tval_Loss: 0.005367\n",
      "Train Epoch: 8054/10000 (81%)\ttrain_Loss: 0.004816\tval_Loss: 0.005367\n",
      "Train Epoch: 8055/10000 (81%)\ttrain_Loss: 0.004816\tval_Loss: 0.005367\n",
      "Train Epoch: 8056/10000 (81%)\ttrain_Loss: 0.004815\tval_Loss: 0.005365\n",
      "Train Epoch: 8057/10000 (81%)\ttrain_Loss: 0.004815\tval_Loss: 0.005366\n",
      "Train Epoch: 8058/10000 (81%)\ttrain_Loss: 0.004815\tval_Loss: 0.005365\n",
      "Train Epoch: 8059/10000 (81%)\ttrain_Loss: 0.004815\tval_Loss: 0.005366\n",
      "Train Epoch: 8060/10000 (81%)\ttrain_Loss: 0.004815\tval_Loss: 0.005366\n",
      "Train Epoch: 8061/10000 (81%)\ttrain_Loss: 0.004815\tval_Loss: 0.005365\n",
      "Train Epoch: 8062/10000 (81%)\ttrain_Loss: 0.004815\tval_Loss: 0.005367\n",
      "Train Epoch: 8063/10000 (81%)\ttrain_Loss: 0.004814\tval_Loss: 0.005366\n",
      "Train Epoch: 8064/10000 (81%)\ttrain_Loss: 0.004814\tval_Loss: 0.005366\n",
      "Train Epoch: 8065/10000 (81%)\ttrain_Loss: 0.004814\tval_Loss: 0.005366\n",
      "Train Epoch: 8066/10000 (81%)\ttrain_Loss: 0.004814\tval_Loss: 0.005364\n",
      "Train Epoch: 8067/10000 (81%)\ttrain_Loss: 0.004814\tval_Loss: 0.005365\n",
      "Train Epoch: 8068/10000 (81%)\ttrain_Loss: 0.004814\tval_Loss: 0.005364\n",
      "Train Epoch: 8069/10000 (81%)\ttrain_Loss: 0.004813\tval_Loss: 0.005364\n",
      "Train Epoch: 8070/10000 (81%)\ttrain_Loss: 0.004813\tval_Loss: 0.005366\n",
      "Train Epoch: 8071/10000 (81%)\ttrain_Loss: 0.004813\tval_Loss: 0.005363\n",
      "Train Epoch: 8072/10000 (81%)\ttrain_Loss: 0.004813\tval_Loss: 0.005367\n",
      "Train Epoch: 8073/10000 (81%)\ttrain_Loss: 0.004813\tval_Loss: 0.005362\n",
      "Train Epoch: 8074/10000 (81%)\ttrain_Loss: 0.004813\tval_Loss: 0.005365\n",
      "Train Epoch: 8075/10000 (81%)\ttrain_Loss: 0.004812\tval_Loss: 0.005363\n",
      "Train Epoch: 8076/10000 (81%)\ttrain_Loss: 0.004812\tval_Loss: 0.005361\n",
      "Train Epoch: 8077/10000 (81%)\ttrain_Loss: 0.004812\tval_Loss: 0.005366\n",
      "Train Epoch: 8078/10000 (81%)\ttrain_Loss: 0.004812\tval_Loss: 0.005361\n",
      "Train Epoch: 8079/10000 (81%)\ttrain_Loss: 0.004812\tval_Loss: 0.005366\n",
      "Train Epoch: 8080/10000 (81%)\ttrain_Loss: 0.004812\tval_Loss: 0.005362\n",
      "Train Epoch: 8081/10000 (81%)\ttrain_Loss: 0.004812\tval_Loss: 0.005365\n",
      "Train Epoch: 8082/10000 (81%)\ttrain_Loss: 0.004811\tval_Loss: 0.005363\n",
      "Train Epoch: 8083/10000 (81%)\ttrain_Loss: 0.004811\tval_Loss: 0.005362\n",
      "Train Epoch: 8084/10000 (81%)\ttrain_Loss: 0.004811\tval_Loss: 0.005363\n",
      "Train Epoch: 8085/10000 (81%)\ttrain_Loss: 0.004811\tval_Loss: 0.005360\n",
      "Train Epoch: 8086/10000 (81%)\ttrain_Loss: 0.004811\tval_Loss: 0.005363\n",
      "Train Epoch: 8087/10000 (81%)\ttrain_Loss: 0.004811\tval_Loss: 0.005361\n",
      "Train Epoch: 8088/10000 (81%)\ttrain_Loss: 0.004810\tval_Loss: 0.005362\n",
      "Train Epoch: 8089/10000 (81%)\ttrain_Loss: 0.004810\tval_Loss: 0.005363\n",
      "Train Epoch: 8090/10000 (81%)\ttrain_Loss: 0.004810\tval_Loss: 0.005360\n",
      "Train Epoch: 8091/10000 (81%)\ttrain_Loss: 0.004810\tval_Loss: 0.005363\n",
      "Train Epoch: 8092/10000 (81%)\ttrain_Loss: 0.004810\tval_Loss: 0.005360\n",
      "Train Epoch: 8093/10000 (81%)\ttrain_Loss: 0.004810\tval_Loss: 0.005362\n",
      "Train Epoch: 8094/10000 (81%)\ttrain_Loss: 0.004809\tval_Loss: 0.005359\n",
      "Train Epoch: 8095/10000 (81%)\ttrain_Loss: 0.004809\tval_Loss: 0.005363\n",
      "Train Epoch: 8096/10000 (81%)\ttrain_Loss: 0.004809\tval_Loss: 0.005359\n",
      "Train Epoch: 8097/10000 (81%)\ttrain_Loss: 0.004809\tval_Loss: 0.005361\n",
      "Train Epoch: 8098/10000 (81%)\ttrain_Loss: 0.004809\tval_Loss: 0.005360\n",
      "Train Epoch: 8099/10000 (81%)\ttrain_Loss: 0.004809\tval_Loss: 0.005358\n",
      "Train Epoch: 8100/10000 (81%)\ttrain_Loss: 0.004808\tval_Loss: 0.005361\n",
      "Train Epoch: 8101/10000 (81%)\ttrain_Loss: 0.004808\tval_Loss: 0.005357\n",
      "Train Epoch: 8102/10000 (81%)\ttrain_Loss: 0.004808\tval_Loss: 0.005362\n",
      "Train Epoch: 8103/10000 (81%)\ttrain_Loss: 0.004808\tval_Loss: 0.005358\n",
      "Train Epoch: 8104/10000 (81%)\ttrain_Loss: 0.004808\tval_Loss: 0.005361\n",
      "Train Epoch: 8105/10000 (81%)\ttrain_Loss: 0.004808\tval_Loss: 0.005358\n",
      "Train Epoch: 8106/10000 (81%)\ttrain_Loss: 0.004808\tval_Loss: 0.005359\n",
      "Train Epoch: 8107/10000 (81%)\ttrain_Loss: 0.004807\tval_Loss: 0.005359\n",
      "Train Epoch: 8108/10000 (81%)\ttrain_Loss: 0.004807\tval_Loss: 0.005357\n",
      "Train Epoch: 8109/10000 (81%)\ttrain_Loss: 0.004807\tval_Loss: 0.005359\n",
      "Train Epoch: 8110/10000 (81%)\ttrain_Loss: 0.004807\tval_Loss: 0.005356\n",
      "Train Epoch: 8111/10000 (81%)\ttrain_Loss: 0.004807\tval_Loss: 0.005359\n",
      "Train Epoch: 8112/10000 (81%)\ttrain_Loss: 0.004807\tval_Loss: 0.005358\n",
      "Train Epoch: 8113/10000 (81%)\ttrain_Loss: 0.004806\tval_Loss: 0.005358\n",
      "Train Epoch: 8114/10000 (81%)\ttrain_Loss: 0.004806\tval_Loss: 0.005358\n",
      "Train Epoch: 8115/10000 (81%)\ttrain_Loss: 0.004806\tval_Loss: 0.005356\n",
      "Train Epoch: 8116/10000 (81%)\ttrain_Loss: 0.004806\tval_Loss: 0.005358\n",
      "Train Epoch: 8117/10000 (81%)\ttrain_Loss: 0.004806\tval_Loss: 0.005355\n",
      "Train Epoch: 8118/10000 (81%)\ttrain_Loss: 0.004806\tval_Loss: 0.005360\n",
      "Train Epoch: 8119/10000 (81%)\ttrain_Loss: 0.004805\tval_Loss: 0.005354\n",
      "Train Epoch: 8120/10000 (81%)\ttrain_Loss: 0.004805\tval_Loss: 0.005362\n",
      "Train Epoch: 8121/10000 (81%)\ttrain_Loss: 0.004805\tval_Loss: 0.005353\n",
      "Train Epoch: 8122/10000 (81%)\ttrain_Loss: 0.004805\tval_Loss: 0.005361\n",
      "Train Epoch: 8123/10000 (81%)\ttrain_Loss: 0.004805\tval_Loss: 0.005352\n",
      "Train Epoch: 8124/10000 (81%)\ttrain_Loss: 0.004805\tval_Loss: 0.005359\n",
      "Train Epoch: 8125/10000 (81%)\ttrain_Loss: 0.004805\tval_Loss: 0.005352\n",
      "Train Epoch: 8126/10000 (81%)\ttrain_Loss: 0.004804\tval_Loss: 0.005356\n",
      "Train Epoch: 8127/10000 (81%)\ttrain_Loss: 0.004804\tval_Loss: 0.005355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8128/10000 (81%)\ttrain_Loss: 0.004804\tval_Loss: 0.005354\n",
      "Train Epoch: 8129/10000 (81%)\ttrain_Loss: 0.004804\tval_Loss: 0.005357\n",
      "Train Epoch: 8130/10000 (81%)\ttrain_Loss: 0.004804\tval_Loss: 0.005352\n",
      "Train Epoch: 8131/10000 (81%)\ttrain_Loss: 0.004803\tval_Loss: 0.005358\n",
      "Train Epoch: 8132/10000 (81%)\ttrain_Loss: 0.004803\tval_Loss: 0.005350\n",
      "Train Epoch: 8133/10000 (81%)\ttrain_Loss: 0.004803\tval_Loss: 0.005359\n",
      "Train Epoch: 8134/10000 (81%)\ttrain_Loss: 0.004803\tval_Loss: 0.005349\n",
      "Train Epoch: 8135/10000 (81%)\ttrain_Loss: 0.004803\tval_Loss: 0.005358\n",
      "Train Epoch: 8136/10000 (81%)\ttrain_Loss: 0.004803\tval_Loss: 0.005349\n",
      "Train Epoch: 8137/10000 (81%)\ttrain_Loss: 0.004803\tval_Loss: 0.005357\n",
      "Train Epoch: 8138/10000 (81%)\ttrain_Loss: 0.004802\tval_Loss: 0.005351\n",
      "Train Epoch: 8139/10000 (81%)\ttrain_Loss: 0.004802\tval_Loss: 0.005354\n",
      "Train Epoch: 8140/10000 (81%)\ttrain_Loss: 0.004802\tval_Loss: 0.005352\n",
      "Train Epoch: 8141/10000 (81%)\ttrain_Loss: 0.004802\tval_Loss: 0.005352\n",
      "Train Epoch: 8142/10000 (81%)\ttrain_Loss: 0.004802\tval_Loss: 0.005353\n",
      "Train Epoch: 8143/10000 (81%)\ttrain_Loss: 0.004801\tval_Loss: 0.005351\n",
      "Train Epoch: 8144/10000 (81%)\ttrain_Loss: 0.004801\tval_Loss: 0.005355\n",
      "Train Epoch: 8145/10000 (81%)\ttrain_Loss: 0.004801\tval_Loss: 0.005349\n",
      "Train Epoch: 8146/10000 (81%)\ttrain_Loss: 0.004801\tval_Loss: 0.005359\n",
      "Train Epoch: 8147/10000 (81%)\ttrain_Loss: 0.004801\tval_Loss: 0.005346\n",
      "Train Epoch: 8148/10000 (81%)\ttrain_Loss: 0.004801\tval_Loss: 0.005361\n",
      "Train Epoch: 8149/10000 (81%)\ttrain_Loss: 0.004801\tval_Loss: 0.005342\n",
      "Train Epoch: 8150/10000 (81%)\ttrain_Loss: 0.004801\tval_Loss: 0.005360\n",
      "Train Epoch: 8151/10000 (82%)\ttrain_Loss: 0.004801\tval_Loss: 0.005340\n",
      "Train Epoch: 8152/10000 (82%)\ttrain_Loss: 0.004801\tval_Loss: 0.005361\n",
      "Train Epoch: 8153/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005340\n",
      "Train Epoch: 8154/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005361\n",
      "Train Epoch: 8155/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005340\n",
      "Train Epoch: 8156/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005362\n",
      "Train Epoch: 8157/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005339\n",
      "Train Epoch: 8158/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005363\n",
      "Train Epoch: 8159/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005337\n",
      "Train Epoch: 8160/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005364\n",
      "Train Epoch: 8161/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005335\n",
      "Train Epoch: 8162/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005369\n",
      "Train Epoch: 8163/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005332\n",
      "Train Epoch: 8164/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005377\n",
      "Train Epoch: 8165/10000 (82%)\ttrain_Loss: 0.004801\tval_Loss: 0.005327\n",
      "Train Epoch: 8166/10000 (82%)\ttrain_Loss: 0.004802\tval_Loss: 0.005387\n",
      "Train Epoch: 8167/10000 (82%)\ttrain_Loss: 0.004804\tval_Loss: 0.005319\n",
      "Train Epoch: 8168/10000 (82%)\ttrain_Loss: 0.004806\tval_Loss: 0.005403\n",
      "Train Epoch: 8169/10000 (82%)\ttrain_Loss: 0.004809\tval_Loss: 0.005312\n",
      "Train Epoch: 8170/10000 (82%)\ttrain_Loss: 0.004813\tval_Loss: 0.005430\n",
      "Train Epoch: 8171/10000 (82%)\ttrain_Loss: 0.004820\tval_Loss: 0.005308\n",
      "Train Epoch: 8172/10000 (82%)\ttrain_Loss: 0.004828\tval_Loss: 0.005471\n",
      "Train Epoch: 8173/10000 (82%)\ttrain_Loss: 0.004838\tval_Loss: 0.005309\n",
      "Train Epoch: 8174/10000 (82%)\ttrain_Loss: 0.004850\tval_Loss: 0.005515\n",
      "Train Epoch: 8175/10000 (82%)\ttrain_Loss: 0.004861\tval_Loss: 0.005312\n",
      "Train Epoch: 8176/10000 (82%)\ttrain_Loss: 0.004868\tval_Loss: 0.005526\n",
      "Train Epoch: 8177/10000 (82%)\ttrain_Loss: 0.004867\tval_Loss: 0.005309\n",
      "Train Epoch: 8178/10000 (82%)\ttrain_Loss: 0.004856\tval_Loss: 0.005468\n",
      "Train Epoch: 8179/10000 (82%)\ttrain_Loss: 0.004836\tval_Loss: 0.005310\n",
      "Train Epoch: 8180/10000 (82%)\ttrain_Loss: 0.004814\tval_Loss: 0.005374\n",
      "Train Epoch: 8181/10000 (82%)\ttrain_Loss: 0.004799\tval_Loss: 0.005353\n",
      "Train Epoch: 8182/10000 (82%)\ttrain_Loss: 0.004795\tval_Loss: 0.005320\n",
      "Train Epoch: 8183/10000 (82%)\ttrain_Loss: 0.004802\tval_Loss: 0.005416\n",
      "Train Epoch: 8184/10000 (82%)\ttrain_Loss: 0.004812\tval_Loss: 0.005307\n",
      "Train Epoch: 8185/10000 (82%)\ttrain_Loss: 0.004820\tval_Loss: 0.005438\n",
      "Train Epoch: 8186/10000 (82%)\ttrain_Loss: 0.004821\tval_Loss: 0.005308\n",
      "Train Epoch: 8187/10000 (82%)\ttrain_Loss: 0.004816\tval_Loss: 0.005399\n",
      "Train Epoch: 8188/10000 (82%)\ttrain_Loss: 0.004806\tval_Loss: 0.005324\n",
      "Train Epoch: 8189/10000 (82%)\ttrain_Loss: 0.004798\tval_Loss: 0.005346\n",
      "Train Epoch: 8190/10000 (82%)\ttrain_Loss: 0.004794\tval_Loss: 0.005362\n",
      "Train Epoch: 8191/10000 (82%)\ttrain_Loss: 0.004796\tval_Loss: 0.005317\n",
      "Train Epoch: 8192/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005395\n",
      "Train Epoch: 8193/10000 (82%)\ttrain_Loss: 0.004804\tval_Loss: 0.005311\n",
      "Train Epoch: 8194/10000 (82%)\ttrain_Loss: 0.004805\tval_Loss: 0.005394\n",
      "Train Epoch: 8195/10000 (82%)\ttrain_Loss: 0.004803\tval_Loss: 0.005317\n",
      "Train Epoch: 8196/10000 (82%)\ttrain_Loss: 0.004800\tval_Loss: 0.005365\n",
      "Train Epoch: 8197/10000 (82%)\ttrain_Loss: 0.004796\tval_Loss: 0.005335\n",
      "Train Epoch: 8198/10000 (82%)\ttrain_Loss: 0.004793\tval_Loss: 0.005336\n",
      "Train Epoch: 8199/10000 (82%)\ttrain_Loss: 0.004793\tval_Loss: 0.005359\n",
      "Train Epoch: 8200/10000 (82%)\ttrain_Loss: 0.004794\tval_Loss: 0.005321\n",
      "Train Epoch: 8201/10000 (82%)\ttrain_Loss: 0.004796\tval_Loss: 0.005375\n",
      "Train Epoch: 8202/10000 (82%)\ttrain_Loss: 0.004797\tval_Loss: 0.005318\n",
      "Train Epoch: 8203/10000 (82%)\ttrain_Loss: 0.004797\tval_Loss: 0.005372\n",
      "Train Epoch: 8204/10000 (82%)\ttrain_Loss: 0.004796\tval_Loss: 0.005324\n",
      "Train Epoch: 8205/10000 (82%)\ttrain_Loss: 0.004794\tval_Loss: 0.005355\n",
      "Train Epoch: 8206/10000 (82%)\ttrain_Loss: 0.004793\tval_Loss: 0.005337\n",
      "Train Epoch: 8207/10000 (82%)\ttrain_Loss: 0.004792\tval_Loss: 0.005338\n",
      "Train Epoch: 8208/10000 (82%)\ttrain_Loss: 0.004791\tval_Loss: 0.005352\n",
      "Train Epoch: 8209/10000 (82%)\ttrain_Loss: 0.004792\tval_Loss: 0.005327\n",
      "Train Epoch: 8210/10000 (82%)\ttrain_Loss: 0.004792\tval_Loss: 0.005361\n",
      "Train Epoch: 8211/10000 (82%)\ttrain_Loss: 0.004793\tval_Loss: 0.005323\n",
      "Train Epoch: 8212/10000 (82%)\ttrain_Loss: 0.004793\tval_Loss: 0.005361\n",
      "Train Epoch: 8213/10000 (82%)\ttrain_Loss: 0.004793\tval_Loss: 0.005324\n",
      "Train Epoch: 8214/10000 (82%)\ttrain_Loss: 0.004792\tval_Loss: 0.005356\n",
      "Train Epoch: 8215/10000 (82%)\ttrain_Loss: 0.004791\tval_Loss: 0.005329\n",
      "Train Epoch: 8216/10000 (82%)\ttrain_Loss: 0.004791\tval_Loss: 0.005346\n",
      "Train Epoch: 8217/10000 (82%)\ttrain_Loss: 0.004790\tval_Loss: 0.005337\n",
      "Train Epoch: 8218/10000 (82%)\ttrain_Loss: 0.004790\tval_Loss: 0.005337\n",
      "Train Epoch: 8219/10000 (82%)\ttrain_Loss: 0.004790\tval_Loss: 0.005345\n",
      "Train Epoch: 8220/10000 (82%)\ttrain_Loss: 0.004790\tval_Loss: 0.005330\n",
      "Train Epoch: 8221/10000 (82%)\ttrain_Loss: 0.004790\tval_Loss: 0.005350\n",
      "Train Epoch: 8222/10000 (82%)\ttrain_Loss: 0.004790\tval_Loss: 0.005326\n",
      "Train Epoch: 8223/10000 (82%)\ttrain_Loss: 0.004790\tval_Loss: 0.005352\n",
      "Train Epoch: 8224/10000 (82%)\ttrain_Loss: 0.004790\tval_Loss: 0.005325\n",
      "Train Epoch: 8225/10000 (82%)\ttrain_Loss: 0.004790\tval_Loss: 0.005352\n",
      "Train Epoch: 8226/10000 (82%)\ttrain_Loss: 0.004789\tval_Loss: 0.005325\n",
      "Train Epoch: 8227/10000 (82%)\ttrain_Loss: 0.004789\tval_Loss: 0.005350\n",
      "Train Epoch: 8228/10000 (82%)\ttrain_Loss: 0.004789\tval_Loss: 0.005327\n",
      "Train Epoch: 8229/10000 (82%)\ttrain_Loss: 0.004789\tval_Loss: 0.005347\n",
      "Train Epoch: 8230/10000 (82%)\ttrain_Loss: 0.004788\tval_Loss: 0.005329\n",
      "Train Epoch: 8231/10000 (82%)\ttrain_Loss: 0.004788\tval_Loss: 0.005342\n",
      "Train Epoch: 8232/10000 (82%)\ttrain_Loss: 0.004788\tval_Loss: 0.005332\n",
      "Train Epoch: 8233/10000 (82%)\ttrain_Loss: 0.004787\tval_Loss: 0.005338\n",
      "Train Epoch: 8234/10000 (82%)\ttrain_Loss: 0.004787\tval_Loss: 0.005334\n",
      "Train Epoch: 8235/10000 (82%)\ttrain_Loss: 0.004787\tval_Loss: 0.005335\n",
      "Train Epoch: 8236/10000 (82%)\ttrain_Loss: 0.004787\tval_Loss: 0.005338\n",
      "Train Epoch: 8237/10000 (82%)\ttrain_Loss: 0.004787\tval_Loss: 0.005332\n",
      "Train Epoch: 8238/10000 (82%)\ttrain_Loss: 0.004787\tval_Loss: 0.005341\n",
      "Train Epoch: 8239/10000 (82%)\ttrain_Loss: 0.004786\tval_Loss: 0.005329\n",
      "Train Epoch: 8240/10000 (82%)\ttrain_Loss: 0.004786\tval_Loss: 0.005344\n",
      "Train Epoch: 8241/10000 (82%)\ttrain_Loss: 0.004786\tval_Loss: 0.005326\n",
      "Train Epoch: 8242/10000 (82%)\ttrain_Loss: 0.004786\tval_Loss: 0.005347\n",
      "Train Epoch: 8243/10000 (82%)\ttrain_Loss: 0.004786\tval_Loss: 0.005322\n",
      "Train Epoch: 8244/10000 (82%)\ttrain_Loss: 0.004787\tval_Loss: 0.005350\n",
      "Train Epoch: 8245/10000 (82%)\ttrain_Loss: 0.004787\tval_Loss: 0.005318\n",
      "Train Epoch: 8246/10000 (82%)\ttrain_Loss: 0.004787\tval_Loss: 0.005355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8247/10000 (82%)\ttrain_Loss: 0.004787\tval_Loss: 0.005314\n",
      "Train Epoch: 8248/10000 (82%)\ttrain_Loss: 0.004788\tval_Loss: 0.005365\n",
      "Train Epoch: 8249/10000 (82%)\ttrain_Loss: 0.004789\tval_Loss: 0.005309\n",
      "Train Epoch: 8250/10000 (82%)\ttrain_Loss: 0.004791\tval_Loss: 0.005383\n",
      "Train Epoch: 8251/10000 (82%)\ttrain_Loss: 0.004794\tval_Loss: 0.005301\n",
      "Train Epoch: 8252/10000 (83%)\ttrain_Loss: 0.004797\tval_Loss: 0.005408\n",
      "Train Epoch: 8253/10000 (83%)\ttrain_Loss: 0.004803\tval_Loss: 0.005294\n",
      "Train Epoch: 8254/10000 (83%)\ttrain_Loss: 0.004810\tval_Loss: 0.005447\n",
      "Train Epoch: 8255/10000 (83%)\ttrain_Loss: 0.004820\tval_Loss: 0.005293\n",
      "Train Epoch: 8256/10000 (83%)\ttrain_Loss: 0.004833\tval_Loss: 0.005495\n",
      "Train Epoch: 8257/10000 (83%)\ttrain_Loss: 0.004845\tval_Loss: 0.005298\n",
      "Train Epoch: 8258/10000 (83%)\ttrain_Loss: 0.004857\tval_Loss: 0.005524\n",
      "Train Epoch: 8259/10000 (83%)\ttrain_Loss: 0.004861\tval_Loss: 0.005297\n",
      "Train Epoch: 8260/10000 (83%)\ttrain_Loss: 0.004856\tval_Loss: 0.005485\n",
      "Train Epoch: 8261/10000 (83%)\ttrain_Loss: 0.004839\tval_Loss: 0.005292\n",
      "Train Epoch: 8262/10000 (83%)\ttrain_Loss: 0.004816\tval_Loss: 0.005388\n",
      "Train Epoch: 8263/10000 (83%)\ttrain_Loss: 0.004794\tval_Loss: 0.005320\n",
      "Train Epoch: 8264/10000 (83%)\ttrain_Loss: 0.004783\tval_Loss: 0.005314\n",
      "Train Epoch: 8265/10000 (83%)\ttrain_Loss: 0.004784\tval_Loss: 0.005385\n",
      "Train Epoch: 8266/10000 (83%)\ttrain_Loss: 0.004794\tval_Loss: 0.005292\n",
      "Train Epoch: 8267/10000 (83%)\ttrain_Loss: 0.004805\tval_Loss: 0.005427\n",
      "Train Epoch: 8268/10000 (83%)\ttrain_Loss: 0.004810\tval_Loss: 0.005291\n",
      "Train Epoch: 8269/10000 (83%)\ttrain_Loss: 0.004808\tval_Loss: 0.005402\n",
      "Train Epoch: 8270/10000 (83%)\ttrain_Loss: 0.004800\tval_Loss: 0.005301\n",
      "Train Epoch: 8271/10000 (83%)\ttrain_Loss: 0.004790\tval_Loss: 0.005346\n",
      "Train Epoch: 8272/10000 (83%)\ttrain_Loss: 0.004783\tval_Loss: 0.005336\n",
      "Train Epoch: 8273/10000 (83%)\ttrain_Loss: 0.004781\tval_Loss: 0.005308\n",
      "Train Epoch: 8274/10000 (83%)\ttrain_Loss: 0.004785\tval_Loss: 0.005376\n",
      "Train Epoch: 8275/10000 (83%)\ttrain_Loss: 0.004790\tval_Loss: 0.005296\n",
      "Train Epoch: 8276/10000 (83%)\ttrain_Loss: 0.004793\tval_Loss: 0.005387\n",
      "Train Epoch: 8277/10000 (83%)\ttrain_Loss: 0.004793\tval_Loss: 0.005299\n",
      "Train Epoch: 8278/10000 (83%)\ttrain_Loss: 0.004790\tval_Loss: 0.005363\n",
      "Train Epoch: 8279/10000 (83%)\ttrain_Loss: 0.004785\tval_Loss: 0.005315\n",
      "Train Epoch: 8280/10000 (83%)\ttrain_Loss: 0.004782\tval_Loss: 0.005330\n",
      "Train Epoch: 8281/10000 (83%)\ttrain_Loss: 0.004780\tval_Loss: 0.005341\n",
      "Train Epoch: 8282/10000 (83%)\ttrain_Loss: 0.004781\tval_Loss: 0.005309\n",
      "Train Epoch: 8283/10000 (83%)\ttrain_Loss: 0.004783\tval_Loss: 0.005361\n",
      "Train Epoch: 8284/10000 (83%)\ttrain_Loss: 0.004784\tval_Loss: 0.005303\n",
      "Train Epoch: 8285/10000 (83%)\ttrain_Loss: 0.004785\tval_Loss: 0.005361\n",
      "Train Epoch: 8286/10000 (83%)\ttrain_Loss: 0.004784\tval_Loss: 0.005307\n",
      "Train Epoch: 8287/10000 (83%)\ttrain_Loss: 0.004783\tval_Loss: 0.005345\n",
      "Train Epoch: 8288/10000 (83%)\ttrain_Loss: 0.004781\tval_Loss: 0.005318\n",
      "Train Epoch: 8289/10000 (83%)\ttrain_Loss: 0.004779\tval_Loss: 0.005326\n",
      "Train Epoch: 8290/10000 (83%)\ttrain_Loss: 0.004779\tval_Loss: 0.005334\n",
      "Train Epoch: 8291/10000 (83%)\ttrain_Loss: 0.004779\tval_Loss: 0.005314\n",
      "Train Epoch: 8292/10000 (83%)\ttrain_Loss: 0.004779\tval_Loss: 0.005345\n",
      "Train Epoch: 8293/10000 (83%)\ttrain_Loss: 0.004780\tval_Loss: 0.005309\n",
      "Train Epoch: 8294/10000 (83%)\ttrain_Loss: 0.004780\tval_Loss: 0.005348\n",
      "Train Epoch: 8295/10000 (83%)\ttrain_Loss: 0.004780\tval_Loss: 0.005310\n",
      "Train Epoch: 8296/10000 (83%)\ttrain_Loss: 0.004780\tval_Loss: 0.005343\n",
      "Train Epoch: 8297/10000 (83%)\ttrain_Loss: 0.004779\tval_Loss: 0.005315\n",
      "Train Epoch: 8298/10000 (83%)\ttrain_Loss: 0.004778\tval_Loss: 0.005332\n",
      "Train Epoch: 8299/10000 (83%)\ttrain_Loss: 0.004778\tval_Loss: 0.005322\n",
      "Train Epoch: 8300/10000 (83%)\ttrain_Loss: 0.004777\tval_Loss: 0.005323\n",
      "Train Epoch: 8301/10000 (83%)\ttrain_Loss: 0.004777\tval_Loss: 0.005330\n",
      "Train Epoch: 8302/10000 (83%)\ttrain_Loss: 0.004777\tval_Loss: 0.005317\n",
      "Train Epoch: 8303/10000 (83%)\ttrain_Loss: 0.004777\tval_Loss: 0.005336\n",
      "Train Epoch: 8304/10000 (83%)\ttrain_Loss: 0.004777\tval_Loss: 0.005313\n",
      "Train Epoch: 8305/10000 (83%)\ttrain_Loss: 0.004777\tval_Loss: 0.005338\n",
      "Train Epoch: 8306/10000 (83%)\ttrain_Loss: 0.004777\tval_Loss: 0.005312\n",
      "Train Epoch: 8307/10000 (83%)\ttrain_Loss: 0.004777\tval_Loss: 0.005336\n",
      "Train Epoch: 8308/10000 (83%)\ttrain_Loss: 0.004777\tval_Loss: 0.005311\n",
      "Train Epoch: 8309/10000 (83%)\ttrain_Loss: 0.004777\tval_Loss: 0.005334\n",
      "Train Epoch: 8310/10000 (83%)\ttrain_Loss: 0.004776\tval_Loss: 0.005314\n",
      "Train Epoch: 8311/10000 (83%)\ttrain_Loss: 0.004776\tval_Loss: 0.005331\n",
      "Train Epoch: 8312/10000 (83%)\ttrain_Loss: 0.004776\tval_Loss: 0.005317\n",
      "Train Epoch: 8313/10000 (83%)\ttrain_Loss: 0.004775\tval_Loss: 0.005327\n",
      "Train Epoch: 8314/10000 (83%)\ttrain_Loss: 0.004775\tval_Loss: 0.005320\n",
      "Train Epoch: 8315/10000 (83%)\ttrain_Loss: 0.004775\tval_Loss: 0.005323\n",
      "Train Epoch: 8316/10000 (83%)\ttrain_Loss: 0.004775\tval_Loss: 0.005322\n",
      "Train Epoch: 8317/10000 (83%)\ttrain_Loss: 0.004774\tval_Loss: 0.005319\n",
      "Train Epoch: 8318/10000 (83%)\ttrain_Loss: 0.004774\tval_Loss: 0.005325\n",
      "Train Epoch: 8319/10000 (83%)\ttrain_Loss: 0.004774\tval_Loss: 0.005315\n",
      "Train Epoch: 8320/10000 (83%)\ttrain_Loss: 0.004774\tval_Loss: 0.005331\n",
      "Train Epoch: 8321/10000 (83%)\ttrain_Loss: 0.004774\tval_Loss: 0.005311\n",
      "Train Epoch: 8322/10000 (83%)\ttrain_Loss: 0.004774\tval_Loss: 0.005339\n",
      "Train Epoch: 8323/10000 (83%)\ttrain_Loss: 0.004775\tval_Loss: 0.005305\n",
      "Train Epoch: 8324/10000 (83%)\ttrain_Loss: 0.004775\tval_Loss: 0.005345\n",
      "Train Epoch: 8325/10000 (83%)\ttrain_Loss: 0.004776\tval_Loss: 0.005298\n",
      "Train Epoch: 8326/10000 (83%)\ttrain_Loss: 0.004777\tval_Loss: 0.005351\n",
      "Train Epoch: 8327/10000 (83%)\ttrain_Loss: 0.004778\tval_Loss: 0.005294\n",
      "Train Epoch: 8328/10000 (83%)\ttrain_Loss: 0.004778\tval_Loss: 0.005360\n",
      "Train Epoch: 8329/10000 (83%)\ttrain_Loss: 0.004780\tval_Loss: 0.005291\n",
      "Train Epoch: 8330/10000 (83%)\ttrain_Loss: 0.004781\tval_Loss: 0.005374\n",
      "Train Epoch: 8331/10000 (83%)\ttrain_Loss: 0.004783\tval_Loss: 0.005286\n",
      "Train Epoch: 8332/10000 (83%)\ttrain_Loss: 0.004786\tval_Loss: 0.005394\n",
      "Train Epoch: 8333/10000 (83%)\ttrain_Loss: 0.004790\tval_Loss: 0.005281\n",
      "Train Epoch: 8334/10000 (83%)\ttrain_Loss: 0.004795\tval_Loss: 0.005416\n",
      "Train Epoch: 8335/10000 (83%)\ttrain_Loss: 0.004800\tval_Loss: 0.005278\n",
      "Train Epoch: 8336/10000 (83%)\ttrain_Loss: 0.004806\tval_Loss: 0.005438\n",
      "Train Epoch: 8337/10000 (83%)\ttrain_Loss: 0.004811\tval_Loss: 0.005277\n",
      "Train Epoch: 8338/10000 (83%)\ttrain_Loss: 0.004814\tval_Loss: 0.005447\n",
      "Train Epoch: 8339/10000 (83%)\ttrain_Loss: 0.004815\tval_Loss: 0.005277\n",
      "Train Epoch: 8340/10000 (83%)\ttrain_Loss: 0.004812\tval_Loss: 0.005427\n",
      "Train Epoch: 8341/10000 (83%)\ttrain_Loss: 0.004805\tval_Loss: 0.005279\n",
      "Train Epoch: 8342/10000 (83%)\ttrain_Loss: 0.004795\tval_Loss: 0.005379\n",
      "Train Epoch: 8343/10000 (83%)\ttrain_Loss: 0.004784\tval_Loss: 0.005292\n",
      "Train Epoch: 8344/10000 (83%)\ttrain_Loss: 0.004776\tval_Loss: 0.005329\n",
      "Train Epoch: 8345/10000 (83%)\ttrain_Loss: 0.004771\tval_Loss: 0.005321\n",
      "Train Epoch: 8346/10000 (83%)\ttrain_Loss: 0.004770\tval_Loss: 0.005298\n",
      "Train Epoch: 8347/10000 (83%)\ttrain_Loss: 0.004772\tval_Loss: 0.005354\n",
      "Train Epoch: 8348/10000 (83%)\ttrain_Loss: 0.004776\tval_Loss: 0.005285\n",
      "Train Epoch: 8349/10000 (83%)\ttrain_Loss: 0.004780\tval_Loss: 0.005377\n",
      "Train Epoch: 8350/10000 (83%)\ttrain_Loss: 0.004783\tval_Loss: 0.005281\n",
      "Train Epoch: 8351/10000 (84%)\ttrain_Loss: 0.004784\tval_Loss: 0.005377\n",
      "Train Epoch: 8352/10000 (84%)\ttrain_Loss: 0.004783\tval_Loss: 0.005283\n",
      "Train Epoch: 8353/10000 (84%)\ttrain_Loss: 0.004781\tval_Loss: 0.005360\n",
      "Train Epoch: 8354/10000 (84%)\ttrain_Loss: 0.004777\tval_Loss: 0.005291\n",
      "Train Epoch: 8355/10000 (84%)\ttrain_Loss: 0.004773\tval_Loss: 0.005334\n",
      "Train Epoch: 8356/10000 (84%)\ttrain_Loss: 0.004770\tval_Loss: 0.005308\n",
      "Train Epoch: 8357/10000 (84%)\ttrain_Loss: 0.004769\tval_Loss: 0.005312\n",
      "Train Epoch: 8358/10000 (84%)\ttrain_Loss: 0.004768\tval_Loss: 0.005327\n",
      "Train Epoch: 8359/10000 (84%)\ttrain_Loss: 0.004769\tval_Loss: 0.005298\n",
      "Train Epoch: 8360/10000 (84%)\ttrain_Loss: 0.004770\tval_Loss: 0.005342\n",
      "Train Epoch: 8361/10000 (84%)\ttrain_Loss: 0.004771\tval_Loss: 0.005291\n",
      "Train Epoch: 8362/10000 (84%)\ttrain_Loss: 0.004772\tval_Loss: 0.005348\n",
      "Train Epoch: 8363/10000 (84%)\ttrain_Loss: 0.004773\tval_Loss: 0.005288\n",
      "Train Epoch: 8364/10000 (84%)\ttrain_Loss: 0.004773\tval_Loss: 0.005345\n",
      "Train Epoch: 8365/10000 (84%)\ttrain_Loss: 0.004772\tval_Loss: 0.005289\n",
      "Train Epoch: 8366/10000 (84%)\ttrain_Loss: 0.004771\tval_Loss: 0.005338\n",
      "Train Epoch: 8367/10000 (84%)\ttrain_Loss: 0.004770\tval_Loss: 0.005294\n",
      "Train Epoch: 8368/10000 (84%)\ttrain_Loss: 0.004769\tval_Loss: 0.005329\n",
      "Train Epoch: 8369/10000 (84%)\ttrain_Loss: 0.004768\tval_Loss: 0.005300\n",
      "Train Epoch: 8370/10000 (84%)\ttrain_Loss: 0.004767\tval_Loss: 0.005320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8371/10000 (84%)\ttrain_Loss: 0.004767\tval_Loss: 0.005306\n",
      "Train Epoch: 8372/10000 (84%)\ttrain_Loss: 0.004766\tval_Loss: 0.005312\n",
      "Train Epoch: 8373/10000 (84%)\ttrain_Loss: 0.004766\tval_Loss: 0.005312\n",
      "Train Epoch: 8374/10000 (84%)\ttrain_Loss: 0.004766\tval_Loss: 0.005306\n",
      "Train Epoch: 8375/10000 (84%)\ttrain_Loss: 0.004766\tval_Loss: 0.005317\n",
      "Train Epoch: 8376/10000 (84%)\ttrain_Loss: 0.004766\tval_Loss: 0.005302\n",
      "Train Epoch: 8377/10000 (84%)\ttrain_Loss: 0.004766\tval_Loss: 0.005321\n",
      "Train Epoch: 8378/10000 (84%)\ttrain_Loss: 0.004766\tval_Loss: 0.005298\n",
      "Train Epoch: 8379/10000 (84%)\ttrain_Loss: 0.004766\tval_Loss: 0.005326\n",
      "Train Epoch: 8380/10000 (84%)\ttrain_Loss: 0.004766\tval_Loss: 0.005294\n",
      "Train Epoch: 8381/10000 (84%)\ttrain_Loss: 0.004767\tval_Loss: 0.005332\n",
      "Train Epoch: 8382/10000 (84%)\ttrain_Loss: 0.004767\tval_Loss: 0.005290\n",
      "Train Epoch: 8383/10000 (84%)\ttrain_Loss: 0.004767\tval_Loss: 0.005338\n",
      "Train Epoch: 8384/10000 (84%)\ttrain_Loss: 0.004768\tval_Loss: 0.005286\n",
      "Train Epoch: 8385/10000 (84%)\ttrain_Loss: 0.004769\tval_Loss: 0.005344\n",
      "Train Epoch: 8386/10000 (84%)\ttrain_Loss: 0.004769\tval_Loss: 0.005282\n",
      "Train Epoch: 8387/10000 (84%)\ttrain_Loss: 0.004770\tval_Loss: 0.005352\n",
      "Train Epoch: 8388/10000 (84%)\ttrain_Loss: 0.004771\tval_Loss: 0.005278\n",
      "Train Epoch: 8389/10000 (84%)\ttrain_Loss: 0.004773\tval_Loss: 0.005365\n",
      "Train Epoch: 8390/10000 (84%)\ttrain_Loss: 0.004775\tval_Loss: 0.005274\n",
      "Train Epoch: 8391/10000 (84%)\ttrain_Loss: 0.004779\tval_Loss: 0.005386\n",
      "Train Epoch: 8392/10000 (84%)\ttrain_Loss: 0.004783\tval_Loss: 0.005269\n",
      "Train Epoch: 8393/10000 (84%)\ttrain_Loss: 0.004788\tval_Loss: 0.005411\n",
      "Train Epoch: 8394/10000 (84%)\ttrain_Loss: 0.004794\tval_Loss: 0.005267\n",
      "Train Epoch: 8395/10000 (84%)\ttrain_Loss: 0.004801\tval_Loss: 0.005436\n",
      "Train Epoch: 8396/10000 (84%)\ttrain_Loss: 0.004806\tval_Loss: 0.005267\n",
      "Train Epoch: 8397/10000 (84%)\ttrain_Loss: 0.004810\tval_Loss: 0.005445\n",
      "Train Epoch: 8398/10000 (84%)\ttrain_Loss: 0.004811\tval_Loss: 0.005266\n",
      "Train Epoch: 8399/10000 (84%)\ttrain_Loss: 0.004807\tval_Loss: 0.005423\n",
      "Train Epoch: 8400/10000 (84%)\ttrain_Loss: 0.004799\tval_Loss: 0.005268\n",
      "Train Epoch: 8401/10000 (84%)\ttrain_Loss: 0.004788\tval_Loss: 0.005369\n",
      "Train Epoch: 8402/10000 (84%)\ttrain_Loss: 0.004776\tval_Loss: 0.005281\n",
      "Train Epoch: 8403/10000 (84%)\ttrain_Loss: 0.004767\tval_Loss: 0.005315\n",
      "Train Epoch: 8404/10000 (84%)\ttrain_Loss: 0.004762\tval_Loss: 0.005315\n",
      "Train Epoch: 8405/10000 (84%)\ttrain_Loss: 0.004762\tval_Loss: 0.005283\n",
      "Train Epoch: 8406/10000 (84%)\ttrain_Loss: 0.004765\tval_Loss: 0.005352\n",
      "Train Epoch: 8407/10000 (84%)\ttrain_Loss: 0.004770\tval_Loss: 0.005270\n",
      "Train Epoch: 8408/10000 (84%)\ttrain_Loss: 0.004774\tval_Loss: 0.005372\n",
      "Train Epoch: 8409/10000 (84%)\ttrain_Loss: 0.004777\tval_Loss: 0.005269\n",
      "Train Epoch: 8410/10000 (84%)\ttrain_Loss: 0.004777\tval_Loss: 0.005367\n",
      "Train Epoch: 8411/10000 (84%)\ttrain_Loss: 0.004774\tval_Loss: 0.005274\n",
      "Train Epoch: 8412/10000 (84%)\ttrain_Loss: 0.004770\tval_Loss: 0.005340\n",
      "Train Epoch: 8413/10000 (84%)\ttrain_Loss: 0.004766\tval_Loss: 0.005287\n",
      "Train Epoch: 8414/10000 (84%)\ttrain_Loss: 0.004762\tval_Loss: 0.005310\n",
      "Train Epoch: 8415/10000 (84%)\ttrain_Loss: 0.004760\tval_Loss: 0.005308\n",
      "Train Epoch: 8416/10000 (84%)\ttrain_Loss: 0.004760\tval_Loss: 0.005290\n",
      "Train Epoch: 8417/10000 (84%)\ttrain_Loss: 0.004761\tval_Loss: 0.005328\n",
      "Train Epoch: 8418/10000 (84%)\ttrain_Loss: 0.004762\tval_Loss: 0.005279\n",
      "Train Epoch: 8419/10000 (84%)\ttrain_Loss: 0.004764\tval_Loss: 0.005340\n",
      "Train Epoch: 8420/10000 (84%)\ttrain_Loss: 0.004765\tval_Loss: 0.005276\n",
      "Train Epoch: 8421/10000 (84%)\ttrain_Loss: 0.004765\tval_Loss: 0.005340\n",
      "Train Epoch: 8422/10000 (84%)\ttrain_Loss: 0.004765\tval_Loss: 0.005277\n",
      "Train Epoch: 8423/10000 (84%)\ttrain_Loss: 0.004764\tval_Loss: 0.005333\n",
      "Train Epoch: 8424/10000 (84%)\ttrain_Loss: 0.004763\tval_Loss: 0.005283\n",
      "Train Epoch: 8425/10000 (84%)\ttrain_Loss: 0.004761\tval_Loss: 0.005319\n",
      "Train Epoch: 8426/10000 (84%)\ttrain_Loss: 0.004760\tval_Loss: 0.005291\n",
      "Train Epoch: 8427/10000 (84%)\ttrain_Loss: 0.004759\tval_Loss: 0.005306\n",
      "Train Epoch: 8428/10000 (84%)\ttrain_Loss: 0.004758\tval_Loss: 0.005301\n",
      "Train Epoch: 8429/10000 (84%)\ttrain_Loss: 0.004758\tval_Loss: 0.005295\n",
      "Train Epoch: 8430/10000 (84%)\ttrain_Loss: 0.004758\tval_Loss: 0.005310\n",
      "Train Epoch: 8431/10000 (84%)\ttrain_Loss: 0.004758\tval_Loss: 0.005288\n",
      "Train Epoch: 8432/10000 (84%)\ttrain_Loss: 0.004758\tval_Loss: 0.005317\n",
      "Train Epoch: 8433/10000 (84%)\ttrain_Loss: 0.004759\tval_Loss: 0.005284\n",
      "Train Epoch: 8434/10000 (84%)\ttrain_Loss: 0.004759\tval_Loss: 0.005322\n",
      "Train Epoch: 8435/10000 (84%)\ttrain_Loss: 0.004759\tval_Loss: 0.005281\n",
      "Train Epoch: 8436/10000 (84%)\ttrain_Loss: 0.004759\tval_Loss: 0.005326\n",
      "Train Epoch: 8437/10000 (84%)\ttrain_Loss: 0.004760\tval_Loss: 0.005279\n",
      "Train Epoch: 8438/10000 (84%)\ttrain_Loss: 0.004760\tval_Loss: 0.005326\n",
      "Train Epoch: 8439/10000 (84%)\ttrain_Loss: 0.004760\tval_Loss: 0.005278\n",
      "Train Epoch: 8440/10000 (84%)\ttrain_Loss: 0.004760\tval_Loss: 0.005326\n",
      "Train Epoch: 8441/10000 (84%)\ttrain_Loss: 0.004759\tval_Loss: 0.005278\n",
      "Train Epoch: 8442/10000 (84%)\ttrain_Loss: 0.004759\tval_Loss: 0.005325\n",
      "Train Epoch: 8443/10000 (84%)\ttrain_Loss: 0.004759\tval_Loss: 0.005278\n",
      "Train Epoch: 8444/10000 (84%)\ttrain_Loss: 0.004759\tval_Loss: 0.005325\n",
      "Train Epoch: 8445/10000 (84%)\ttrain_Loss: 0.004759\tval_Loss: 0.005278\n",
      "Train Epoch: 8446/10000 (84%)\ttrain_Loss: 0.004758\tval_Loss: 0.005324\n",
      "Train Epoch: 8447/10000 (84%)\ttrain_Loss: 0.004758\tval_Loss: 0.005276\n",
      "Train Epoch: 8448/10000 (84%)\ttrain_Loss: 0.004758\tval_Loss: 0.005324\n",
      "Train Epoch: 8449/10000 (84%)\ttrain_Loss: 0.004758\tval_Loss: 0.005274\n",
      "Train Epoch: 8450/10000 (84%)\ttrain_Loss: 0.004758\tval_Loss: 0.005328\n",
      "Train Epoch: 8451/10000 (84%)\ttrain_Loss: 0.004759\tval_Loss: 0.005272\n",
      "Train Epoch: 8452/10000 (85%)\ttrain_Loss: 0.004759\tval_Loss: 0.005334\n",
      "Train Epoch: 8453/10000 (85%)\ttrain_Loss: 0.004760\tval_Loss: 0.005270\n",
      "Train Epoch: 8454/10000 (85%)\ttrain_Loss: 0.004761\tval_Loss: 0.005343\n",
      "Train Epoch: 8455/10000 (85%)\ttrain_Loss: 0.004762\tval_Loss: 0.005265\n",
      "Train Epoch: 8456/10000 (85%)\ttrain_Loss: 0.004765\tval_Loss: 0.005358\n",
      "Train Epoch: 8457/10000 (85%)\ttrain_Loss: 0.004768\tval_Loss: 0.005259\n",
      "Train Epoch: 8458/10000 (85%)\ttrain_Loss: 0.004772\tval_Loss: 0.005382\n",
      "Train Epoch: 8459/10000 (85%)\ttrain_Loss: 0.004777\tval_Loss: 0.005255\n",
      "Train Epoch: 8460/10000 (85%)\ttrain_Loss: 0.004783\tval_Loss: 0.005412\n",
      "Train Epoch: 8461/10000 (85%)\ttrain_Loss: 0.004790\tval_Loss: 0.005255\n",
      "Train Epoch: 8462/10000 (85%)\ttrain_Loss: 0.004797\tval_Loss: 0.005437\n",
      "Train Epoch: 8463/10000 (85%)\ttrain_Loss: 0.004803\tval_Loss: 0.005255\n",
      "Train Epoch: 8464/10000 (85%)\ttrain_Loss: 0.004805\tval_Loss: 0.005438\n",
      "Train Epoch: 8465/10000 (85%)\ttrain_Loss: 0.004804\tval_Loss: 0.005253\n",
      "Train Epoch: 8466/10000 (85%)\ttrain_Loss: 0.004796\tval_Loss: 0.005399\n",
      "Train Epoch: 8467/10000 (85%)\ttrain_Loss: 0.004784\tval_Loss: 0.005256\n",
      "Train Epoch: 8468/10000 (85%)\ttrain_Loss: 0.004771\tval_Loss: 0.005336\n",
      "Train Epoch: 8469/10000 (85%)\ttrain_Loss: 0.004759\tval_Loss: 0.005279\n",
      "Train Epoch: 8470/10000 (85%)\ttrain_Loss: 0.004753\tval_Loss: 0.005286\n",
      "Train Epoch: 8471/10000 (85%)\ttrain_Loss: 0.004752\tval_Loss: 0.005321\n",
      "Train Epoch: 8472/10000 (85%)\ttrain_Loss: 0.004755\tval_Loss: 0.005263\n",
      "Train Epoch: 8473/10000 (85%)\ttrain_Loss: 0.004760\tval_Loss: 0.005357\n",
      "Train Epoch: 8474/10000 (85%)\ttrain_Loss: 0.004766\tval_Loss: 0.005256\n",
      "Train Epoch: 8475/10000 (85%)\ttrain_Loss: 0.004769\tval_Loss: 0.005365\n",
      "Train Epoch: 8476/10000 (85%)\ttrain_Loss: 0.004769\tval_Loss: 0.005256\n",
      "Train Epoch: 8477/10000 (85%)\ttrain_Loss: 0.004767\tval_Loss: 0.005346\n",
      "Train Epoch: 8478/10000 (85%)\ttrain_Loss: 0.004762\tval_Loss: 0.005265\n",
      "Train Epoch: 8479/10000 (85%)\ttrain_Loss: 0.004757\tval_Loss: 0.005313\n",
      "Train Epoch: 8480/10000 (85%)\ttrain_Loss: 0.004753\tval_Loss: 0.005285\n",
      "Train Epoch: 8481/10000 (85%)\ttrain_Loss: 0.004750\tval_Loss: 0.005286\n",
      "Train Epoch: 8482/10000 (85%)\ttrain_Loss: 0.004750\tval_Loss: 0.005310\n",
      "Train Epoch: 8483/10000 (85%)\ttrain_Loss: 0.004752\tval_Loss: 0.005270\n",
      "Train Epoch: 8484/10000 (85%)\ttrain_Loss: 0.004754\tval_Loss: 0.005327\n",
      "Train Epoch: 8485/10000 (85%)\ttrain_Loss: 0.004755\tval_Loss: 0.005263\n",
      "Train Epoch: 8486/10000 (85%)\ttrain_Loss: 0.004756\tval_Loss: 0.005331\n",
      "Train Epoch: 8487/10000 (85%)\ttrain_Loss: 0.004756\tval_Loss: 0.005264\n",
      "Train Epoch: 8488/10000 (85%)\ttrain_Loss: 0.004755\tval_Loss: 0.005322\n",
      "Train Epoch: 8489/10000 (85%)\ttrain_Loss: 0.004754\tval_Loss: 0.005270\n",
      "Train Epoch: 8490/10000 (85%)\ttrain_Loss: 0.004752\tval_Loss: 0.005307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8491/10000 (85%)\ttrain_Loss: 0.004750\tval_Loss: 0.005279\n",
      "Train Epoch: 8492/10000 (85%)\ttrain_Loss: 0.004749\tval_Loss: 0.005293\n",
      "Train Epoch: 8493/10000 (85%)\ttrain_Loss: 0.004749\tval_Loss: 0.005290\n",
      "Train Epoch: 8494/10000 (85%)\ttrain_Loss: 0.004748\tval_Loss: 0.005282\n",
      "Train Epoch: 8495/10000 (85%)\ttrain_Loss: 0.004748\tval_Loss: 0.005299\n",
      "Train Epoch: 8496/10000 (85%)\ttrain_Loss: 0.004749\tval_Loss: 0.005275\n",
      "Train Epoch: 8497/10000 (85%)\ttrain_Loss: 0.004749\tval_Loss: 0.005306\n",
      "Train Epoch: 8498/10000 (85%)\ttrain_Loss: 0.004749\tval_Loss: 0.005272\n",
      "Train Epoch: 8499/10000 (85%)\ttrain_Loss: 0.004750\tval_Loss: 0.005309\n",
      "Train Epoch: 8500/10000 (85%)\ttrain_Loss: 0.004750\tval_Loss: 0.005270\n",
      "Train Epoch: 8501/10000 (85%)\ttrain_Loss: 0.004750\tval_Loss: 0.005311\n",
      "Train Epoch: 8502/10000 (85%)\ttrain_Loss: 0.004750\tval_Loss: 0.005269\n",
      "Train Epoch: 8503/10000 (85%)\ttrain_Loss: 0.004750\tval_Loss: 0.005309\n",
      "Train Epoch: 8504/10000 (85%)\ttrain_Loss: 0.004749\tval_Loss: 0.005268\n",
      "Train Epoch: 8505/10000 (85%)\ttrain_Loss: 0.004749\tval_Loss: 0.005307\n",
      "Train Epoch: 8506/10000 (85%)\ttrain_Loss: 0.004749\tval_Loss: 0.005270\n",
      "Train Epoch: 8507/10000 (85%)\ttrain_Loss: 0.004748\tval_Loss: 0.005304\n",
      "Train Epoch: 8508/10000 (85%)\ttrain_Loss: 0.004748\tval_Loss: 0.005272\n",
      "Train Epoch: 8509/10000 (85%)\ttrain_Loss: 0.004747\tval_Loss: 0.005301\n",
      "Train Epoch: 8510/10000 (85%)\ttrain_Loss: 0.004747\tval_Loss: 0.005274\n",
      "Train Epoch: 8511/10000 (85%)\ttrain_Loss: 0.004747\tval_Loss: 0.005298\n",
      "Train Epoch: 8512/10000 (85%)\ttrain_Loss: 0.004746\tval_Loss: 0.005274\n",
      "Train Epoch: 8513/10000 (85%)\ttrain_Loss: 0.004746\tval_Loss: 0.005295\n",
      "Train Epoch: 8514/10000 (85%)\ttrain_Loss: 0.004746\tval_Loss: 0.005274\n",
      "Train Epoch: 8515/10000 (85%)\ttrain_Loss: 0.004746\tval_Loss: 0.005295\n",
      "Train Epoch: 8516/10000 (85%)\ttrain_Loss: 0.004746\tval_Loss: 0.005274\n",
      "Train Epoch: 8517/10000 (85%)\ttrain_Loss: 0.004746\tval_Loss: 0.005298\n",
      "Train Epoch: 8518/10000 (85%)\ttrain_Loss: 0.004746\tval_Loss: 0.005273\n",
      "Train Epoch: 8519/10000 (85%)\ttrain_Loss: 0.004746\tval_Loss: 0.005301\n",
      "Train Epoch: 8520/10000 (85%)\ttrain_Loss: 0.004746\tval_Loss: 0.005268\n",
      "Train Epoch: 8521/10000 (85%)\ttrain_Loss: 0.004746\tval_Loss: 0.005305\n",
      "Train Epoch: 8522/10000 (85%)\ttrain_Loss: 0.004747\tval_Loss: 0.005263\n",
      "Train Epoch: 8523/10000 (85%)\ttrain_Loss: 0.004747\tval_Loss: 0.005314\n",
      "Train Epoch: 8524/10000 (85%)\ttrain_Loss: 0.004748\tval_Loss: 0.005256\n",
      "Train Epoch: 8525/10000 (85%)\ttrain_Loss: 0.004750\tval_Loss: 0.005331\n",
      "Train Epoch: 8526/10000 (85%)\ttrain_Loss: 0.004753\tval_Loss: 0.005249\n",
      "Train Epoch: 8527/10000 (85%)\ttrain_Loss: 0.004757\tval_Loss: 0.005360\n",
      "Train Epoch: 8528/10000 (85%)\ttrain_Loss: 0.004763\tval_Loss: 0.005243\n",
      "Train Epoch: 8529/10000 (85%)\ttrain_Loss: 0.004772\tval_Loss: 0.005406\n",
      "Train Epoch: 8530/10000 (85%)\ttrain_Loss: 0.004784\tval_Loss: 0.005243\n",
      "Train Epoch: 8531/10000 (85%)\ttrain_Loss: 0.004800\tval_Loss: 0.005467\n",
      "Train Epoch: 8532/10000 (85%)\ttrain_Loss: 0.004817\tval_Loss: 0.005251\n",
      "Train Epoch: 8533/10000 (85%)\ttrain_Loss: 0.004833\tval_Loss: 0.005509\n",
      "Train Epoch: 8534/10000 (85%)\ttrain_Loss: 0.004841\tval_Loss: 0.005252\n",
      "Train Epoch: 8535/10000 (85%)\ttrain_Loss: 0.004836\tval_Loss: 0.005463\n",
      "Train Epoch: 8536/10000 (85%)\ttrain_Loss: 0.004814\tval_Loss: 0.005240\n",
      "Train Epoch: 8537/10000 (85%)\ttrain_Loss: 0.004783\tval_Loss: 0.005343\n",
      "Train Epoch: 8538/10000 (85%)\ttrain_Loss: 0.004756\tval_Loss: 0.005272\n",
      "Train Epoch: 8539/10000 (85%)\ttrain_Loss: 0.004742\tval_Loss: 0.005259\n",
      "Train Epoch: 8540/10000 (85%)\ttrain_Loss: 0.004746\tval_Loss: 0.005352\n",
      "Train Epoch: 8541/10000 (85%)\ttrain_Loss: 0.004759\tval_Loss: 0.005239\n",
      "Train Epoch: 8542/10000 (85%)\ttrain_Loss: 0.004773\tval_Loss: 0.005394\n",
      "Train Epoch: 8543/10000 (85%)\ttrain_Loss: 0.004778\tval_Loss: 0.005238\n",
      "Train Epoch: 8544/10000 (85%)\ttrain_Loss: 0.004773\tval_Loss: 0.005352\n",
      "Train Epoch: 8545/10000 (85%)\ttrain_Loss: 0.004759\tval_Loss: 0.005253\n",
      "Train Epoch: 8546/10000 (85%)\ttrain_Loss: 0.004747\tval_Loss: 0.005283\n",
      "Train Epoch: 8547/10000 (85%)\ttrain_Loss: 0.004741\tval_Loss: 0.005301\n",
      "Train Epoch: 8548/10000 (85%)\ttrain_Loss: 0.004743\tval_Loss: 0.005248\n",
      "Train Epoch: 8549/10000 (85%)\ttrain_Loss: 0.004750\tval_Loss: 0.005344\n",
      "Train Epoch: 8550/10000 (85%)\ttrain_Loss: 0.004756\tval_Loss: 0.005242\n",
      "Train Epoch: 8551/10000 (86%)\ttrain_Loss: 0.004757\tval_Loss: 0.005336\n",
      "Train Epoch: 8552/10000 (86%)\ttrain_Loss: 0.004753\tval_Loss: 0.005250\n",
      "Train Epoch: 8553/10000 (86%)\ttrain_Loss: 0.004747\tval_Loss: 0.005295\n",
      "Train Epoch: 8554/10000 (86%)\ttrain_Loss: 0.004742\tval_Loss: 0.005278\n",
      "Train Epoch: 8555/10000 (86%)\ttrain_Loss: 0.004740\tval_Loss: 0.005262\n",
      "Train Epoch: 8556/10000 (86%)\ttrain_Loss: 0.004741\tval_Loss: 0.005310\n",
      "Train Epoch: 8557/10000 (86%)\ttrain_Loss: 0.004744\tval_Loss: 0.005250\n",
      "Train Epoch: 8558/10000 (86%)\ttrain_Loss: 0.004747\tval_Loss: 0.005321\n",
      "Train Epoch: 8559/10000 (86%)\ttrain_Loss: 0.004747\tval_Loss: 0.005251\n",
      "Train Epoch: 8560/10000 (86%)\ttrain_Loss: 0.004745\tval_Loss: 0.005305\n",
      "Train Epoch: 8561/10000 (86%)\ttrain_Loss: 0.004743\tval_Loss: 0.005264\n",
      "Train Epoch: 8562/10000 (86%)\ttrain_Loss: 0.004740\tval_Loss: 0.005278\n",
      "Train Epoch: 8563/10000 (86%)\ttrain_Loss: 0.004739\tval_Loss: 0.005285\n",
      "Train Epoch: 8564/10000 (86%)\ttrain_Loss: 0.004739\tval_Loss: 0.005261\n",
      "Train Epoch: 8565/10000 (86%)\ttrain_Loss: 0.004740\tval_Loss: 0.005301\n",
      "Train Epoch: 8566/10000 (86%)\ttrain_Loss: 0.004741\tval_Loss: 0.005254\n",
      "Train Epoch: 8567/10000 (86%)\ttrain_Loss: 0.004742\tval_Loss: 0.005302\n",
      "Train Epoch: 8568/10000 (86%)\ttrain_Loss: 0.004741\tval_Loss: 0.005257\n",
      "Train Epoch: 8569/10000 (86%)\ttrain_Loss: 0.004740\tval_Loss: 0.005291\n",
      "Train Epoch: 8570/10000 (86%)\ttrain_Loss: 0.004739\tval_Loss: 0.005267\n",
      "Train Epoch: 8571/10000 (86%)\ttrain_Loss: 0.004738\tval_Loss: 0.005275\n",
      "Train Epoch: 8572/10000 (86%)\ttrain_Loss: 0.004737\tval_Loss: 0.005279\n",
      "Train Epoch: 8573/10000 (86%)\ttrain_Loss: 0.004737\tval_Loss: 0.005264\n",
      "Train Epoch: 8574/10000 (86%)\ttrain_Loss: 0.004738\tval_Loss: 0.005289\n",
      "Train Epoch: 8575/10000 (86%)\ttrain_Loss: 0.004738\tval_Loss: 0.005259\n",
      "Train Epoch: 8576/10000 (86%)\ttrain_Loss: 0.004738\tval_Loss: 0.005292\n",
      "Train Epoch: 8577/10000 (86%)\ttrain_Loss: 0.004738\tval_Loss: 0.005259\n",
      "Train Epoch: 8578/10000 (86%)\ttrain_Loss: 0.004738\tval_Loss: 0.005290\n",
      "Train Epoch: 8579/10000 (86%)\ttrain_Loss: 0.004738\tval_Loss: 0.005262\n",
      "Train Epoch: 8580/10000 (86%)\ttrain_Loss: 0.004737\tval_Loss: 0.005283\n",
      "Train Epoch: 8581/10000 (86%)\ttrain_Loss: 0.004737\tval_Loss: 0.005267\n",
      "Train Epoch: 8582/10000 (86%)\ttrain_Loss: 0.004736\tval_Loss: 0.005274\n",
      "Train Epoch: 8583/10000 (86%)\ttrain_Loss: 0.004736\tval_Loss: 0.005273\n",
      "Train Epoch: 8584/10000 (86%)\ttrain_Loss: 0.004736\tval_Loss: 0.005268\n",
      "Train Epoch: 8585/10000 (86%)\ttrain_Loss: 0.004736\tval_Loss: 0.005278\n",
      "Train Epoch: 8586/10000 (86%)\ttrain_Loss: 0.004736\tval_Loss: 0.005264\n",
      "Train Epoch: 8587/10000 (86%)\ttrain_Loss: 0.004736\tval_Loss: 0.005283\n",
      "Train Epoch: 8588/10000 (86%)\ttrain_Loss: 0.004736\tval_Loss: 0.005262\n",
      "Train Epoch: 8589/10000 (86%)\ttrain_Loss: 0.004736\tval_Loss: 0.005284\n",
      "Train Epoch: 8590/10000 (86%)\ttrain_Loss: 0.004736\tval_Loss: 0.005260\n",
      "Train Epoch: 8591/10000 (86%)\ttrain_Loss: 0.004735\tval_Loss: 0.005284\n",
      "Train Epoch: 8592/10000 (86%)\ttrain_Loss: 0.004735\tval_Loss: 0.005258\n",
      "Train Epoch: 8593/10000 (86%)\ttrain_Loss: 0.004735\tval_Loss: 0.005284\n",
      "Train Epoch: 8594/10000 (86%)\ttrain_Loss: 0.004735\tval_Loss: 0.005259\n",
      "Train Epoch: 8595/10000 (86%)\ttrain_Loss: 0.004735\tval_Loss: 0.005282\n",
      "Train Epoch: 8596/10000 (86%)\ttrain_Loss: 0.004735\tval_Loss: 0.005260\n",
      "Train Epoch: 8597/10000 (86%)\ttrain_Loss: 0.004734\tval_Loss: 0.005280\n",
      "Train Epoch: 8598/10000 (86%)\ttrain_Loss: 0.004734\tval_Loss: 0.005262\n",
      "Train Epoch: 8599/10000 (86%)\ttrain_Loss: 0.004734\tval_Loss: 0.005276\n",
      "Train Epoch: 8600/10000 (86%)\ttrain_Loss: 0.004734\tval_Loss: 0.005264\n",
      "Train Epoch: 8601/10000 (86%)\ttrain_Loss: 0.004733\tval_Loss: 0.005273\n",
      "Train Epoch: 8602/10000 (86%)\ttrain_Loss: 0.004733\tval_Loss: 0.005265\n",
      "Train Epoch: 8603/10000 (86%)\ttrain_Loss: 0.004733\tval_Loss: 0.005271\n",
      "Train Epoch: 8604/10000 (86%)\ttrain_Loss: 0.004733\tval_Loss: 0.005265\n",
      "Train Epoch: 8605/10000 (86%)\ttrain_Loss: 0.004733\tval_Loss: 0.005271\n",
      "Train Epoch: 8606/10000 (86%)\ttrain_Loss: 0.004733\tval_Loss: 0.005265\n",
      "Train Epoch: 8607/10000 (86%)\ttrain_Loss: 0.004732\tval_Loss: 0.005271\n",
      "Train Epoch: 8608/10000 (86%)\ttrain_Loss: 0.004732\tval_Loss: 0.005264\n",
      "Train Epoch: 8609/10000 (86%)\ttrain_Loss: 0.004732\tval_Loss: 0.005272\n",
      "Train Epoch: 8610/10000 (86%)\ttrain_Loss: 0.004732\tval_Loss: 0.005263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8611/10000 (86%)\ttrain_Loss: 0.004732\tval_Loss: 0.005272\n",
      "Train Epoch: 8612/10000 (86%)\ttrain_Loss: 0.004732\tval_Loss: 0.005261\n",
      "Train Epoch: 8613/10000 (86%)\ttrain_Loss: 0.004732\tval_Loss: 0.005274\n",
      "Train Epoch: 8614/10000 (86%)\ttrain_Loss: 0.004732\tval_Loss: 0.005257\n",
      "Train Epoch: 8615/10000 (86%)\ttrain_Loss: 0.004732\tval_Loss: 0.005281\n",
      "Train Epoch: 8616/10000 (86%)\ttrain_Loss: 0.004732\tval_Loss: 0.005252\n",
      "Train Epoch: 8617/10000 (86%)\ttrain_Loss: 0.004732\tval_Loss: 0.005290\n",
      "Train Epoch: 8618/10000 (86%)\ttrain_Loss: 0.004733\tval_Loss: 0.005244\n",
      "Train Epoch: 8619/10000 (86%)\ttrain_Loss: 0.004735\tval_Loss: 0.005306\n",
      "Train Epoch: 8620/10000 (86%)\ttrain_Loss: 0.004737\tval_Loss: 0.005235\n",
      "Train Epoch: 8621/10000 (86%)\ttrain_Loss: 0.004741\tval_Loss: 0.005333\n",
      "Train Epoch: 8622/10000 (86%)\ttrain_Loss: 0.004746\tval_Loss: 0.005227\n",
      "Train Epoch: 8623/10000 (86%)\ttrain_Loss: 0.004754\tval_Loss: 0.005377\n",
      "Train Epoch: 8624/10000 (86%)\ttrain_Loss: 0.004765\tval_Loss: 0.005224\n",
      "Train Epoch: 8625/10000 (86%)\ttrain_Loss: 0.004781\tval_Loss: 0.005445\n",
      "Train Epoch: 8626/10000 (86%)\ttrain_Loss: 0.004800\tval_Loss: 0.005235\n",
      "Train Epoch: 8627/10000 (86%)\ttrain_Loss: 0.004823\tval_Loss: 0.005513\n",
      "Train Epoch: 8628/10000 (86%)\ttrain_Loss: 0.004841\tval_Loss: 0.005244\n",
      "Train Epoch: 8629/10000 (86%)\ttrain_Loss: 0.004848\tval_Loss: 0.005501\n",
      "Train Epoch: 8630/10000 (86%)\ttrain_Loss: 0.004834\tval_Loss: 0.005227\n",
      "Train Epoch: 8631/10000 (86%)\ttrain_Loss: 0.004801\tval_Loss: 0.005370\n",
      "Train Epoch: 8632/10000 (86%)\ttrain_Loss: 0.004761\tval_Loss: 0.005239\n",
      "Train Epoch: 8633/10000 (86%)\ttrain_Loss: 0.004734\tval_Loss: 0.005251\n",
      "Train Epoch: 8634/10000 (86%)\ttrain_Loss: 0.004730\tval_Loss: 0.005330\n",
      "Train Epoch: 8635/10000 (86%)\ttrain_Loss: 0.004744\tval_Loss: 0.005221\n",
      "Train Epoch: 8636/10000 (86%)\ttrain_Loss: 0.004764\tval_Loss: 0.005396\n",
      "Train Epoch: 8637/10000 (86%)\ttrain_Loss: 0.004774\tval_Loss: 0.005220\n",
      "Train Epoch: 8638/10000 (86%)\ttrain_Loss: 0.004769\tval_Loss: 0.005349\n",
      "Train Epoch: 8639/10000 (86%)\ttrain_Loss: 0.004752\tval_Loss: 0.005234\n",
      "Train Epoch: 8640/10000 (86%)\ttrain_Loss: 0.004735\tval_Loss: 0.005263\n",
      "Train Epoch: 8641/10000 (86%)\ttrain_Loss: 0.004728\tval_Loss: 0.005293\n",
      "Train Epoch: 8642/10000 (86%)\ttrain_Loss: 0.004732\tval_Loss: 0.005226\n",
      "Train Epoch: 8643/10000 (86%)\ttrain_Loss: 0.004742\tval_Loss: 0.005342\n",
      "Train Epoch: 8644/10000 (86%)\ttrain_Loss: 0.004749\tval_Loss: 0.005223\n",
      "Train Epoch: 8645/10000 (86%)\ttrain_Loss: 0.004748\tval_Loss: 0.005319\n",
      "Train Epoch: 8646/10000 (86%)\ttrain_Loss: 0.004740\tval_Loss: 0.005238\n",
      "Train Epoch: 8647/10000 (86%)\ttrain_Loss: 0.004731\tval_Loss: 0.005265\n",
      "Train Epoch: 8648/10000 (86%)\ttrain_Loss: 0.004727\tval_Loss: 0.005278\n",
      "Train Epoch: 8649/10000 (86%)\ttrain_Loss: 0.004728\tval_Loss: 0.005234\n",
      "Train Epoch: 8650/10000 (86%)\ttrain_Loss: 0.004733\tval_Loss: 0.005311\n",
      "Train Epoch: 8651/10000 (86%)\ttrain_Loss: 0.004737\tval_Loss: 0.005229\n",
      "Train Epoch: 8652/10000 (87%)\ttrain_Loss: 0.004737\tval_Loss: 0.005302\n",
      "Train Epoch: 8653/10000 (87%)\ttrain_Loss: 0.004734\tval_Loss: 0.005239\n",
      "Train Epoch: 8654/10000 (87%)\ttrain_Loss: 0.004729\tval_Loss: 0.005269\n",
      "Train Epoch: 8655/10000 (87%)\ttrain_Loss: 0.004726\tval_Loss: 0.005265\n",
      "Train Epoch: 8656/10000 (87%)\ttrain_Loss: 0.004726\tval_Loss: 0.005243\n",
      "Train Epoch: 8657/10000 (87%)\ttrain_Loss: 0.004728\tval_Loss: 0.005289\n",
      "Train Epoch: 8658/10000 (87%)\ttrain_Loss: 0.004730\tval_Loss: 0.005235\n",
      "Train Epoch: 8659/10000 (87%)\ttrain_Loss: 0.004731\tval_Loss: 0.005290\n",
      "Train Epoch: 8660/10000 (87%)\ttrain_Loss: 0.004730\tval_Loss: 0.005240\n",
      "Train Epoch: 8661/10000 (87%)\ttrain_Loss: 0.004728\tval_Loss: 0.005271\n",
      "Train Epoch: 8662/10000 (87%)\ttrain_Loss: 0.004726\tval_Loss: 0.005255\n",
      "Train Epoch: 8663/10000 (87%)\ttrain_Loss: 0.004725\tval_Loss: 0.005250\n",
      "Train Epoch: 8664/10000 (87%)\ttrain_Loss: 0.004725\tval_Loss: 0.005272\n",
      "Train Epoch: 8665/10000 (87%)\ttrain_Loss: 0.004726\tval_Loss: 0.005240\n",
      "Train Epoch: 8666/10000 (87%)\ttrain_Loss: 0.004727\tval_Loss: 0.005281\n",
      "Train Epoch: 8667/10000 (87%)\ttrain_Loss: 0.004727\tval_Loss: 0.005240\n",
      "Train Epoch: 8668/10000 (87%)\ttrain_Loss: 0.004727\tval_Loss: 0.005276\n",
      "Train Epoch: 8669/10000 (87%)\ttrain_Loss: 0.004726\tval_Loss: 0.005247\n",
      "Train Epoch: 8670/10000 (87%)\ttrain_Loss: 0.004725\tval_Loss: 0.005262\n",
      "Train Epoch: 8671/10000 (87%)\ttrain_Loss: 0.004724\tval_Loss: 0.005257\n",
      "Train Epoch: 8672/10000 (87%)\ttrain_Loss: 0.004724\tval_Loss: 0.005249\n",
      "Train Epoch: 8673/10000 (87%)\ttrain_Loss: 0.004724\tval_Loss: 0.005266\n",
      "Train Epoch: 8674/10000 (87%)\ttrain_Loss: 0.004724\tval_Loss: 0.005242\n",
      "Train Epoch: 8675/10000 (87%)\ttrain_Loss: 0.004724\tval_Loss: 0.005270\n",
      "Train Epoch: 8676/10000 (87%)\ttrain_Loss: 0.004724\tval_Loss: 0.005243\n",
      "Train Epoch: 8677/10000 (87%)\ttrain_Loss: 0.004724\tval_Loss: 0.005267\n",
      "Train Epoch: 8678/10000 (87%)\ttrain_Loss: 0.004724\tval_Loss: 0.005248\n",
      "Train Epoch: 8679/10000 (87%)\ttrain_Loss: 0.004723\tval_Loss: 0.005260\n",
      "Train Epoch: 8680/10000 (87%)\ttrain_Loss: 0.004723\tval_Loss: 0.005253\n",
      "Train Epoch: 8681/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005252\n",
      "Train Epoch: 8682/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005258\n",
      "Train Epoch: 8683/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005246\n",
      "Train Epoch: 8684/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005263\n",
      "Train Epoch: 8685/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005245\n",
      "Train Epoch: 8686/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005264\n",
      "Train Epoch: 8687/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005246\n",
      "Train Epoch: 8688/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005262\n",
      "Train Epoch: 8689/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005246\n",
      "Train Epoch: 8690/10000 (87%)\ttrain_Loss: 0.004721\tval_Loss: 0.005257\n",
      "Train Epoch: 8691/10000 (87%)\ttrain_Loss: 0.004721\tval_Loss: 0.005248\n",
      "Train Epoch: 8692/10000 (87%)\ttrain_Loss: 0.004721\tval_Loss: 0.005253\n",
      "Train Epoch: 8693/10000 (87%)\ttrain_Loss: 0.004721\tval_Loss: 0.005251\n",
      "Train Epoch: 8694/10000 (87%)\ttrain_Loss: 0.004721\tval_Loss: 0.005251\n",
      "Train Epoch: 8695/10000 (87%)\ttrain_Loss: 0.004720\tval_Loss: 0.005253\n",
      "Train Epoch: 8696/10000 (87%)\ttrain_Loss: 0.004720\tval_Loss: 0.005249\n",
      "Train Epoch: 8697/10000 (87%)\ttrain_Loss: 0.004720\tval_Loss: 0.005255\n",
      "Train Epoch: 8698/10000 (87%)\ttrain_Loss: 0.004720\tval_Loss: 0.005247\n",
      "Train Epoch: 8699/10000 (87%)\ttrain_Loss: 0.004720\tval_Loss: 0.005258\n",
      "Train Epoch: 8700/10000 (87%)\ttrain_Loss: 0.004720\tval_Loss: 0.005244\n",
      "Train Epoch: 8701/10000 (87%)\ttrain_Loss: 0.004720\tval_Loss: 0.005262\n",
      "Train Epoch: 8702/10000 (87%)\ttrain_Loss: 0.004720\tval_Loss: 0.005240\n",
      "Train Epoch: 8703/10000 (87%)\ttrain_Loss: 0.004720\tval_Loss: 0.005265\n",
      "Train Epoch: 8704/10000 (87%)\ttrain_Loss: 0.004720\tval_Loss: 0.005236\n",
      "Train Epoch: 8705/10000 (87%)\ttrain_Loss: 0.004720\tval_Loss: 0.005267\n",
      "Train Epoch: 8706/10000 (87%)\ttrain_Loss: 0.004721\tval_Loss: 0.005233\n",
      "Train Epoch: 8707/10000 (87%)\ttrain_Loss: 0.004721\tval_Loss: 0.005271\n",
      "Train Epoch: 8708/10000 (87%)\ttrain_Loss: 0.004721\tval_Loss: 0.005231\n",
      "Train Epoch: 8709/10000 (87%)\ttrain_Loss: 0.004721\tval_Loss: 0.005273\n",
      "Train Epoch: 8710/10000 (87%)\ttrain_Loss: 0.004721\tval_Loss: 0.005229\n",
      "Train Epoch: 8711/10000 (87%)\ttrain_Loss: 0.004721\tval_Loss: 0.005276\n",
      "Train Epoch: 8712/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005226\n",
      "Train Epoch: 8713/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005279\n",
      "Train Epoch: 8714/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005224\n",
      "Train Epoch: 8715/10000 (87%)\ttrain_Loss: 0.004723\tval_Loss: 0.005285\n",
      "Train Epoch: 8716/10000 (87%)\ttrain_Loss: 0.004723\tval_Loss: 0.005221\n",
      "Train Epoch: 8717/10000 (87%)\ttrain_Loss: 0.004724\tval_Loss: 0.005292\n",
      "Train Epoch: 8718/10000 (87%)\ttrain_Loss: 0.004725\tval_Loss: 0.005217\n",
      "Train Epoch: 8719/10000 (87%)\ttrain_Loss: 0.004727\tval_Loss: 0.005304\n",
      "Train Epoch: 8720/10000 (87%)\ttrain_Loss: 0.004729\tval_Loss: 0.005213\n",
      "Train Epoch: 8721/10000 (87%)\ttrain_Loss: 0.004731\tval_Loss: 0.005320\n",
      "Train Epoch: 8722/10000 (87%)\ttrain_Loss: 0.004735\tval_Loss: 0.005209\n",
      "Train Epoch: 8723/10000 (87%)\ttrain_Loss: 0.004739\tval_Loss: 0.005343\n",
      "Train Epoch: 8724/10000 (87%)\ttrain_Loss: 0.004744\tval_Loss: 0.005205\n",
      "Train Epoch: 8725/10000 (87%)\ttrain_Loss: 0.004750\tval_Loss: 0.005367\n",
      "Train Epoch: 8726/10000 (87%)\ttrain_Loss: 0.004756\tval_Loss: 0.005204\n",
      "Train Epoch: 8727/10000 (87%)\ttrain_Loss: 0.004761\tval_Loss: 0.005383\n",
      "Train Epoch: 8728/10000 (87%)\ttrain_Loss: 0.004764\tval_Loss: 0.005204\n",
      "Train Epoch: 8729/10000 (87%)\ttrain_Loss: 0.004764\tval_Loss: 0.005377\n",
      "Train Epoch: 8730/10000 (87%)\ttrain_Loss: 0.004761\tval_Loss: 0.005204\n",
      "Train Epoch: 8731/10000 (87%)\ttrain_Loss: 0.004753\tval_Loss: 0.005338\n",
      "Train Epoch: 8732/10000 (87%)\ttrain_Loss: 0.004742\tval_Loss: 0.005210\n",
      "Train Epoch: 8733/10000 (87%)\ttrain_Loss: 0.004731\tval_Loss: 0.005283\n",
      "Train Epoch: 8734/10000 (87%)\ttrain_Loss: 0.004721\tval_Loss: 0.005232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8735/10000 (87%)\ttrain_Loss: 0.004716\tval_Loss: 0.005239\n",
      "Train Epoch: 8736/10000 (87%)\ttrain_Loss: 0.004715\tval_Loss: 0.005267\n",
      "Train Epoch: 8737/10000 (87%)\ttrain_Loss: 0.004717\tval_Loss: 0.005217\n",
      "Train Epoch: 8738/10000 (87%)\ttrain_Loss: 0.004722\tval_Loss: 0.005298\n",
      "Train Epoch: 8739/10000 (87%)\ttrain_Loss: 0.004726\tval_Loss: 0.005209\n",
      "Train Epoch: 8740/10000 (87%)\ttrain_Loss: 0.004729\tval_Loss: 0.005310\n",
      "Train Epoch: 8741/10000 (87%)\ttrain_Loss: 0.004730\tval_Loss: 0.005208\n",
      "Train Epoch: 8742/10000 (87%)\ttrain_Loss: 0.004729\tval_Loss: 0.005301\n",
      "Train Epoch: 8743/10000 (87%)\ttrain_Loss: 0.004727\tval_Loss: 0.005213\n",
      "Train Epoch: 8744/10000 (87%)\ttrain_Loss: 0.004723\tval_Loss: 0.005278\n",
      "Train Epoch: 8745/10000 (87%)\ttrain_Loss: 0.004719\tval_Loss: 0.005225\n",
      "Train Epoch: 8746/10000 (87%)\ttrain_Loss: 0.004716\tval_Loss: 0.005254\n",
      "Train Epoch: 8747/10000 (87%)\ttrain_Loss: 0.004714\tval_Loss: 0.005242\n",
      "Train Epoch: 8748/10000 (87%)\ttrain_Loss: 0.004713\tval_Loss: 0.005234\n",
      "Train Epoch: 8749/10000 (87%)\ttrain_Loss: 0.004714\tval_Loss: 0.005259\n",
      "Train Epoch: 8750/10000 (87%)\ttrain_Loss: 0.004715\tval_Loss: 0.005222\n",
      "Train Epoch: 8751/10000 (88%)\ttrain_Loss: 0.004716\tval_Loss: 0.005271\n",
      "Train Epoch: 8752/10000 (88%)\ttrain_Loss: 0.004717\tval_Loss: 0.005217\n",
      "Train Epoch: 8753/10000 (88%)\ttrain_Loss: 0.004718\tval_Loss: 0.005276\n",
      "Train Epoch: 8754/10000 (88%)\ttrain_Loss: 0.004718\tval_Loss: 0.005216\n",
      "Train Epoch: 8755/10000 (88%)\ttrain_Loss: 0.004718\tval_Loss: 0.005276\n",
      "Train Epoch: 8756/10000 (88%)\ttrain_Loss: 0.004718\tval_Loss: 0.005218\n",
      "Train Epoch: 8757/10000 (88%)\ttrain_Loss: 0.004717\tval_Loss: 0.005269\n",
      "Train Epoch: 8758/10000 (88%)\ttrain_Loss: 0.004716\tval_Loss: 0.005221\n",
      "Train Epoch: 8759/10000 (88%)\ttrain_Loss: 0.004715\tval_Loss: 0.005258\n",
      "Train Epoch: 8760/10000 (88%)\ttrain_Loss: 0.004714\tval_Loss: 0.005225\n",
      "Train Epoch: 8761/10000 (88%)\ttrain_Loss: 0.004713\tval_Loss: 0.005251\n",
      "Train Epoch: 8762/10000 (88%)\ttrain_Loss: 0.004712\tval_Loss: 0.005232\n",
      "Train Epoch: 8763/10000 (88%)\ttrain_Loss: 0.004712\tval_Loss: 0.005245\n",
      "Train Epoch: 8764/10000 (88%)\ttrain_Loss: 0.004711\tval_Loss: 0.005237\n",
      "Train Epoch: 8765/10000 (88%)\ttrain_Loss: 0.004711\tval_Loss: 0.005238\n",
      "Train Epoch: 8766/10000 (88%)\ttrain_Loss: 0.004711\tval_Loss: 0.005241\n",
      "Train Epoch: 8767/10000 (88%)\ttrain_Loss: 0.004711\tval_Loss: 0.005234\n",
      "Train Epoch: 8768/10000 (88%)\ttrain_Loss: 0.004711\tval_Loss: 0.005245\n",
      "Train Epoch: 8769/10000 (88%)\ttrain_Loss: 0.004711\tval_Loss: 0.005230\n",
      "Train Epoch: 8770/10000 (88%)\ttrain_Loss: 0.004711\tval_Loss: 0.005250\n",
      "Train Epoch: 8771/10000 (88%)\ttrain_Loss: 0.004711\tval_Loss: 0.005225\n",
      "Train Epoch: 8772/10000 (88%)\ttrain_Loss: 0.004711\tval_Loss: 0.005254\n",
      "Train Epoch: 8773/10000 (88%)\ttrain_Loss: 0.004712\tval_Loss: 0.005220\n",
      "Train Epoch: 8774/10000 (88%)\ttrain_Loss: 0.004712\tval_Loss: 0.005259\n",
      "Train Epoch: 8775/10000 (88%)\ttrain_Loss: 0.004712\tval_Loss: 0.005216\n",
      "Train Epoch: 8776/10000 (88%)\ttrain_Loss: 0.004713\tval_Loss: 0.005269\n",
      "Train Epoch: 8777/10000 (88%)\ttrain_Loss: 0.004714\tval_Loss: 0.005212\n",
      "Train Epoch: 8778/10000 (88%)\ttrain_Loss: 0.004715\tval_Loss: 0.005282\n",
      "Train Epoch: 8779/10000 (88%)\ttrain_Loss: 0.004717\tval_Loss: 0.005206\n",
      "Train Epoch: 8780/10000 (88%)\ttrain_Loss: 0.004720\tval_Loss: 0.005298\n",
      "Train Epoch: 8781/10000 (88%)\ttrain_Loss: 0.004723\tval_Loss: 0.005199\n",
      "Train Epoch: 8782/10000 (88%)\ttrain_Loss: 0.004727\tval_Loss: 0.005322\n",
      "Train Epoch: 8783/10000 (88%)\ttrain_Loss: 0.004732\tval_Loss: 0.005195\n",
      "Train Epoch: 8784/10000 (88%)\ttrain_Loss: 0.004739\tval_Loss: 0.005354\n",
      "Train Epoch: 8785/10000 (88%)\ttrain_Loss: 0.004747\tval_Loss: 0.005195\n",
      "Train Epoch: 8786/10000 (88%)\ttrain_Loss: 0.004756\tval_Loss: 0.005387\n",
      "Train Epoch: 8787/10000 (88%)\ttrain_Loss: 0.004764\tval_Loss: 0.005196\n",
      "Train Epoch: 8788/10000 (88%)\ttrain_Loss: 0.004770\tval_Loss: 0.005400\n",
      "Train Epoch: 8789/10000 (88%)\ttrain_Loss: 0.004771\tval_Loss: 0.005195\n",
      "Train Epoch: 8790/10000 (88%)\ttrain_Loss: 0.004766\tval_Loss: 0.005370\n",
      "Train Epoch: 8791/10000 (88%)\ttrain_Loss: 0.004755\tval_Loss: 0.005193\n",
      "Train Epoch: 8792/10000 (88%)\ttrain_Loss: 0.004740\tval_Loss: 0.005302\n",
      "Train Epoch: 8793/10000 (88%)\ttrain_Loss: 0.004724\tval_Loss: 0.005210\n",
      "Train Epoch: 8794/10000 (88%)\ttrain_Loss: 0.004712\tval_Loss: 0.005239\n",
      "Train Epoch: 8795/10000 (88%)\ttrain_Loss: 0.004708\tval_Loss: 0.005251\n",
      "Train Epoch: 8796/10000 (88%)\ttrain_Loss: 0.004709\tval_Loss: 0.005207\n",
      "Train Epoch: 8797/10000 (88%)\ttrain_Loss: 0.004714\tval_Loss: 0.005294\n",
      "Train Epoch: 8798/10000 (88%)\ttrain_Loss: 0.004720\tval_Loss: 0.005197\n",
      "Train Epoch: 8799/10000 (88%)\ttrain_Loss: 0.004726\tval_Loss: 0.005315\n",
      "Train Epoch: 8800/10000 (88%)\ttrain_Loss: 0.004728\tval_Loss: 0.005196\n",
      "Train Epoch: 8801/10000 (88%)\ttrain_Loss: 0.004727\tval_Loss: 0.005302\n",
      "Train Epoch: 8802/10000 (88%)\ttrain_Loss: 0.004723\tval_Loss: 0.005201\n",
      "Train Epoch: 8803/10000 (88%)\ttrain_Loss: 0.004717\tval_Loss: 0.005267\n",
      "Train Epoch: 8804/10000 (88%)\ttrain_Loss: 0.004711\tval_Loss: 0.005218\n",
      "Train Epoch: 8805/10000 (88%)\ttrain_Loss: 0.004708\tval_Loss: 0.005233\n",
      "Train Epoch: 8806/10000 (88%)\ttrain_Loss: 0.004706\tval_Loss: 0.005243\n",
      "Train Epoch: 8807/10000 (88%)\ttrain_Loss: 0.004707\tval_Loss: 0.005212\n",
      "Train Epoch: 8808/10000 (88%)\ttrain_Loss: 0.004709\tval_Loss: 0.005266\n",
      "Train Epoch: 8809/10000 (88%)\ttrain_Loss: 0.004711\tval_Loss: 0.005203\n",
      "Train Epoch: 8810/10000 (88%)\ttrain_Loss: 0.004713\tval_Loss: 0.005278\n",
      "Train Epoch: 8811/10000 (88%)\ttrain_Loss: 0.004714\tval_Loss: 0.005202\n",
      "Train Epoch: 8812/10000 (88%)\ttrain_Loss: 0.004714\tval_Loss: 0.005273\n",
      "Train Epoch: 8813/10000 (88%)\ttrain_Loss: 0.004712\tval_Loss: 0.005206\n",
      "Train Epoch: 8814/10000 (88%)\ttrain_Loss: 0.004710\tval_Loss: 0.005257\n",
      "Train Epoch: 8815/10000 (88%)\ttrain_Loss: 0.004708\tval_Loss: 0.005216\n",
      "Train Epoch: 8816/10000 (88%)\ttrain_Loss: 0.004706\tval_Loss: 0.005238\n",
      "Train Epoch: 8817/10000 (88%)\ttrain_Loss: 0.004705\tval_Loss: 0.005230\n",
      "Train Epoch: 8818/10000 (88%)\ttrain_Loss: 0.004704\tval_Loss: 0.005223\n",
      "Train Epoch: 8819/10000 (88%)\ttrain_Loss: 0.004705\tval_Loss: 0.005243\n",
      "Train Epoch: 8820/10000 (88%)\ttrain_Loss: 0.004705\tval_Loss: 0.005214\n",
      "Train Epoch: 8821/10000 (88%)\ttrain_Loss: 0.004706\tval_Loss: 0.005252\n",
      "Train Epoch: 8822/10000 (88%)\ttrain_Loss: 0.004707\tval_Loss: 0.005208\n",
      "Train Epoch: 8823/10000 (88%)\ttrain_Loss: 0.004707\tval_Loss: 0.005256\n",
      "Train Epoch: 8824/10000 (88%)\ttrain_Loss: 0.004707\tval_Loss: 0.005207\n",
      "Train Epoch: 8825/10000 (88%)\ttrain_Loss: 0.004707\tval_Loss: 0.005254\n",
      "Train Epoch: 8826/10000 (88%)\ttrain_Loss: 0.004707\tval_Loss: 0.005209\n",
      "Train Epoch: 8827/10000 (88%)\ttrain_Loss: 0.004706\tval_Loss: 0.005249\n",
      "Train Epoch: 8828/10000 (88%)\ttrain_Loss: 0.004705\tval_Loss: 0.005212\n",
      "Train Epoch: 8829/10000 (88%)\ttrain_Loss: 0.004705\tval_Loss: 0.005242\n",
      "Train Epoch: 8830/10000 (88%)\ttrain_Loss: 0.004704\tval_Loss: 0.005216\n",
      "Train Epoch: 8831/10000 (88%)\ttrain_Loss: 0.004704\tval_Loss: 0.005238\n",
      "Train Epoch: 8832/10000 (88%)\ttrain_Loss: 0.004703\tval_Loss: 0.005219\n",
      "Train Epoch: 8833/10000 (88%)\ttrain_Loss: 0.004703\tval_Loss: 0.005234\n",
      "Train Epoch: 8834/10000 (88%)\ttrain_Loss: 0.004703\tval_Loss: 0.005223\n",
      "Train Epoch: 8835/10000 (88%)\ttrain_Loss: 0.004702\tval_Loss: 0.005230\n",
      "Train Epoch: 8836/10000 (88%)\ttrain_Loss: 0.004702\tval_Loss: 0.005225\n",
      "Train Epoch: 8837/10000 (88%)\ttrain_Loss: 0.004702\tval_Loss: 0.005227\n",
      "Train Epoch: 8838/10000 (88%)\ttrain_Loss: 0.004702\tval_Loss: 0.005226\n",
      "Train Epoch: 8839/10000 (88%)\ttrain_Loss: 0.004702\tval_Loss: 0.005224\n",
      "Train Epoch: 8840/10000 (88%)\ttrain_Loss: 0.004702\tval_Loss: 0.005227\n",
      "Train Epoch: 8841/10000 (88%)\ttrain_Loss: 0.004701\tval_Loss: 0.005222\n",
      "Train Epoch: 8842/10000 (88%)\ttrain_Loss: 0.004701\tval_Loss: 0.005228\n",
      "Train Epoch: 8843/10000 (88%)\ttrain_Loss: 0.004701\tval_Loss: 0.005221\n",
      "Train Epoch: 8844/10000 (88%)\ttrain_Loss: 0.004701\tval_Loss: 0.005232\n",
      "Train Epoch: 8845/10000 (88%)\ttrain_Loss: 0.004701\tval_Loss: 0.005218\n",
      "Train Epoch: 8846/10000 (88%)\ttrain_Loss: 0.004701\tval_Loss: 0.005235\n",
      "Train Epoch: 8847/10000 (88%)\ttrain_Loss: 0.004701\tval_Loss: 0.005213\n",
      "Train Epoch: 8848/10000 (88%)\ttrain_Loss: 0.004702\tval_Loss: 0.005242\n",
      "Train Epoch: 8849/10000 (88%)\ttrain_Loss: 0.004702\tval_Loss: 0.005206\n",
      "Train Epoch: 8850/10000 (88%)\ttrain_Loss: 0.004703\tval_Loss: 0.005253\n",
      "Train Epoch: 8851/10000 (88%)\ttrain_Loss: 0.004704\tval_Loss: 0.005199\n",
      "Train Epoch: 8852/10000 (89%)\ttrain_Loss: 0.004706\tval_Loss: 0.005271\n",
      "Train Epoch: 8853/10000 (89%)\ttrain_Loss: 0.004709\tval_Loss: 0.005190\n",
      "Train Epoch: 8854/10000 (89%)\ttrain_Loss: 0.004713\tval_Loss: 0.005300\n",
      "Train Epoch: 8855/10000 (89%)\ttrain_Loss: 0.004719\tval_Loss: 0.005183\n",
      "Train Epoch: 8856/10000 (89%)\ttrain_Loss: 0.004728\tval_Loss: 0.005349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8857/10000 (89%)\ttrain_Loss: 0.004741\tval_Loss: 0.005184\n",
      "Train Epoch: 8858/10000 (89%)\ttrain_Loss: 0.004759\tval_Loss: 0.005418\n",
      "Train Epoch: 8859/10000 (89%)\ttrain_Loss: 0.004779\tval_Loss: 0.005195\n",
      "Train Epoch: 8860/10000 (89%)\ttrain_Loss: 0.004800\tval_Loss: 0.005477\n",
      "Train Epoch: 8861/10000 (89%)\ttrain_Loss: 0.004814\tval_Loss: 0.005201\n",
      "Train Epoch: 8862/10000 (89%)\ttrain_Loss: 0.004816\tval_Loss: 0.005452\n",
      "Train Epoch: 8863/10000 (89%)\ttrain_Loss: 0.004798\tval_Loss: 0.005185\n",
      "Train Epoch: 8864/10000 (89%)\ttrain_Loss: 0.004764\tval_Loss: 0.005321\n",
      "Train Epoch: 8865/10000 (89%)\ttrain_Loss: 0.004727\tval_Loss: 0.005200\n",
      "Train Epoch: 8866/10000 (89%)\ttrain_Loss: 0.004703\tval_Loss: 0.005209\n",
      "Train Epoch: 8867/10000 (89%)\ttrain_Loss: 0.004700\tval_Loss: 0.005287\n",
      "Train Epoch: 8868/10000 (89%)\ttrain_Loss: 0.004713\tval_Loss: 0.005180\n",
      "Train Epoch: 8869/10000 (89%)\ttrain_Loss: 0.004732\tval_Loss: 0.005352\n",
      "Train Epoch: 8870/10000 (89%)\ttrain_Loss: 0.004743\tval_Loss: 0.005179\n",
      "Train Epoch: 8871/10000 (89%)\ttrain_Loss: 0.004739\tval_Loss: 0.005313\n",
      "Train Epoch: 8872/10000 (89%)\ttrain_Loss: 0.004724\tval_Loss: 0.005191\n",
      "Train Epoch: 8873/10000 (89%)\ttrain_Loss: 0.004707\tval_Loss: 0.005230\n",
      "Train Epoch: 8874/10000 (89%)\ttrain_Loss: 0.004698\tval_Loss: 0.005242\n",
      "Train Epoch: 8875/10000 (89%)\ttrain_Loss: 0.004700\tval_Loss: 0.005188\n",
      "Train Epoch: 8876/10000 (89%)\ttrain_Loss: 0.004708\tval_Loss: 0.005294\n",
      "Train Epoch: 8877/10000 (89%)\ttrain_Loss: 0.004716\tval_Loss: 0.005180\n",
      "Train Epoch: 8878/10000 (89%)\ttrain_Loss: 0.004718\tval_Loss: 0.005286\n",
      "Train Epoch: 8879/10000 (89%)\ttrain_Loss: 0.004713\tval_Loss: 0.005191\n",
      "Train Epoch: 8880/10000 (89%)\ttrain_Loss: 0.004704\tval_Loss: 0.005236\n",
      "Train Epoch: 8881/10000 (89%)\ttrain_Loss: 0.004698\tval_Loss: 0.005225\n",
      "Train Epoch: 8882/10000 (89%)\ttrain_Loss: 0.004697\tval_Loss: 0.005199\n",
      "Train Epoch: 8883/10000 (89%)\ttrain_Loss: 0.004700\tval_Loss: 0.005262\n",
      "Train Epoch: 8884/10000 (89%)\ttrain_Loss: 0.004704\tval_Loss: 0.005186\n",
      "Train Epoch: 8885/10000 (89%)\ttrain_Loss: 0.004706\tval_Loss: 0.005265\n",
      "Train Epoch: 8886/10000 (89%)\ttrain_Loss: 0.004705\tval_Loss: 0.005191\n",
      "Train Epoch: 8887/10000 (89%)\ttrain_Loss: 0.004702\tval_Loss: 0.005240\n",
      "Train Epoch: 8888/10000 (89%)\ttrain_Loss: 0.004698\tval_Loss: 0.005212\n",
      "Train Epoch: 8889/10000 (89%)\ttrain_Loss: 0.004696\tval_Loss: 0.005211\n",
      "Train Epoch: 8890/10000 (89%)\ttrain_Loss: 0.004696\tval_Loss: 0.005238\n",
      "Train Epoch: 8891/10000 (89%)\ttrain_Loss: 0.004697\tval_Loss: 0.005196\n",
      "Train Epoch: 8892/10000 (89%)\ttrain_Loss: 0.004699\tval_Loss: 0.005251\n",
      "Train Epoch: 8893/10000 (89%)\ttrain_Loss: 0.004700\tval_Loss: 0.005193\n",
      "Train Epoch: 8894/10000 (89%)\ttrain_Loss: 0.004700\tval_Loss: 0.005242\n",
      "Train Epoch: 8895/10000 (89%)\ttrain_Loss: 0.004698\tval_Loss: 0.005201\n",
      "Train Epoch: 8896/10000 (89%)\ttrain_Loss: 0.004696\tval_Loss: 0.005222\n",
      "Train Epoch: 8897/10000 (89%)\ttrain_Loss: 0.004695\tval_Loss: 0.005218\n",
      "Train Epoch: 8898/10000 (89%)\ttrain_Loss: 0.004695\tval_Loss: 0.005206\n",
      "Train Epoch: 8899/10000 (89%)\ttrain_Loss: 0.004695\tval_Loss: 0.005234\n",
      "Train Epoch: 8900/10000 (89%)\ttrain_Loss: 0.004696\tval_Loss: 0.005199\n",
      "Train Epoch: 8901/10000 (89%)\ttrain_Loss: 0.004697\tval_Loss: 0.005239\n",
      "Train Epoch: 8902/10000 (89%)\ttrain_Loss: 0.004697\tval_Loss: 0.005199\n",
      "Train Epoch: 8903/10000 (89%)\ttrain_Loss: 0.004696\tval_Loss: 0.005232\n",
      "Train Epoch: 8904/10000 (89%)\ttrain_Loss: 0.004695\tval_Loss: 0.005205\n",
      "Train Epoch: 8905/10000 (89%)\ttrain_Loss: 0.004694\tval_Loss: 0.005219\n",
      "Train Epoch: 8906/10000 (89%)\ttrain_Loss: 0.004694\tval_Loss: 0.005215\n",
      "Train Epoch: 8907/10000 (89%)\ttrain_Loss: 0.004694\tval_Loss: 0.005209\n",
      "Train Epoch: 8908/10000 (89%)\ttrain_Loss: 0.004694\tval_Loss: 0.005224\n",
      "Train Epoch: 8909/10000 (89%)\ttrain_Loss: 0.004694\tval_Loss: 0.005202\n",
      "Train Epoch: 8910/10000 (89%)\ttrain_Loss: 0.004694\tval_Loss: 0.005229\n",
      "Train Epoch: 8911/10000 (89%)\ttrain_Loss: 0.004694\tval_Loss: 0.005199\n",
      "Train Epoch: 8912/10000 (89%)\ttrain_Loss: 0.004694\tval_Loss: 0.005228\n",
      "Train Epoch: 8913/10000 (89%)\ttrain_Loss: 0.004694\tval_Loss: 0.005202\n",
      "Train Epoch: 8914/10000 (89%)\ttrain_Loss: 0.004694\tval_Loss: 0.005224\n",
      "Train Epoch: 8915/10000 (89%)\ttrain_Loss: 0.004693\tval_Loss: 0.005207\n",
      "Train Epoch: 8916/10000 (89%)\ttrain_Loss: 0.004693\tval_Loss: 0.005217\n",
      "Train Epoch: 8917/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005213\n",
      "Train Epoch: 8918/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005210\n",
      "Train Epoch: 8919/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005216\n",
      "Train Epoch: 8920/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005204\n",
      "Train Epoch: 8921/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005220\n",
      "Train Epoch: 8922/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005203\n",
      "Train Epoch: 8923/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005222\n",
      "Train Epoch: 8924/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005202\n",
      "Train Epoch: 8925/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005223\n",
      "Train Epoch: 8926/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005201\n",
      "Train Epoch: 8927/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005224\n",
      "Train Epoch: 8928/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005199\n",
      "Train Epoch: 8929/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005225\n",
      "Train Epoch: 8930/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005198\n",
      "Train Epoch: 8931/10000 (89%)\ttrain_Loss: 0.004692\tval_Loss: 0.005224\n",
      "Train Epoch: 8932/10000 (89%)\ttrain_Loss: 0.004691\tval_Loss: 0.005199\n",
      "Train Epoch: 8933/10000 (89%)\ttrain_Loss: 0.004691\tval_Loss: 0.005223\n",
      "Train Epoch: 8934/10000 (89%)\ttrain_Loss: 0.004691\tval_Loss: 0.005199\n",
      "Train Epoch: 8935/10000 (89%)\ttrain_Loss: 0.004691\tval_Loss: 0.005220\n",
      "Train Epoch: 8936/10000 (89%)\ttrain_Loss: 0.004691\tval_Loss: 0.005200\n",
      "Train Epoch: 8937/10000 (89%)\ttrain_Loss: 0.004690\tval_Loss: 0.005218\n",
      "Train Epoch: 8938/10000 (89%)\ttrain_Loss: 0.004690\tval_Loss: 0.005200\n",
      "Train Epoch: 8939/10000 (89%)\ttrain_Loss: 0.004690\tval_Loss: 0.005216\n",
      "Train Epoch: 8940/10000 (89%)\ttrain_Loss: 0.004690\tval_Loss: 0.005201\n",
      "Train Epoch: 8941/10000 (89%)\ttrain_Loss: 0.004690\tval_Loss: 0.005216\n",
      "Train Epoch: 8942/10000 (89%)\ttrain_Loss: 0.004689\tval_Loss: 0.005202\n",
      "Train Epoch: 8943/10000 (89%)\ttrain_Loss: 0.004689\tval_Loss: 0.005216\n",
      "Train Epoch: 8944/10000 (89%)\ttrain_Loss: 0.004689\tval_Loss: 0.005199\n",
      "Train Epoch: 8945/10000 (89%)\ttrain_Loss: 0.004689\tval_Loss: 0.005218\n",
      "Train Epoch: 8946/10000 (89%)\ttrain_Loss: 0.004689\tval_Loss: 0.005197\n",
      "Train Epoch: 8947/10000 (89%)\ttrain_Loss: 0.004689\tval_Loss: 0.005222\n",
      "Train Epoch: 8948/10000 (89%)\ttrain_Loss: 0.004689\tval_Loss: 0.005194\n",
      "Train Epoch: 8949/10000 (89%)\ttrain_Loss: 0.004690\tval_Loss: 0.005229\n",
      "Train Epoch: 8950/10000 (89%)\ttrain_Loss: 0.004690\tval_Loss: 0.005188\n",
      "Train Epoch: 8951/10000 (90%)\ttrain_Loss: 0.004691\tval_Loss: 0.005238\n",
      "Train Epoch: 8952/10000 (90%)\ttrain_Loss: 0.004692\tval_Loss: 0.005181\n",
      "Train Epoch: 8953/10000 (90%)\ttrain_Loss: 0.004694\tval_Loss: 0.005254\n",
      "Train Epoch: 8954/10000 (90%)\ttrain_Loss: 0.004697\tval_Loss: 0.005172\n",
      "Train Epoch: 8955/10000 (90%)\ttrain_Loss: 0.004701\tval_Loss: 0.005280\n",
      "Train Epoch: 8956/10000 (90%)\ttrain_Loss: 0.004706\tval_Loss: 0.005166\n",
      "Train Epoch: 8957/10000 (90%)\ttrain_Loss: 0.004714\tval_Loss: 0.005325\n",
      "Train Epoch: 8958/10000 (90%)\ttrain_Loss: 0.004726\tval_Loss: 0.005166\n",
      "Train Epoch: 8959/10000 (90%)\ttrain_Loss: 0.004741\tval_Loss: 0.005387\n",
      "Train Epoch: 8960/10000 (90%)\ttrain_Loss: 0.004758\tval_Loss: 0.005174\n",
      "Train Epoch: 8961/10000 (90%)\ttrain_Loss: 0.004777\tval_Loss: 0.005441\n",
      "Train Epoch: 8962/10000 (90%)\ttrain_Loss: 0.004791\tval_Loss: 0.005180\n",
      "Train Epoch: 8963/10000 (90%)\ttrain_Loss: 0.004795\tval_Loss: 0.005427\n",
      "Train Epoch: 8964/10000 (90%)\ttrain_Loss: 0.004782\tval_Loss: 0.005167\n",
      "Train Epoch: 8965/10000 (90%)\ttrain_Loss: 0.004755\tval_Loss: 0.005317\n",
      "Train Epoch: 8966/10000 (90%)\ttrain_Loss: 0.004721\tval_Loss: 0.005175\n",
      "Train Epoch: 8967/10000 (90%)\ttrain_Loss: 0.004695\tval_Loss: 0.005206\n",
      "Train Epoch: 8968/10000 (90%)\ttrain_Loss: 0.004686\tval_Loss: 0.005247\n",
      "Train Epoch: 8969/10000 (90%)\ttrain_Loss: 0.004693\tval_Loss: 0.005166\n",
      "Train Epoch: 8970/10000 (90%)\ttrain_Loss: 0.004709\tval_Loss: 0.005320\n",
      "Train Epoch: 8971/10000 (90%)\ttrain_Loss: 0.004723\tval_Loss: 0.005163\n",
      "Train Epoch: 8972/10000 (90%)\ttrain_Loss: 0.004727\tval_Loss: 0.005313\n",
      "Train Epoch: 8973/10000 (90%)\ttrain_Loss: 0.004719\tval_Loss: 0.005166\n",
      "Train Epoch: 8974/10000 (90%)\ttrain_Loss: 0.004705\tval_Loss: 0.005241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8975/10000 (90%)\ttrain_Loss: 0.004692\tval_Loss: 0.005198\n",
      "Train Epoch: 8976/10000 (90%)\ttrain_Loss: 0.004685\tval_Loss: 0.005185\n",
      "Train Epoch: 8977/10000 (90%)\ttrain_Loss: 0.004688\tval_Loss: 0.005252\n",
      "Train Epoch: 8978/10000 (90%)\ttrain_Loss: 0.004695\tval_Loss: 0.005167\n",
      "Train Epoch: 8979/10000 (90%)\ttrain_Loss: 0.004701\tval_Loss: 0.005277\n",
      "Train Epoch: 8980/10000 (90%)\ttrain_Loss: 0.004703\tval_Loss: 0.005167\n",
      "Train Epoch: 8981/10000 (90%)\ttrain_Loss: 0.004700\tval_Loss: 0.005250\n",
      "Train Epoch: 8982/10000 (90%)\ttrain_Loss: 0.004694\tval_Loss: 0.005182\n",
      "Train Epoch: 8983/10000 (90%)\ttrain_Loss: 0.004688\tval_Loss: 0.005206\n",
      "Train Epoch: 8984/10000 (90%)\ttrain_Loss: 0.004684\tval_Loss: 0.005214\n",
      "Train Epoch: 8985/10000 (90%)\ttrain_Loss: 0.004685\tval_Loss: 0.005179\n",
      "Train Epoch: 8986/10000 (90%)\ttrain_Loss: 0.004688\tval_Loss: 0.005241\n",
      "Train Epoch: 8987/10000 (90%)\ttrain_Loss: 0.004691\tval_Loss: 0.005171\n",
      "Train Epoch: 8988/10000 (90%)\ttrain_Loss: 0.004692\tval_Loss: 0.005243\n",
      "Train Epoch: 8989/10000 (90%)\ttrain_Loss: 0.004691\tval_Loss: 0.005176\n",
      "Train Epoch: 8990/10000 (90%)\ttrain_Loss: 0.004689\tval_Loss: 0.005224\n",
      "Train Epoch: 8991/10000 (90%)\ttrain_Loss: 0.004686\tval_Loss: 0.005191\n",
      "Train Epoch: 8992/10000 (90%)\ttrain_Loss: 0.004684\tval_Loss: 0.005200\n",
      "Train Epoch: 8993/10000 (90%)\ttrain_Loss: 0.004683\tval_Loss: 0.005211\n",
      "Train Epoch: 8994/10000 (90%)\ttrain_Loss: 0.004684\tval_Loss: 0.005183\n",
      "Train Epoch: 8995/10000 (90%)\ttrain_Loss: 0.004685\tval_Loss: 0.005225\n",
      "Train Epoch: 8996/10000 (90%)\ttrain_Loss: 0.004686\tval_Loss: 0.005178\n",
      "Train Epoch: 8997/10000 (90%)\ttrain_Loss: 0.004686\tval_Loss: 0.005225\n",
      "Train Epoch: 8998/10000 (90%)\ttrain_Loss: 0.004686\tval_Loss: 0.005182\n",
      "Train Epoch: 8999/10000 (90%)\ttrain_Loss: 0.004685\tval_Loss: 0.005214\n",
      "Train Epoch: 9000/10000 (90%)\ttrain_Loss: 0.004684\tval_Loss: 0.005192\n",
      "Train Epoch: 9001/10000 (90%)\ttrain_Loss: 0.004683\tval_Loss: 0.005199\n",
      "Train Epoch: 9002/10000 (90%)\ttrain_Loss: 0.004682\tval_Loss: 0.005203\n",
      "Train Epoch: 9003/10000 (90%)\ttrain_Loss: 0.004682\tval_Loss: 0.005188\n",
      "Train Epoch: 9004/10000 (90%)\ttrain_Loss: 0.004683\tval_Loss: 0.005212\n",
      "Train Epoch: 9005/10000 (90%)\ttrain_Loss: 0.004683\tval_Loss: 0.005183\n",
      "Train Epoch: 9006/10000 (90%)\ttrain_Loss: 0.004683\tval_Loss: 0.005216\n",
      "Train Epoch: 9007/10000 (90%)\ttrain_Loss: 0.004683\tval_Loss: 0.005183\n",
      "Train Epoch: 9008/10000 (90%)\ttrain_Loss: 0.004683\tval_Loss: 0.005215\n",
      "Train Epoch: 9009/10000 (90%)\ttrain_Loss: 0.004683\tval_Loss: 0.005185\n",
      "Train Epoch: 9010/10000 (90%)\ttrain_Loss: 0.004682\tval_Loss: 0.005209\n",
      "Train Epoch: 9011/10000 (90%)\ttrain_Loss: 0.004682\tval_Loss: 0.005190\n",
      "Train Epoch: 9012/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005201\n",
      "Train Epoch: 9013/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005196\n",
      "Train Epoch: 9014/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005194\n",
      "Train Epoch: 9015/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005200\n",
      "Train Epoch: 9016/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005189\n",
      "Train Epoch: 9017/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005204\n",
      "Train Epoch: 9018/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005187\n",
      "Train Epoch: 9019/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005207\n",
      "Train Epoch: 9020/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005185\n",
      "Train Epoch: 9021/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005207\n",
      "Train Epoch: 9022/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005183\n",
      "Train Epoch: 9023/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005209\n",
      "Train Epoch: 9024/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005182\n",
      "Train Epoch: 9025/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005210\n",
      "Train Epoch: 9026/10000 (90%)\ttrain_Loss: 0.004681\tval_Loss: 0.005181\n",
      "Train Epoch: 9027/10000 (90%)\ttrain_Loss: 0.004680\tval_Loss: 0.005209\n",
      "Train Epoch: 9028/10000 (90%)\ttrain_Loss: 0.004680\tval_Loss: 0.005180\n",
      "Train Epoch: 9029/10000 (90%)\ttrain_Loss: 0.004680\tval_Loss: 0.005208\n",
      "Train Epoch: 9030/10000 (90%)\ttrain_Loss: 0.004680\tval_Loss: 0.005181\n",
      "Train Epoch: 9031/10000 (90%)\ttrain_Loss: 0.004680\tval_Loss: 0.005207\n",
      "Train Epoch: 9032/10000 (90%)\ttrain_Loss: 0.004680\tval_Loss: 0.005181\n",
      "Train Epoch: 9033/10000 (90%)\ttrain_Loss: 0.004679\tval_Loss: 0.005206\n",
      "Train Epoch: 9034/10000 (90%)\ttrain_Loss: 0.004679\tval_Loss: 0.005182\n",
      "Train Epoch: 9035/10000 (90%)\ttrain_Loss: 0.004679\tval_Loss: 0.005206\n",
      "Train Epoch: 9036/10000 (90%)\ttrain_Loss: 0.004679\tval_Loss: 0.005181\n",
      "Train Epoch: 9037/10000 (90%)\ttrain_Loss: 0.004679\tval_Loss: 0.005205\n",
      "Train Epoch: 9038/10000 (90%)\ttrain_Loss: 0.004679\tval_Loss: 0.005180\n",
      "Train Epoch: 9039/10000 (90%)\ttrain_Loss: 0.004679\tval_Loss: 0.005205\n",
      "Train Epoch: 9040/10000 (90%)\ttrain_Loss: 0.004679\tval_Loss: 0.005178\n",
      "Train Epoch: 9041/10000 (90%)\ttrain_Loss: 0.004679\tval_Loss: 0.005210\n",
      "Train Epoch: 9042/10000 (90%)\ttrain_Loss: 0.004679\tval_Loss: 0.005174\n",
      "Train Epoch: 9043/10000 (90%)\ttrain_Loss: 0.004680\tval_Loss: 0.005219\n",
      "Train Epoch: 9044/10000 (90%)\ttrain_Loss: 0.004680\tval_Loss: 0.005169\n",
      "Train Epoch: 9045/10000 (90%)\ttrain_Loss: 0.004682\tval_Loss: 0.005231\n",
      "Train Epoch: 9046/10000 (90%)\ttrain_Loss: 0.004683\tval_Loss: 0.005161\n",
      "Train Epoch: 9047/10000 (90%)\ttrain_Loss: 0.004686\tval_Loss: 0.005249\n",
      "Train Epoch: 9048/10000 (90%)\ttrain_Loss: 0.004689\tval_Loss: 0.005154\n",
      "Train Epoch: 9049/10000 (90%)\ttrain_Loss: 0.004694\tval_Loss: 0.005279\n",
      "Train Epoch: 9050/10000 (90%)\ttrain_Loss: 0.004701\tval_Loss: 0.005150\n",
      "Train Epoch: 9051/10000 (90%)\ttrain_Loss: 0.004709\tval_Loss: 0.005321\n",
      "Train Epoch: 9052/10000 (91%)\ttrain_Loss: 0.004720\tval_Loss: 0.005151\n",
      "Train Epoch: 9053/10000 (91%)\ttrain_Loss: 0.004734\tval_Loss: 0.005371\n",
      "Train Epoch: 9054/10000 (91%)\ttrain_Loss: 0.004747\tval_Loss: 0.005156\n",
      "Train Epoch: 9055/10000 (91%)\ttrain_Loss: 0.004759\tval_Loss: 0.005401\n",
      "Train Epoch: 9056/10000 (91%)\ttrain_Loss: 0.004765\tval_Loss: 0.005156\n",
      "Train Epoch: 9057/10000 (91%)\ttrain_Loss: 0.004761\tval_Loss: 0.005367\n",
      "Train Epoch: 9058/10000 (91%)\ttrain_Loss: 0.004745\tval_Loss: 0.005148\n",
      "Train Epoch: 9059/10000 (91%)\ttrain_Loss: 0.004722\tval_Loss: 0.005273\n",
      "Train Epoch: 9060/10000 (91%)\ttrain_Loss: 0.004697\tval_Loss: 0.005166\n",
      "Train Epoch: 9061/10000 (91%)\ttrain_Loss: 0.004681\tval_Loss: 0.005189\n",
      "Train Epoch: 9062/10000 (91%)\ttrain_Loss: 0.004675\tval_Loss: 0.005224\n",
      "Train Epoch: 9063/10000 (91%)\ttrain_Loss: 0.004680\tval_Loss: 0.005153\n",
      "Train Epoch: 9064/10000 (91%)\ttrain_Loss: 0.004691\tval_Loss: 0.005281\n",
      "Train Epoch: 9065/10000 (91%)\ttrain_Loss: 0.004701\tval_Loss: 0.005146\n",
      "Train Epoch: 9066/10000 (91%)\ttrain_Loss: 0.004706\tval_Loss: 0.005288\n",
      "Train Epoch: 9067/10000 (91%)\ttrain_Loss: 0.004704\tval_Loss: 0.005149\n",
      "Train Epoch: 9068/10000 (91%)\ttrain_Loss: 0.004696\tval_Loss: 0.005244\n",
      "Train Epoch: 9069/10000 (91%)\ttrain_Loss: 0.004686\tval_Loss: 0.005167\n",
      "Train Epoch: 9070/10000 (91%)\ttrain_Loss: 0.004678\tval_Loss: 0.005191\n",
      "Train Epoch: 9071/10000 (91%)\ttrain_Loss: 0.004674\tval_Loss: 0.005203\n",
      "Train Epoch: 9072/10000 (91%)\ttrain_Loss: 0.004676\tval_Loss: 0.005161\n",
      "Train Epoch: 9073/10000 (91%)\ttrain_Loss: 0.004680\tval_Loss: 0.005237\n",
      "Train Epoch: 9074/10000 (91%)\ttrain_Loss: 0.004684\tval_Loss: 0.005152\n",
      "Train Epoch: 9075/10000 (91%)\ttrain_Loss: 0.004686\tval_Loss: 0.005243\n",
      "Train Epoch: 9076/10000 (91%)\ttrain_Loss: 0.004686\tval_Loss: 0.005156\n",
      "Train Epoch: 9077/10000 (91%)\ttrain_Loss: 0.004683\tval_Loss: 0.005222\n",
      "Train Epoch: 9078/10000 (91%)\ttrain_Loss: 0.004679\tval_Loss: 0.005170\n",
      "Train Epoch: 9079/10000 (91%)\ttrain_Loss: 0.004675\tval_Loss: 0.005191\n",
      "Train Epoch: 9080/10000 (91%)\ttrain_Loss: 0.004673\tval_Loss: 0.005192\n",
      "Train Epoch: 9081/10000 (91%)\ttrain_Loss: 0.004673\tval_Loss: 0.005170\n",
      "Train Epoch: 9082/10000 (91%)\ttrain_Loss: 0.004675\tval_Loss: 0.005212\n",
      "Train Epoch: 9083/10000 (91%)\ttrain_Loss: 0.004676\tval_Loss: 0.005159\n",
      "Train Epoch: 9084/10000 (91%)\ttrain_Loss: 0.004678\tval_Loss: 0.005221\n",
      "Train Epoch: 9085/10000 (91%)\ttrain_Loss: 0.004679\tval_Loss: 0.005159\n",
      "Train Epoch: 9086/10000 (91%)\ttrain_Loss: 0.004678\tval_Loss: 0.005216\n",
      "Train Epoch: 9087/10000 (91%)\ttrain_Loss: 0.004677\tval_Loss: 0.005165\n",
      "Train Epoch: 9088/10000 (91%)\ttrain_Loss: 0.004675\tval_Loss: 0.005200\n",
      "Train Epoch: 9089/10000 (91%)\ttrain_Loss: 0.004674\tval_Loss: 0.005176\n",
      "Train Epoch: 9090/10000 (91%)\ttrain_Loss: 0.004672\tval_Loss: 0.005184\n",
      "Train Epoch: 9091/10000 (91%)\ttrain_Loss: 0.004672\tval_Loss: 0.005189\n",
      "Train Epoch: 9092/10000 (91%)\ttrain_Loss: 0.004672\tval_Loss: 0.005173\n",
      "Train Epoch: 9093/10000 (91%)\ttrain_Loss: 0.004673\tval_Loss: 0.005200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9094/10000 (91%)\ttrain_Loss: 0.004673\tval_Loss: 0.005168\n",
      "Train Epoch: 9095/10000 (91%)\ttrain_Loss: 0.004673\tval_Loss: 0.005204\n",
      "Train Epoch: 9096/10000 (91%)\ttrain_Loss: 0.004674\tval_Loss: 0.005166\n",
      "Train Epoch: 9097/10000 (91%)\ttrain_Loss: 0.004673\tval_Loss: 0.005202\n",
      "Train Epoch: 9098/10000 (91%)\ttrain_Loss: 0.004673\tval_Loss: 0.005167\n",
      "Train Epoch: 9099/10000 (91%)\ttrain_Loss: 0.004673\tval_Loss: 0.005198\n",
      "Train Epoch: 9100/10000 (91%)\ttrain_Loss: 0.004672\tval_Loss: 0.005171\n",
      "Train Epoch: 9101/10000 (91%)\ttrain_Loss: 0.004672\tval_Loss: 0.005193\n",
      "Train Epoch: 9102/10000 (91%)\ttrain_Loss: 0.004671\tval_Loss: 0.005175\n",
      "Train Epoch: 9103/10000 (91%)\ttrain_Loss: 0.004671\tval_Loss: 0.005186\n",
      "Train Epoch: 9104/10000 (91%)\ttrain_Loss: 0.004671\tval_Loss: 0.005179\n",
      "Train Epoch: 9105/10000 (91%)\ttrain_Loss: 0.004670\tval_Loss: 0.005180\n",
      "Train Epoch: 9106/10000 (91%)\ttrain_Loss: 0.004670\tval_Loss: 0.005182\n",
      "Train Epoch: 9107/10000 (91%)\ttrain_Loss: 0.004670\tval_Loss: 0.005176\n",
      "Train Epoch: 9108/10000 (91%)\ttrain_Loss: 0.004670\tval_Loss: 0.005187\n",
      "Train Epoch: 9109/10000 (91%)\ttrain_Loss: 0.004670\tval_Loss: 0.005174\n",
      "Train Epoch: 9110/10000 (91%)\ttrain_Loss: 0.004670\tval_Loss: 0.005191\n",
      "Train Epoch: 9111/10000 (91%)\ttrain_Loss: 0.004670\tval_Loss: 0.005170\n",
      "Train Epoch: 9112/10000 (91%)\ttrain_Loss: 0.004670\tval_Loss: 0.005196\n",
      "Train Epoch: 9113/10000 (91%)\ttrain_Loss: 0.004671\tval_Loss: 0.005165\n",
      "Train Epoch: 9114/10000 (91%)\ttrain_Loss: 0.004671\tval_Loss: 0.005201\n",
      "Train Epoch: 9115/10000 (91%)\ttrain_Loss: 0.004671\tval_Loss: 0.005162\n",
      "Train Epoch: 9116/10000 (91%)\ttrain_Loss: 0.004672\tval_Loss: 0.005206\n",
      "Train Epoch: 9117/10000 (91%)\ttrain_Loss: 0.004672\tval_Loss: 0.005158\n",
      "Train Epoch: 9118/10000 (91%)\ttrain_Loss: 0.004673\tval_Loss: 0.005212\n",
      "Train Epoch: 9119/10000 (91%)\ttrain_Loss: 0.004673\tval_Loss: 0.005155\n",
      "Train Epoch: 9120/10000 (91%)\ttrain_Loss: 0.004674\tval_Loss: 0.005219\n",
      "Train Epoch: 9121/10000 (91%)\ttrain_Loss: 0.004675\tval_Loss: 0.005150\n",
      "Train Epoch: 9122/10000 (91%)\ttrain_Loss: 0.004676\tval_Loss: 0.005227\n",
      "Train Epoch: 9123/10000 (91%)\ttrain_Loss: 0.004678\tval_Loss: 0.005146\n",
      "Train Epoch: 9124/10000 (91%)\ttrain_Loss: 0.004679\tval_Loss: 0.005239\n",
      "Train Epoch: 9125/10000 (91%)\ttrain_Loss: 0.004681\tval_Loss: 0.005142\n",
      "Train Epoch: 9126/10000 (91%)\ttrain_Loss: 0.004684\tval_Loss: 0.005255\n",
      "Train Epoch: 9127/10000 (91%)\ttrain_Loss: 0.004687\tval_Loss: 0.005139\n",
      "Train Epoch: 9128/10000 (91%)\ttrain_Loss: 0.004691\tval_Loss: 0.005273\n",
      "Train Epoch: 9129/10000 (91%)\ttrain_Loss: 0.004695\tval_Loss: 0.005137\n",
      "Train Epoch: 9130/10000 (91%)\ttrain_Loss: 0.004699\tval_Loss: 0.005292\n",
      "Train Epoch: 9131/10000 (91%)\ttrain_Loss: 0.004703\tval_Loss: 0.005136\n",
      "Train Epoch: 9132/10000 (91%)\ttrain_Loss: 0.004707\tval_Loss: 0.005303\n",
      "Train Epoch: 9133/10000 (91%)\ttrain_Loss: 0.004709\tval_Loss: 0.005135\n",
      "Train Epoch: 9134/10000 (91%)\ttrain_Loss: 0.004708\tval_Loss: 0.005295\n",
      "Train Epoch: 9135/10000 (91%)\ttrain_Loss: 0.004705\tval_Loss: 0.005136\n",
      "Train Epoch: 9136/10000 (91%)\ttrain_Loss: 0.004699\tval_Loss: 0.005266\n",
      "Train Epoch: 9137/10000 (91%)\ttrain_Loss: 0.004691\tval_Loss: 0.005140\n",
      "Train Epoch: 9138/10000 (91%)\ttrain_Loss: 0.004683\tval_Loss: 0.005225\n",
      "Train Epoch: 9139/10000 (91%)\ttrain_Loss: 0.004676\tval_Loss: 0.005155\n",
      "Train Epoch: 9140/10000 (91%)\ttrain_Loss: 0.004670\tval_Loss: 0.005187\n",
      "Train Epoch: 9141/10000 (91%)\ttrain_Loss: 0.004667\tval_Loss: 0.005178\n",
      "Train Epoch: 9142/10000 (91%)\ttrain_Loss: 0.004666\tval_Loss: 0.005161\n",
      "Train Epoch: 9143/10000 (91%)\ttrain_Loss: 0.004667\tval_Loss: 0.005204\n",
      "Train Epoch: 9144/10000 (91%)\ttrain_Loss: 0.004670\tval_Loss: 0.005148\n",
      "Train Epoch: 9145/10000 (91%)\ttrain_Loss: 0.004672\tval_Loss: 0.005222\n",
      "Train Epoch: 9146/10000 (91%)\ttrain_Loss: 0.004675\tval_Loss: 0.005141\n",
      "Train Epoch: 9147/10000 (91%)\ttrain_Loss: 0.004677\tval_Loss: 0.005232\n",
      "Train Epoch: 9148/10000 (91%)\ttrain_Loss: 0.004678\tval_Loss: 0.005140\n",
      "Train Epoch: 9149/10000 (91%)\ttrain_Loss: 0.004678\tval_Loss: 0.005232\n",
      "Train Epoch: 9150/10000 (91%)\ttrain_Loss: 0.004678\tval_Loss: 0.005142\n",
      "Train Epoch: 9151/10000 (92%)\ttrain_Loss: 0.004676\tval_Loss: 0.005221\n",
      "Train Epoch: 9152/10000 (92%)\ttrain_Loss: 0.004674\tval_Loss: 0.005147\n",
      "Train Epoch: 9153/10000 (92%)\ttrain_Loss: 0.004672\tval_Loss: 0.005205\n",
      "Train Epoch: 9154/10000 (92%)\ttrain_Loss: 0.004669\tval_Loss: 0.005155\n",
      "Train Epoch: 9155/10000 (92%)\ttrain_Loss: 0.004667\tval_Loss: 0.005189\n",
      "Train Epoch: 9156/10000 (92%)\ttrain_Loss: 0.004666\tval_Loss: 0.005165\n",
      "Train Epoch: 9157/10000 (92%)\ttrain_Loss: 0.004665\tval_Loss: 0.005175\n",
      "Train Epoch: 9158/10000 (92%)\ttrain_Loss: 0.004664\tval_Loss: 0.005176\n",
      "Train Epoch: 9159/10000 (92%)\ttrain_Loss: 0.004664\tval_Loss: 0.005165\n",
      "Train Epoch: 9160/10000 (92%)\ttrain_Loss: 0.004665\tval_Loss: 0.005185\n",
      "Train Epoch: 9161/10000 (92%)\ttrain_Loss: 0.004665\tval_Loss: 0.005159\n",
      "Train Epoch: 9162/10000 (92%)\ttrain_Loss: 0.004665\tval_Loss: 0.005192\n",
      "Train Epoch: 9163/10000 (92%)\ttrain_Loss: 0.004666\tval_Loss: 0.005153\n",
      "Train Epoch: 9164/10000 (92%)\ttrain_Loss: 0.004666\tval_Loss: 0.005197\n",
      "Train Epoch: 9165/10000 (92%)\ttrain_Loss: 0.004667\tval_Loss: 0.005149\n",
      "Train Epoch: 9166/10000 (92%)\ttrain_Loss: 0.004667\tval_Loss: 0.005201\n",
      "Train Epoch: 9167/10000 (92%)\ttrain_Loss: 0.004668\tval_Loss: 0.005146\n",
      "Train Epoch: 9168/10000 (92%)\ttrain_Loss: 0.004668\tval_Loss: 0.005208\n",
      "Train Epoch: 9169/10000 (92%)\ttrain_Loss: 0.004669\tval_Loss: 0.005144\n",
      "Train Epoch: 9170/10000 (92%)\ttrain_Loss: 0.004670\tval_Loss: 0.005216\n",
      "Train Epoch: 9171/10000 (92%)\ttrain_Loss: 0.004671\tval_Loss: 0.005142\n",
      "Train Epoch: 9172/10000 (92%)\ttrain_Loss: 0.004672\tval_Loss: 0.005223\n",
      "Train Epoch: 9173/10000 (92%)\ttrain_Loss: 0.004673\tval_Loss: 0.005138\n",
      "Train Epoch: 9174/10000 (92%)\ttrain_Loss: 0.004674\tval_Loss: 0.005231\n",
      "Train Epoch: 9175/10000 (92%)\ttrain_Loss: 0.004676\tval_Loss: 0.005135\n",
      "Train Epoch: 9176/10000 (92%)\ttrain_Loss: 0.004677\tval_Loss: 0.005240\n",
      "Train Epoch: 9177/10000 (92%)\ttrain_Loss: 0.004679\tval_Loss: 0.005133\n",
      "Train Epoch: 9178/10000 (92%)\ttrain_Loss: 0.004681\tval_Loss: 0.005251\n",
      "Train Epoch: 9179/10000 (92%)\ttrain_Loss: 0.004683\tval_Loss: 0.005130\n",
      "Train Epoch: 9180/10000 (92%)\ttrain_Loss: 0.004686\tval_Loss: 0.005262\n",
      "Train Epoch: 9181/10000 (92%)\ttrain_Loss: 0.004688\tval_Loss: 0.005128\n",
      "Train Epoch: 9182/10000 (92%)\ttrain_Loss: 0.004690\tval_Loss: 0.005271\n",
      "Train Epoch: 9183/10000 (92%)\ttrain_Loss: 0.004692\tval_Loss: 0.005127\n",
      "Train Epoch: 9184/10000 (92%)\ttrain_Loss: 0.004693\tval_Loss: 0.005274\n",
      "Train Epoch: 9185/10000 (92%)\ttrain_Loss: 0.004693\tval_Loss: 0.005128\n",
      "Train Epoch: 9186/10000 (92%)\ttrain_Loss: 0.004692\tval_Loss: 0.005264\n",
      "Train Epoch: 9187/10000 (92%)\ttrain_Loss: 0.004688\tval_Loss: 0.005129\n",
      "Train Epoch: 9188/10000 (92%)\ttrain_Loss: 0.004684\tval_Loss: 0.005241\n",
      "Train Epoch: 9189/10000 (92%)\ttrain_Loss: 0.004679\tval_Loss: 0.005135\n",
      "Train Epoch: 9190/10000 (92%)\ttrain_Loss: 0.004673\tval_Loss: 0.005211\n",
      "Train Epoch: 9191/10000 (92%)\ttrain_Loss: 0.004669\tval_Loss: 0.005145\n",
      "Train Epoch: 9192/10000 (92%)\ttrain_Loss: 0.004665\tval_Loss: 0.005184\n",
      "Train Epoch: 9193/10000 (92%)\ttrain_Loss: 0.004662\tval_Loss: 0.005162\n",
      "Train Epoch: 9194/10000 (92%)\ttrain_Loss: 0.004661\tval_Loss: 0.005163\n",
      "Train Epoch: 9195/10000 (92%)\ttrain_Loss: 0.004660\tval_Loss: 0.005180\n",
      "Train Epoch: 9196/10000 (92%)\ttrain_Loss: 0.004661\tval_Loss: 0.005149\n",
      "Train Epoch: 9197/10000 (92%)\ttrain_Loss: 0.004662\tval_Loss: 0.005195\n",
      "Train Epoch: 9198/10000 (92%)\ttrain_Loss: 0.004664\tval_Loss: 0.005140\n",
      "Train Epoch: 9199/10000 (92%)\ttrain_Loss: 0.004666\tval_Loss: 0.005207\n",
      "Train Epoch: 9200/10000 (92%)\ttrain_Loss: 0.004667\tval_Loss: 0.005136\n",
      "Train Epoch: 9201/10000 (92%)\ttrain_Loss: 0.004669\tval_Loss: 0.005217\n",
      "Train Epoch: 9202/10000 (92%)\ttrain_Loss: 0.004670\tval_Loss: 0.005134\n",
      "Train Epoch: 9203/10000 (92%)\ttrain_Loss: 0.004671\tval_Loss: 0.005223\n",
      "Train Epoch: 9204/10000 (92%)\ttrain_Loss: 0.004671\tval_Loss: 0.005133\n",
      "Train Epoch: 9205/10000 (92%)\ttrain_Loss: 0.004671\tval_Loss: 0.005223\n",
      "Train Epoch: 9206/10000 (92%)\ttrain_Loss: 0.004671\tval_Loss: 0.005133\n",
      "Train Epoch: 9207/10000 (92%)\ttrain_Loss: 0.004671\tval_Loss: 0.005218\n",
      "Train Epoch: 9208/10000 (92%)\ttrain_Loss: 0.004670\tval_Loss: 0.005134\n",
      "Train Epoch: 9209/10000 (92%)\ttrain_Loss: 0.004669\tval_Loss: 0.005211\n",
      "Train Epoch: 9210/10000 (92%)\ttrain_Loss: 0.004667\tval_Loss: 0.005137\n",
      "Train Epoch: 9211/10000 (92%)\ttrain_Loss: 0.004666\tval_Loss: 0.005203\n",
      "Train Epoch: 9212/10000 (92%)\ttrain_Loss: 0.004665\tval_Loss: 0.005141\n",
      "Train Epoch: 9213/10000 (92%)\ttrain_Loss: 0.004663\tval_Loss: 0.005195\n",
      "Train Epoch: 9214/10000 (92%)\ttrain_Loss: 0.004662\tval_Loss: 0.005144\n",
      "Train Epoch: 9215/10000 (92%)\ttrain_Loss: 0.004661\tval_Loss: 0.005186\n",
      "Train Epoch: 9216/10000 (92%)\ttrain_Loss: 0.004661\tval_Loss: 0.005147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9217/10000 (92%)\ttrain_Loss: 0.004660\tval_Loss: 0.005180\n",
      "Train Epoch: 9218/10000 (92%)\ttrain_Loss: 0.004659\tval_Loss: 0.005150\n",
      "Train Epoch: 9219/10000 (92%)\ttrain_Loss: 0.004659\tval_Loss: 0.005176\n",
      "Train Epoch: 9220/10000 (92%)\ttrain_Loss: 0.004659\tval_Loss: 0.005152\n",
      "Train Epoch: 9221/10000 (92%)\ttrain_Loss: 0.004658\tval_Loss: 0.005173\n",
      "Train Epoch: 9222/10000 (92%)\ttrain_Loss: 0.004658\tval_Loss: 0.005153\n",
      "Train Epoch: 9223/10000 (92%)\ttrain_Loss: 0.004658\tval_Loss: 0.005172\n",
      "Train Epoch: 9224/10000 (92%)\ttrain_Loss: 0.004658\tval_Loss: 0.005153\n",
      "Train Epoch: 9225/10000 (92%)\ttrain_Loss: 0.004658\tval_Loss: 0.005172\n",
      "Train Epoch: 9226/10000 (92%)\ttrain_Loss: 0.004658\tval_Loss: 0.005152\n",
      "Train Epoch: 9227/10000 (92%)\ttrain_Loss: 0.004658\tval_Loss: 0.005173\n",
      "Train Epoch: 9228/10000 (92%)\ttrain_Loss: 0.004658\tval_Loss: 0.005149\n",
      "Train Epoch: 9229/10000 (92%)\ttrain_Loss: 0.004658\tval_Loss: 0.005176\n",
      "Train Epoch: 9230/10000 (92%)\ttrain_Loss: 0.004658\tval_Loss: 0.005145\n",
      "Train Epoch: 9231/10000 (92%)\ttrain_Loss: 0.004658\tval_Loss: 0.005185\n",
      "Train Epoch: 9232/10000 (92%)\ttrain_Loss: 0.004659\tval_Loss: 0.005139\n",
      "Train Epoch: 9233/10000 (92%)\ttrain_Loss: 0.004660\tval_Loss: 0.005200\n",
      "Train Epoch: 9234/10000 (92%)\ttrain_Loss: 0.004662\tval_Loss: 0.005131\n",
      "Train Epoch: 9235/10000 (92%)\ttrain_Loss: 0.004665\tval_Loss: 0.005224\n",
      "Train Epoch: 9236/10000 (92%)\ttrain_Loss: 0.004670\tval_Loss: 0.005123\n",
      "Train Epoch: 9237/10000 (92%)\ttrain_Loss: 0.004677\tval_Loss: 0.005264\n",
      "Train Epoch: 9238/10000 (92%)\ttrain_Loss: 0.004686\tval_Loss: 0.005119\n",
      "Train Epoch: 9239/10000 (92%)\ttrain_Loss: 0.004700\tval_Loss: 0.005325\n",
      "Train Epoch: 9240/10000 (92%)\ttrain_Loss: 0.004717\tval_Loss: 0.005127\n",
      "Train Epoch: 9241/10000 (92%)\ttrain_Loss: 0.004739\tval_Loss: 0.005406\n",
      "Train Epoch: 9242/10000 (92%)\ttrain_Loss: 0.004764\tval_Loss: 0.005146\n",
      "Train Epoch: 9243/10000 (92%)\ttrain_Loss: 0.004786\tval_Loss: 0.005456\n",
      "Train Epoch: 9244/10000 (92%)\ttrain_Loss: 0.004795\tval_Loss: 0.005144\n",
      "Train Epoch: 9245/10000 (92%)\ttrain_Loss: 0.004783\tval_Loss: 0.005379\n",
      "Train Epoch: 9246/10000 (92%)\ttrain_Loss: 0.004747\tval_Loss: 0.005119\n",
      "Train Epoch: 9247/10000 (92%)\ttrain_Loss: 0.004701\tval_Loss: 0.005214\n",
      "Train Epoch: 9248/10000 (92%)\ttrain_Loss: 0.004666\tval_Loss: 0.005165\n",
      "Train Epoch: 9249/10000 (92%)\ttrain_Loss: 0.004655\tval_Loss: 0.005126\n",
      "Train Epoch: 9250/10000 (92%)\ttrain_Loss: 0.004668\tval_Loss: 0.005274\n",
      "Train Epoch: 9251/10000 (92%)\ttrain_Loss: 0.004690\tval_Loss: 0.005118\n",
      "Train Epoch: 9252/10000 (93%)\ttrain_Loss: 0.004706\tval_Loss: 0.005304\n",
      "Train Epoch: 9253/10000 (93%)\ttrain_Loss: 0.004705\tval_Loss: 0.005118\n",
      "Train Epoch: 9254/10000 (93%)\ttrain_Loss: 0.004689\tval_Loss: 0.005222\n",
      "Train Epoch: 9255/10000 (93%)\ttrain_Loss: 0.004668\tval_Loss: 0.005145\n",
      "Train Epoch: 9256/10000 (93%)\ttrain_Loss: 0.004655\tval_Loss: 0.005141\n",
      "Train Epoch: 9257/10000 (93%)\ttrain_Loss: 0.004656\tval_Loss: 0.005215\n",
      "Train Epoch: 9258/10000 (93%)\ttrain_Loss: 0.004666\tval_Loss: 0.005118\n",
      "Train Epoch: 9259/10000 (93%)\ttrain_Loss: 0.004676\tval_Loss: 0.005247\n",
      "Train Epoch: 9260/10000 (93%)\ttrain_Loss: 0.004679\tval_Loss: 0.005119\n",
      "Train Epoch: 9261/10000 (93%)\ttrain_Loss: 0.004673\tval_Loss: 0.005205\n",
      "Train Epoch: 9262/10000 (93%)\ttrain_Loss: 0.004663\tval_Loss: 0.005142\n",
      "Train Epoch: 9263/10000 (93%)\ttrain_Loss: 0.004655\tval_Loss: 0.005149\n",
      "Train Epoch: 9264/10000 (93%)\ttrain_Loss: 0.004654\tval_Loss: 0.005188\n",
      "Train Epoch: 9265/10000 (93%)\ttrain_Loss: 0.004658\tval_Loss: 0.005125\n",
      "Train Epoch: 9266/10000 (93%)\ttrain_Loss: 0.004663\tval_Loss: 0.005214\n",
      "Train Epoch: 9267/10000 (93%)\ttrain_Loss: 0.004665\tval_Loss: 0.005123\n",
      "Train Epoch: 9268/10000 (93%)\ttrain_Loss: 0.004664\tval_Loss: 0.005193\n",
      "Train Epoch: 9269/10000 (93%)\ttrain_Loss: 0.004659\tval_Loss: 0.005139\n",
      "Train Epoch: 9270/10000 (93%)\ttrain_Loss: 0.004654\tval_Loss: 0.005156\n",
      "Train Epoch: 9271/10000 (93%)\ttrain_Loss: 0.004653\tval_Loss: 0.005170\n",
      "Train Epoch: 9272/10000 (93%)\ttrain_Loss: 0.004654\tval_Loss: 0.005134\n",
      "Train Epoch: 9273/10000 (93%)\ttrain_Loss: 0.004656\tval_Loss: 0.005191\n",
      "Train Epoch: 9274/10000 (93%)\ttrain_Loss: 0.004658\tval_Loss: 0.005129\n",
      "Train Epoch: 9275/10000 (93%)\ttrain_Loss: 0.004658\tval_Loss: 0.005186\n",
      "Train Epoch: 9276/10000 (93%)\ttrain_Loss: 0.004656\tval_Loss: 0.005138\n",
      "Train Epoch: 9277/10000 (93%)\ttrain_Loss: 0.004654\tval_Loss: 0.005163\n",
      "Train Epoch: 9278/10000 (93%)\ttrain_Loss: 0.004652\tval_Loss: 0.005156\n",
      "Train Epoch: 9279/10000 (93%)\ttrain_Loss: 0.004652\tval_Loss: 0.005143\n",
      "Train Epoch: 9280/10000 (93%)\ttrain_Loss: 0.004652\tval_Loss: 0.005172\n",
      "Train Epoch: 9281/10000 (93%)\ttrain_Loss: 0.004653\tval_Loss: 0.005134\n",
      "Train Epoch: 9282/10000 (93%)\ttrain_Loss: 0.004654\tval_Loss: 0.005178\n",
      "Train Epoch: 9283/10000 (93%)\ttrain_Loss: 0.004654\tval_Loss: 0.005135\n",
      "Train Epoch: 9284/10000 (93%)\ttrain_Loss: 0.004654\tval_Loss: 0.005171\n",
      "Train Epoch: 9285/10000 (93%)\ttrain_Loss: 0.004653\tval_Loss: 0.005145\n",
      "Train Epoch: 9286/10000 (93%)\ttrain_Loss: 0.004652\tval_Loss: 0.005157\n",
      "Train Epoch: 9287/10000 (93%)\ttrain_Loss: 0.004651\tval_Loss: 0.005157\n",
      "Train Epoch: 9288/10000 (93%)\ttrain_Loss: 0.004651\tval_Loss: 0.005145\n",
      "Train Epoch: 9289/10000 (93%)\ttrain_Loss: 0.004651\tval_Loss: 0.005166\n",
      "Train Epoch: 9290/10000 (93%)\ttrain_Loss: 0.004652\tval_Loss: 0.005139\n",
      "Train Epoch: 9291/10000 (93%)\ttrain_Loss: 0.004652\tval_Loss: 0.005168\n",
      "Train Epoch: 9292/10000 (93%)\ttrain_Loss: 0.004652\tval_Loss: 0.005140\n",
      "Train Epoch: 9293/10000 (93%)\ttrain_Loss: 0.004651\tval_Loss: 0.005164\n",
      "Train Epoch: 9294/10000 (93%)\ttrain_Loss: 0.004651\tval_Loss: 0.005146\n",
      "Train Epoch: 9295/10000 (93%)\ttrain_Loss: 0.004650\tval_Loss: 0.005156\n",
      "Train Epoch: 9296/10000 (93%)\ttrain_Loss: 0.004650\tval_Loss: 0.005152\n",
      "Train Epoch: 9297/10000 (93%)\ttrain_Loss: 0.004650\tval_Loss: 0.005148\n",
      "Train Epoch: 9298/10000 (93%)\ttrain_Loss: 0.004650\tval_Loss: 0.005156\n",
      "Train Epoch: 9299/10000 (93%)\ttrain_Loss: 0.004650\tval_Loss: 0.005142\n",
      "Train Epoch: 9300/10000 (93%)\ttrain_Loss: 0.004650\tval_Loss: 0.005159\n",
      "Train Epoch: 9301/10000 (93%)\ttrain_Loss: 0.004650\tval_Loss: 0.005141\n",
      "Train Epoch: 9302/10000 (93%)\ttrain_Loss: 0.004650\tval_Loss: 0.005160\n",
      "Train Epoch: 9303/10000 (93%)\ttrain_Loss: 0.004650\tval_Loss: 0.005142\n",
      "Train Epoch: 9304/10000 (93%)\ttrain_Loss: 0.004650\tval_Loss: 0.005158\n",
      "Train Epoch: 9305/10000 (93%)\ttrain_Loss: 0.004649\tval_Loss: 0.005144\n",
      "Train Epoch: 9306/10000 (93%)\ttrain_Loss: 0.004649\tval_Loss: 0.005155\n",
      "Train Epoch: 9307/10000 (93%)\ttrain_Loss: 0.004649\tval_Loss: 0.005146\n",
      "Train Epoch: 9308/10000 (93%)\ttrain_Loss: 0.004649\tval_Loss: 0.005151\n",
      "Train Epoch: 9309/10000 (93%)\ttrain_Loss: 0.004649\tval_Loss: 0.005149\n",
      "Train Epoch: 9310/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005148\n",
      "Train Epoch: 9311/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005152\n",
      "Train Epoch: 9312/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005145\n",
      "Train Epoch: 9313/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005153\n",
      "Train Epoch: 9314/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005143\n",
      "Train Epoch: 9315/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005156\n",
      "Train Epoch: 9316/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005141\n",
      "Train Epoch: 9317/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005158\n",
      "Train Epoch: 9318/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005140\n",
      "Train Epoch: 9319/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005159\n",
      "Train Epoch: 9320/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005137\n",
      "Train Epoch: 9321/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005160\n",
      "Train Epoch: 9322/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005136\n",
      "Train Epoch: 9323/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005162\n",
      "Train Epoch: 9324/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005135\n",
      "Train Epoch: 9325/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005163\n",
      "Train Epoch: 9326/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005133\n",
      "Train Epoch: 9327/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005164\n",
      "Train Epoch: 9328/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005131\n",
      "Train Epoch: 9329/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005166\n",
      "Train Epoch: 9330/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005129\n",
      "Train Epoch: 9331/10000 (93%)\ttrain_Loss: 0.004648\tval_Loss: 0.005169\n",
      "Train Epoch: 9332/10000 (93%)\ttrain_Loss: 0.004649\tval_Loss: 0.005127\n",
      "Train Epoch: 9333/10000 (93%)\ttrain_Loss: 0.004649\tval_Loss: 0.005174\n",
      "Train Epoch: 9334/10000 (93%)\ttrain_Loss: 0.004649\tval_Loss: 0.005123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9335/10000 (93%)\ttrain_Loss: 0.004650\tval_Loss: 0.005182\n",
      "Train Epoch: 9336/10000 (93%)\ttrain_Loss: 0.004651\tval_Loss: 0.005119\n",
      "Train Epoch: 9337/10000 (93%)\ttrain_Loss: 0.004652\tval_Loss: 0.005193\n",
      "Train Epoch: 9338/10000 (93%)\ttrain_Loss: 0.004654\tval_Loss: 0.005113\n",
      "Train Epoch: 9339/10000 (93%)\ttrain_Loss: 0.004657\tval_Loss: 0.005210\n",
      "Train Epoch: 9340/10000 (93%)\ttrain_Loss: 0.004660\tval_Loss: 0.005108\n",
      "Train Epoch: 9341/10000 (93%)\ttrain_Loss: 0.004664\tval_Loss: 0.005236\n",
      "Train Epoch: 9342/10000 (93%)\ttrain_Loss: 0.004670\tval_Loss: 0.005104\n",
      "Train Epoch: 9343/10000 (93%)\ttrain_Loss: 0.004677\tval_Loss: 0.005269\n",
      "Train Epoch: 9344/10000 (93%)\ttrain_Loss: 0.004686\tval_Loss: 0.005103\n",
      "Train Epoch: 9345/10000 (93%)\ttrain_Loss: 0.004695\tval_Loss: 0.005306\n",
      "Train Epoch: 9346/10000 (93%)\ttrain_Loss: 0.004705\tval_Loss: 0.005106\n",
      "Train Epoch: 9347/10000 (93%)\ttrain_Loss: 0.004712\tval_Loss: 0.005325\n",
      "Train Epoch: 9348/10000 (93%)\ttrain_Loss: 0.004715\tval_Loss: 0.005105\n",
      "Train Epoch: 9349/10000 (93%)\ttrain_Loss: 0.004712\tval_Loss: 0.005299\n",
      "Train Epoch: 9350/10000 (93%)\ttrain_Loss: 0.004701\tval_Loss: 0.005102\n",
      "Train Epoch: 9351/10000 (94%)\ttrain_Loss: 0.004686\tval_Loss: 0.005233\n",
      "Train Epoch: 9352/10000 (94%)\ttrain_Loss: 0.004669\tval_Loss: 0.005113\n",
      "Train Epoch: 9353/10000 (94%)\ttrain_Loss: 0.004654\tval_Loss: 0.005164\n",
      "Train Epoch: 9354/10000 (94%)\ttrain_Loss: 0.004646\tval_Loss: 0.005150\n",
      "Train Epoch: 9355/10000 (94%)\ttrain_Loss: 0.004644\tval_Loss: 0.005121\n",
      "Train Epoch: 9356/10000 (94%)\ttrain_Loss: 0.004648\tval_Loss: 0.005195\n",
      "Train Epoch: 9357/10000 (94%)\ttrain_Loss: 0.004654\tval_Loss: 0.005105\n",
      "Train Epoch: 9358/10000 (94%)\ttrain_Loss: 0.004661\tval_Loss: 0.005223\n",
      "Train Epoch: 9359/10000 (94%)\ttrain_Loss: 0.004665\tval_Loss: 0.005102\n",
      "Train Epoch: 9360/10000 (94%)\ttrain_Loss: 0.004666\tval_Loss: 0.005220\n",
      "Train Epoch: 9361/10000 (94%)\ttrain_Loss: 0.004663\tval_Loss: 0.005107\n",
      "Train Epoch: 9362/10000 (94%)\ttrain_Loss: 0.004658\tval_Loss: 0.005191\n",
      "Train Epoch: 9363/10000 (94%)\ttrain_Loss: 0.004652\tval_Loss: 0.005119\n",
      "Train Epoch: 9364/10000 (94%)\ttrain_Loss: 0.004647\tval_Loss: 0.005155\n",
      "Train Epoch: 9365/10000 (94%)\ttrain_Loss: 0.004644\tval_Loss: 0.005141\n",
      "Train Epoch: 9366/10000 (94%)\ttrain_Loss: 0.004643\tval_Loss: 0.005128\n",
      "Train Epoch: 9367/10000 (94%)\ttrain_Loss: 0.004644\tval_Loss: 0.005166\n",
      "Train Epoch: 9368/10000 (94%)\ttrain_Loss: 0.004646\tval_Loss: 0.005115\n",
      "Train Epoch: 9369/10000 (94%)\ttrain_Loss: 0.004648\tval_Loss: 0.005184\n",
      "Train Epoch: 9370/10000 (94%)\ttrain_Loss: 0.004650\tval_Loss: 0.005110\n",
      "Train Epoch: 9371/10000 (94%)\ttrain_Loss: 0.004651\tval_Loss: 0.005190\n",
      "Train Epoch: 9372/10000 (94%)\ttrain_Loss: 0.004652\tval_Loss: 0.005111\n",
      "Train Epoch: 9373/10000 (94%)\ttrain_Loss: 0.004651\tval_Loss: 0.005181\n",
      "Train Epoch: 9374/10000 (94%)\ttrain_Loss: 0.004649\tval_Loss: 0.005117\n",
      "Train Epoch: 9375/10000 (94%)\ttrain_Loss: 0.004647\tval_Loss: 0.005164\n",
      "Train Epoch: 9376/10000 (94%)\ttrain_Loss: 0.004645\tval_Loss: 0.005128\n",
      "Train Epoch: 9377/10000 (94%)\ttrain_Loss: 0.004643\tval_Loss: 0.005147\n",
      "Train Epoch: 9378/10000 (94%)\ttrain_Loss: 0.004642\tval_Loss: 0.005141\n",
      "Train Epoch: 9379/10000 (94%)\ttrain_Loss: 0.004642\tval_Loss: 0.005133\n",
      "Train Epoch: 9380/10000 (94%)\ttrain_Loss: 0.004642\tval_Loss: 0.005153\n",
      "Train Epoch: 9381/10000 (94%)\ttrain_Loss: 0.004642\tval_Loss: 0.005124\n",
      "Train Epoch: 9382/10000 (94%)\ttrain_Loss: 0.004643\tval_Loss: 0.005161\n",
      "Train Epoch: 9383/10000 (94%)\ttrain_Loss: 0.004644\tval_Loss: 0.005119\n",
      "Train Epoch: 9384/10000 (94%)\ttrain_Loss: 0.004644\tval_Loss: 0.005164\n",
      "Train Epoch: 9385/10000 (94%)\ttrain_Loss: 0.004644\tval_Loss: 0.005116\n",
      "Train Epoch: 9386/10000 (94%)\ttrain_Loss: 0.004644\tval_Loss: 0.005165\n",
      "Train Epoch: 9387/10000 (94%)\ttrain_Loss: 0.004644\tval_Loss: 0.005118\n",
      "Train Epoch: 9388/10000 (94%)\ttrain_Loss: 0.004644\tval_Loss: 0.005162\n",
      "Train Epoch: 9389/10000 (94%)\ttrain_Loss: 0.004643\tval_Loss: 0.005120\n",
      "Train Epoch: 9390/10000 (94%)\ttrain_Loss: 0.004643\tval_Loss: 0.005158\n",
      "Train Epoch: 9391/10000 (94%)\ttrain_Loss: 0.004642\tval_Loss: 0.005123\n",
      "Train Epoch: 9392/10000 (94%)\ttrain_Loss: 0.004642\tval_Loss: 0.005152\n",
      "Train Epoch: 9393/10000 (94%)\ttrain_Loss: 0.004641\tval_Loss: 0.005125\n",
      "Train Epoch: 9394/10000 (94%)\ttrain_Loss: 0.004641\tval_Loss: 0.005146\n",
      "Train Epoch: 9395/10000 (94%)\ttrain_Loss: 0.004641\tval_Loss: 0.005128\n",
      "Train Epoch: 9396/10000 (94%)\ttrain_Loss: 0.004640\tval_Loss: 0.005142\n",
      "Train Epoch: 9397/10000 (94%)\ttrain_Loss: 0.004640\tval_Loss: 0.005131\n",
      "Train Epoch: 9398/10000 (94%)\ttrain_Loss: 0.004640\tval_Loss: 0.005142\n",
      "Train Epoch: 9399/10000 (94%)\ttrain_Loss: 0.004640\tval_Loss: 0.005133\n",
      "Train Epoch: 9400/10000 (94%)\ttrain_Loss: 0.004640\tval_Loss: 0.005140\n",
      "Train Epoch: 9401/10000 (94%)\ttrain_Loss: 0.004639\tval_Loss: 0.005132\n",
      "Train Epoch: 9402/10000 (94%)\ttrain_Loss: 0.004639\tval_Loss: 0.005138\n",
      "Train Epoch: 9403/10000 (94%)\ttrain_Loss: 0.004639\tval_Loss: 0.005131\n",
      "Train Epoch: 9404/10000 (94%)\ttrain_Loss: 0.004639\tval_Loss: 0.005139\n",
      "Train Epoch: 9405/10000 (94%)\ttrain_Loss: 0.004639\tval_Loss: 0.005131\n",
      "Train Epoch: 9406/10000 (94%)\ttrain_Loss: 0.004639\tval_Loss: 0.005141\n",
      "Train Epoch: 9407/10000 (94%)\ttrain_Loss: 0.004639\tval_Loss: 0.005130\n",
      "Train Epoch: 9408/10000 (94%)\ttrain_Loss: 0.004639\tval_Loss: 0.005144\n",
      "Train Epoch: 9409/10000 (94%)\ttrain_Loss: 0.004639\tval_Loss: 0.005126\n",
      "Train Epoch: 9410/10000 (94%)\ttrain_Loss: 0.004639\tval_Loss: 0.005147\n",
      "Train Epoch: 9411/10000 (94%)\ttrain_Loss: 0.004639\tval_Loss: 0.005120\n",
      "Train Epoch: 9412/10000 (94%)\ttrain_Loss: 0.004640\tval_Loss: 0.005156\n",
      "Train Epoch: 9413/10000 (94%)\ttrain_Loss: 0.004640\tval_Loss: 0.005113\n",
      "Train Epoch: 9414/10000 (94%)\ttrain_Loss: 0.004641\tval_Loss: 0.005170\n",
      "Train Epoch: 9415/10000 (94%)\ttrain_Loss: 0.004643\tval_Loss: 0.005106\n",
      "Train Epoch: 9416/10000 (94%)\ttrain_Loss: 0.004646\tval_Loss: 0.005192\n",
      "Train Epoch: 9417/10000 (94%)\ttrain_Loss: 0.004650\tval_Loss: 0.005097\n",
      "Train Epoch: 9418/10000 (94%)\ttrain_Loss: 0.004656\tval_Loss: 0.005232\n",
      "Train Epoch: 9419/10000 (94%)\ttrain_Loss: 0.004666\tval_Loss: 0.005091\n",
      "Train Epoch: 9420/10000 (94%)\ttrain_Loss: 0.004680\tval_Loss: 0.005296\n",
      "Train Epoch: 9421/10000 (94%)\ttrain_Loss: 0.004698\tval_Loss: 0.005099\n",
      "Train Epoch: 9422/10000 (94%)\ttrain_Loss: 0.004724\tval_Loss: 0.005388\n",
      "Train Epoch: 9423/10000 (94%)\ttrain_Loss: 0.004752\tval_Loss: 0.005123\n",
      "Train Epoch: 9424/10000 (94%)\ttrain_Loss: 0.004781\tval_Loss: 0.005461\n",
      "Train Epoch: 9425/10000 (94%)\ttrain_Loss: 0.004798\tval_Loss: 0.005130\n",
      "Train Epoch: 9426/10000 (94%)\ttrain_Loss: 0.004791\tval_Loss: 0.005396\n",
      "Train Epoch: 9427/10000 (94%)\ttrain_Loss: 0.004755\tval_Loss: 0.005096\n",
      "Train Epoch: 9428/10000 (94%)\ttrain_Loss: 0.004702\tval_Loss: 0.005207\n",
      "Train Epoch: 9429/10000 (94%)\ttrain_Loss: 0.004655\tval_Loss: 0.005131\n",
      "Train Epoch: 9430/10000 (94%)\ttrain_Loss: 0.004637\tval_Loss: 0.005098\n",
      "Train Epoch: 9431/10000 (94%)\ttrain_Loss: 0.004650\tval_Loss: 0.005257\n",
      "Train Epoch: 9432/10000 (94%)\ttrain_Loss: 0.004677\tval_Loss: 0.005094\n",
      "Train Epoch: 9433/10000 (94%)\ttrain_Loss: 0.004699\tval_Loss: 0.005296\n",
      "Train Epoch: 9434/10000 (94%)\ttrain_Loss: 0.004697\tval_Loss: 0.005092\n",
      "Train Epoch: 9435/10000 (94%)\ttrain_Loss: 0.004676\tval_Loss: 0.005195\n",
      "Train Epoch: 9436/10000 (94%)\ttrain_Loss: 0.004650\tval_Loss: 0.005123\n",
      "Train Epoch: 9437/10000 (94%)\ttrain_Loss: 0.004637\tval_Loss: 0.005108\n",
      "Train Epoch: 9438/10000 (94%)\ttrain_Loss: 0.004641\tval_Loss: 0.005206\n",
      "Train Epoch: 9439/10000 (94%)\ttrain_Loss: 0.004655\tval_Loss: 0.005091\n",
      "Train Epoch: 9440/10000 (94%)\ttrain_Loss: 0.004666\tval_Loss: 0.005233\n",
      "Train Epoch: 9441/10000 (94%)\ttrain_Loss: 0.004666\tval_Loss: 0.005094\n",
      "Train Epoch: 9442/10000 (94%)\ttrain_Loss: 0.004655\tval_Loss: 0.005170\n",
      "Train Epoch: 9443/10000 (94%)\ttrain_Loss: 0.004642\tval_Loss: 0.005125\n",
      "Train Epoch: 9444/10000 (94%)\ttrain_Loss: 0.004636\tval_Loss: 0.005111\n",
      "Train Epoch: 9445/10000 (94%)\ttrain_Loss: 0.004638\tval_Loss: 0.005179\n",
      "Train Epoch: 9446/10000 (94%)\ttrain_Loss: 0.004645\tval_Loss: 0.005094\n",
      "Train Epoch: 9447/10000 (94%)\ttrain_Loss: 0.004650\tval_Loss: 0.005191\n",
      "Train Epoch: 9448/10000 (94%)\ttrain_Loss: 0.004649\tval_Loss: 0.005100\n",
      "Train Epoch: 9449/10000 (94%)\ttrain_Loss: 0.004643\tval_Loss: 0.005151\n",
      "Train Epoch: 9450/10000 (94%)\ttrain_Loss: 0.004637\tval_Loss: 0.005128\n",
      "Train Epoch: 9451/10000 (94%)\ttrain_Loss: 0.004635\tval_Loss: 0.005114\n",
      "Train Epoch: 9452/10000 (95%)\ttrain_Loss: 0.004637\tval_Loss: 0.005165\n",
      "Train Epoch: 9453/10000 (95%)\ttrain_Loss: 0.004640\tval_Loss: 0.005101\n",
      "Train Epoch: 9454/10000 (95%)\ttrain_Loss: 0.004643\tval_Loss: 0.005174\n",
      "Train Epoch: 9455/10000 (95%)\ttrain_Loss: 0.004643\tval_Loss: 0.005105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9456/10000 (95%)\ttrain_Loss: 0.004640\tval_Loss: 0.005152\n",
      "Train Epoch: 9457/10000 (95%)\ttrain_Loss: 0.004637\tval_Loss: 0.005123\n",
      "Train Epoch: 9458/10000 (95%)\ttrain_Loss: 0.004634\tval_Loss: 0.005126\n",
      "Train Epoch: 9459/10000 (95%)\ttrain_Loss: 0.004634\tval_Loss: 0.005146\n",
      "Train Epoch: 9460/10000 (95%)\ttrain_Loss: 0.004635\tval_Loss: 0.005111\n",
      "Train Epoch: 9461/10000 (95%)\ttrain_Loss: 0.004637\tval_Loss: 0.005160\n",
      "Train Epoch: 9462/10000 (95%)\ttrain_Loss: 0.004638\tval_Loss: 0.005107\n",
      "Train Epoch: 9463/10000 (95%)\ttrain_Loss: 0.004638\tval_Loss: 0.005154\n",
      "Train Epoch: 9464/10000 (95%)\ttrain_Loss: 0.004637\tval_Loss: 0.005113\n",
      "Train Epoch: 9465/10000 (95%)\ttrain_Loss: 0.004635\tval_Loss: 0.005135\n",
      "Train Epoch: 9466/10000 (95%)\ttrain_Loss: 0.004634\tval_Loss: 0.005128\n",
      "Train Epoch: 9467/10000 (95%)\ttrain_Loss: 0.004633\tval_Loss: 0.005118\n",
      "Train Epoch: 9468/10000 (95%)\ttrain_Loss: 0.004634\tval_Loss: 0.005142\n",
      "Train Epoch: 9469/10000 (95%)\ttrain_Loss: 0.004634\tval_Loss: 0.005112\n",
      "Train Epoch: 9470/10000 (95%)\ttrain_Loss: 0.004635\tval_Loss: 0.005150\n",
      "Train Epoch: 9471/10000 (95%)\ttrain_Loss: 0.004635\tval_Loss: 0.005112\n",
      "Train Epoch: 9472/10000 (95%)\ttrain_Loss: 0.004635\tval_Loss: 0.005147\n",
      "Train Epoch: 9473/10000 (95%)\ttrain_Loss: 0.004634\tval_Loss: 0.005116\n",
      "Train Epoch: 9474/10000 (95%)\ttrain_Loss: 0.004634\tval_Loss: 0.005136\n",
      "Train Epoch: 9475/10000 (95%)\ttrain_Loss: 0.004633\tval_Loss: 0.005122\n",
      "Train Epoch: 9476/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005127\n",
      "Train Epoch: 9477/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005129\n",
      "Train Epoch: 9478/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005122\n",
      "Train Epoch: 9479/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005133\n",
      "Train Epoch: 9480/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005119\n",
      "Train Epoch: 9481/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005135\n",
      "Train Epoch: 9482/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005117\n",
      "Train Epoch: 9483/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005137\n",
      "Train Epoch: 9484/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005117\n",
      "Train Epoch: 9485/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005137\n",
      "Train Epoch: 9486/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005116\n",
      "Train Epoch: 9487/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005136\n",
      "Train Epoch: 9488/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005116\n",
      "Train Epoch: 9489/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005135\n",
      "Train Epoch: 9490/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005116\n",
      "Train Epoch: 9491/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005132\n",
      "Train Epoch: 9492/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005118\n",
      "Train Epoch: 9493/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005127\n",
      "Train Epoch: 9494/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005122\n",
      "Train Epoch: 9495/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005123\n",
      "Train Epoch: 9496/10000 (95%)\ttrain_Loss: 0.004630\tval_Loss: 0.005128\n",
      "Train Epoch: 9497/10000 (95%)\ttrain_Loss: 0.004630\tval_Loss: 0.005120\n",
      "Train Epoch: 9498/10000 (95%)\ttrain_Loss: 0.004630\tval_Loss: 0.005132\n",
      "Train Epoch: 9499/10000 (95%)\ttrain_Loss: 0.004630\tval_Loss: 0.005116\n",
      "Train Epoch: 9500/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005136\n",
      "Train Epoch: 9501/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005111\n",
      "Train Epoch: 9502/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005137\n",
      "Train Epoch: 9503/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005108\n",
      "Train Epoch: 9504/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005137\n",
      "Train Epoch: 9505/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005108\n",
      "Train Epoch: 9506/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005139\n",
      "Train Epoch: 9507/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005110\n",
      "Train Epoch: 9508/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005141\n",
      "Train Epoch: 9509/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005109\n",
      "Train Epoch: 9510/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005142\n",
      "Train Epoch: 9511/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005105\n",
      "Train Epoch: 9512/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005144\n",
      "Train Epoch: 9513/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005102\n",
      "Train Epoch: 9514/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005149\n",
      "Train Epoch: 9515/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005100\n",
      "Train Epoch: 9516/10000 (95%)\ttrain_Loss: 0.004633\tval_Loss: 0.005155\n",
      "Train Epoch: 9517/10000 (95%)\ttrain_Loss: 0.004633\tval_Loss: 0.005097\n",
      "Train Epoch: 9518/10000 (95%)\ttrain_Loss: 0.004634\tval_Loss: 0.005158\n",
      "Train Epoch: 9519/10000 (95%)\ttrain_Loss: 0.004634\tval_Loss: 0.005095\n",
      "Train Epoch: 9520/10000 (95%)\ttrain_Loss: 0.004634\tval_Loss: 0.005156\n",
      "Train Epoch: 9521/10000 (95%)\ttrain_Loss: 0.004634\tval_Loss: 0.005094\n",
      "Train Epoch: 9522/10000 (95%)\ttrain_Loss: 0.004634\tval_Loss: 0.005155\n",
      "Train Epoch: 9523/10000 (95%)\ttrain_Loss: 0.004633\tval_Loss: 0.005096\n",
      "Train Epoch: 9524/10000 (95%)\ttrain_Loss: 0.004633\tval_Loss: 0.005153\n",
      "Train Epoch: 9525/10000 (95%)\ttrain_Loss: 0.004633\tval_Loss: 0.005098\n",
      "Train Epoch: 9526/10000 (95%)\ttrain_Loss: 0.004632\tval_Loss: 0.005149\n",
      "Train Epoch: 9527/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005099\n",
      "Train Epoch: 9528/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005144\n",
      "Train Epoch: 9529/10000 (95%)\ttrain_Loss: 0.004630\tval_Loss: 0.005100\n",
      "Train Epoch: 9530/10000 (95%)\ttrain_Loss: 0.004630\tval_Loss: 0.005144\n",
      "Train Epoch: 9531/10000 (95%)\ttrain_Loss: 0.004630\tval_Loss: 0.005098\n",
      "Train Epoch: 9532/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005151\n",
      "Train Epoch: 9533/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005094\n",
      "Train Epoch: 9534/10000 (95%)\ttrain_Loss: 0.004633\tval_Loss: 0.005163\n",
      "Train Epoch: 9535/10000 (95%)\ttrain_Loss: 0.004635\tval_Loss: 0.005087\n",
      "Train Epoch: 9536/10000 (95%)\ttrain_Loss: 0.004637\tval_Loss: 0.005183\n",
      "Train Epoch: 9537/10000 (95%)\ttrain_Loss: 0.004642\tval_Loss: 0.005081\n",
      "Train Epoch: 9538/10000 (95%)\ttrain_Loss: 0.004647\tval_Loss: 0.005214\n",
      "Train Epoch: 9539/10000 (95%)\ttrain_Loss: 0.004654\tval_Loss: 0.005078\n",
      "Train Epoch: 9540/10000 (95%)\ttrain_Loss: 0.004663\tval_Loss: 0.005257\n",
      "Train Epoch: 9541/10000 (95%)\ttrain_Loss: 0.004674\tval_Loss: 0.005079\n",
      "Train Epoch: 9542/10000 (95%)\ttrain_Loss: 0.004687\tval_Loss: 0.005315\n",
      "Train Epoch: 9543/10000 (95%)\ttrain_Loss: 0.004705\tval_Loss: 0.005090\n",
      "Train Epoch: 9544/10000 (95%)\ttrain_Loss: 0.004721\tval_Loss: 0.005357\n",
      "Train Epoch: 9545/10000 (95%)\ttrain_Loss: 0.004730\tval_Loss: 0.005089\n",
      "Train Epoch: 9546/10000 (95%)\ttrain_Loss: 0.004727\tval_Loss: 0.005319\n",
      "Train Epoch: 9547/10000 (95%)\ttrain_Loss: 0.004709\tval_Loss: 0.005074\n",
      "Train Epoch: 9548/10000 (95%)\ttrain_Loss: 0.004681\tval_Loss: 0.005208\n",
      "Train Epoch: 9549/10000 (95%)\ttrain_Loss: 0.004652\tval_Loss: 0.005090\n",
      "Train Epoch: 9550/10000 (95%)\ttrain_Loss: 0.004631\tval_Loss: 0.005114\n",
      "Train Epoch: 9551/10000 (96%)\ttrain_Loss: 0.004625\tval_Loss: 0.005159\n",
      "Train Epoch: 9552/10000 (96%)\ttrain_Loss: 0.004632\tval_Loss: 0.005080\n",
      "Train Epoch: 9553/10000 (96%)\ttrain_Loss: 0.004645\tval_Loss: 0.005223\n",
      "Train Epoch: 9554/10000 (96%)\ttrain_Loss: 0.004657\tval_Loss: 0.005074\n",
      "Train Epoch: 9555/10000 (96%)\ttrain_Loss: 0.004662\tval_Loss: 0.005223\n",
      "Train Epoch: 9556/10000 (96%)\ttrain_Loss: 0.004658\tval_Loss: 0.005074\n",
      "Train Epoch: 9557/10000 (96%)\ttrain_Loss: 0.004647\tval_Loss: 0.005166\n",
      "Train Epoch: 9558/10000 (96%)\ttrain_Loss: 0.004635\tval_Loss: 0.005096\n",
      "Train Epoch: 9559/10000 (96%)\ttrain_Loss: 0.004627\tval_Loss: 0.005110\n",
      "Train Epoch: 9560/10000 (96%)\ttrain_Loss: 0.004625\tval_Loss: 0.005142\n",
      "Train Epoch: 9561/10000 (96%)\ttrain_Loss: 0.004628\tval_Loss: 0.005084\n",
      "Train Epoch: 9562/10000 (96%)\ttrain_Loss: 0.004634\tval_Loss: 0.005177\n",
      "Train Epoch: 9563/10000 (96%)\ttrain_Loss: 0.004639\tval_Loss: 0.005077\n",
      "Train Epoch: 9564/10000 (96%)\ttrain_Loss: 0.004640\tval_Loss: 0.005174\n",
      "Train Epoch: 9565/10000 (96%)\ttrain_Loss: 0.004638\tval_Loss: 0.005082\n",
      "Train Epoch: 9566/10000 (96%)\ttrain_Loss: 0.004633\tval_Loss: 0.005142\n",
      "Train Epoch: 9567/10000 (96%)\ttrain_Loss: 0.004628\tval_Loss: 0.005101\n",
      "Train Epoch: 9568/10000 (96%)\ttrain_Loss: 0.004625\tval_Loss: 0.005110\n",
      "Train Epoch: 9569/10000 (96%)\ttrain_Loss: 0.004624\tval_Loss: 0.005129\n",
      "Train Epoch: 9570/10000 (96%)\ttrain_Loss: 0.004625\tval_Loss: 0.005092\n",
      "Train Epoch: 9571/10000 (96%)\ttrain_Loss: 0.004627\tval_Loss: 0.005149\n",
      "Train Epoch: 9572/10000 (96%)\ttrain_Loss: 0.004629\tval_Loss: 0.005085\n",
      "Train Epoch: 9573/10000 (96%)\ttrain_Loss: 0.004630\tval_Loss: 0.005150\n",
      "Train Epoch: 9574/10000 (96%)\ttrain_Loss: 0.004629\tval_Loss: 0.005088\n",
      "Train Epoch: 9575/10000 (96%)\ttrain_Loss: 0.004628\tval_Loss: 0.005135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9576/10000 (96%)\ttrain_Loss: 0.004626\tval_Loss: 0.005100\n",
      "Train Epoch: 9577/10000 (96%)\ttrain_Loss: 0.004624\tval_Loss: 0.005117\n",
      "Train Epoch: 9578/10000 (96%)\ttrain_Loss: 0.004623\tval_Loss: 0.005116\n",
      "Train Epoch: 9579/10000 (96%)\ttrain_Loss: 0.004623\tval_Loss: 0.005102\n",
      "Train Epoch: 9580/10000 (96%)\ttrain_Loss: 0.004624\tval_Loss: 0.005129\n",
      "Train Epoch: 9581/10000 (96%)\ttrain_Loss: 0.004624\tval_Loss: 0.005094\n",
      "Train Epoch: 9582/10000 (96%)\ttrain_Loss: 0.004625\tval_Loss: 0.005135\n",
      "Train Epoch: 9583/10000 (96%)\ttrain_Loss: 0.004625\tval_Loss: 0.005092\n",
      "Train Epoch: 9584/10000 (96%)\ttrain_Loss: 0.004625\tval_Loss: 0.005134\n",
      "Train Epoch: 9585/10000 (96%)\ttrain_Loss: 0.004625\tval_Loss: 0.005095\n",
      "Train Epoch: 9586/10000 (96%)\ttrain_Loss: 0.004624\tval_Loss: 0.005127\n",
      "Train Epoch: 9587/10000 (96%)\ttrain_Loss: 0.004623\tval_Loss: 0.005101\n",
      "Train Epoch: 9588/10000 (96%)\ttrain_Loss: 0.004623\tval_Loss: 0.005117\n",
      "Train Epoch: 9589/10000 (96%)\ttrain_Loss: 0.004622\tval_Loss: 0.005109\n",
      "Train Epoch: 9590/10000 (96%)\ttrain_Loss: 0.004622\tval_Loss: 0.005108\n",
      "Train Epoch: 9591/10000 (96%)\ttrain_Loss: 0.004622\tval_Loss: 0.005116\n",
      "Train Epoch: 9592/10000 (96%)\ttrain_Loss: 0.004622\tval_Loss: 0.005101\n",
      "Train Epoch: 9593/10000 (96%)\ttrain_Loss: 0.004622\tval_Loss: 0.005121\n",
      "Train Epoch: 9594/10000 (96%)\ttrain_Loss: 0.004622\tval_Loss: 0.005097\n",
      "Train Epoch: 9595/10000 (96%)\ttrain_Loss: 0.004622\tval_Loss: 0.005124\n",
      "Train Epoch: 9596/10000 (96%)\ttrain_Loss: 0.004623\tval_Loss: 0.005096\n",
      "Train Epoch: 9597/10000 (96%)\ttrain_Loss: 0.004623\tval_Loss: 0.005125\n",
      "Train Epoch: 9598/10000 (96%)\ttrain_Loss: 0.004622\tval_Loss: 0.005097\n",
      "Train Epoch: 9599/10000 (96%)\ttrain_Loss: 0.004622\tval_Loss: 0.005123\n",
      "Train Epoch: 9600/10000 (96%)\ttrain_Loss: 0.004622\tval_Loss: 0.005099\n",
      "Train Epoch: 9601/10000 (96%)\ttrain_Loss: 0.004622\tval_Loss: 0.005120\n",
      "Train Epoch: 9602/10000 (96%)\ttrain_Loss: 0.004621\tval_Loss: 0.005100\n",
      "Train Epoch: 9603/10000 (96%)\ttrain_Loss: 0.004621\tval_Loss: 0.005116\n",
      "Train Epoch: 9604/10000 (96%)\ttrain_Loss: 0.004621\tval_Loss: 0.005102\n",
      "Train Epoch: 9605/10000 (96%)\ttrain_Loss: 0.004621\tval_Loss: 0.005113\n",
      "Train Epoch: 9606/10000 (96%)\ttrain_Loss: 0.004621\tval_Loss: 0.005104\n",
      "Train Epoch: 9607/10000 (96%)\ttrain_Loss: 0.004620\tval_Loss: 0.005111\n",
      "Train Epoch: 9608/10000 (96%)\ttrain_Loss: 0.004620\tval_Loss: 0.005105\n",
      "Train Epoch: 9609/10000 (96%)\ttrain_Loss: 0.004620\tval_Loss: 0.005110\n",
      "Train Epoch: 9610/10000 (96%)\ttrain_Loss: 0.004620\tval_Loss: 0.005105\n",
      "Train Epoch: 9611/10000 (96%)\ttrain_Loss: 0.004620\tval_Loss: 0.005109\n",
      "Train Epoch: 9612/10000 (96%)\ttrain_Loss: 0.004620\tval_Loss: 0.005105\n",
      "Train Epoch: 9613/10000 (96%)\ttrain_Loss: 0.004620\tval_Loss: 0.005109\n",
      "Train Epoch: 9614/10000 (96%)\ttrain_Loss: 0.004620\tval_Loss: 0.005105\n",
      "Train Epoch: 9615/10000 (96%)\ttrain_Loss: 0.004620\tval_Loss: 0.005109\n",
      "Train Epoch: 9616/10000 (96%)\ttrain_Loss: 0.004620\tval_Loss: 0.005104\n",
      "Train Epoch: 9617/10000 (96%)\ttrain_Loss: 0.004619\tval_Loss: 0.005109\n",
      "Train Epoch: 9618/10000 (96%)\ttrain_Loss: 0.004619\tval_Loss: 0.005103\n",
      "Train Epoch: 9619/10000 (96%)\ttrain_Loss: 0.004619\tval_Loss: 0.005110\n",
      "Train Epoch: 9620/10000 (96%)\ttrain_Loss: 0.004619\tval_Loss: 0.005102\n",
      "Train Epoch: 9621/10000 (96%)\ttrain_Loss: 0.004619\tval_Loss: 0.005111\n",
      "Train Epoch: 9622/10000 (96%)\ttrain_Loss: 0.004619\tval_Loss: 0.005101\n",
      "Train Epoch: 9623/10000 (96%)\ttrain_Loss: 0.004619\tval_Loss: 0.005113\n",
      "Train Epoch: 9624/10000 (96%)\ttrain_Loss: 0.004619\tval_Loss: 0.005097\n",
      "Train Epoch: 9625/10000 (96%)\ttrain_Loss: 0.004619\tval_Loss: 0.005116\n",
      "Train Epoch: 9626/10000 (96%)\ttrain_Loss: 0.004619\tval_Loss: 0.005093\n",
      "Train Epoch: 9627/10000 (96%)\ttrain_Loss: 0.004619\tval_Loss: 0.005121\n",
      "Train Epoch: 9628/10000 (96%)\ttrain_Loss: 0.004620\tval_Loss: 0.005088\n",
      "Train Epoch: 9629/10000 (96%)\ttrain_Loss: 0.004620\tval_Loss: 0.005131\n",
      "Train Epoch: 9630/10000 (96%)\ttrain_Loss: 0.004621\tval_Loss: 0.005081\n",
      "Train Epoch: 9631/10000 (96%)\ttrain_Loss: 0.004623\tval_Loss: 0.005147\n",
      "Train Epoch: 9632/10000 (96%)\ttrain_Loss: 0.004625\tval_Loss: 0.005072\n",
      "Train Epoch: 9633/10000 (96%)\ttrain_Loss: 0.004629\tval_Loss: 0.005177\n",
      "Train Epoch: 9634/10000 (96%)\ttrain_Loss: 0.004636\tval_Loss: 0.005063\n",
      "Train Epoch: 9635/10000 (96%)\ttrain_Loss: 0.004646\tval_Loss: 0.005236\n",
      "Train Epoch: 9636/10000 (96%)\ttrain_Loss: 0.004662\tval_Loss: 0.005066\n",
      "Train Epoch: 9637/10000 (96%)\ttrain_Loss: 0.004686\tval_Loss: 0.005335\n",
      "Train Epoch: 9638/10000 (96%)\ttrain_Loss: 0.004717\tval_Loss: 0.005092\n",
      "Train Epoch: 9639/10000 (96%)\ttrain_Loss: 0.004758\tval_Loss: 0.005465\n",
      "Train Epoch: 9640/10000 (96%)\ttrain_Loss: 0.004800\tval_Loss: 0.005130\n",
      "Train Epoch: 9641/10000 (96%)\ttrain_Loss: 0.004831\tval_Loss: 0.005514\n",
      "Train Epoch: 9642/10000 (96%)\ttrain_Loss: 0.004832\tval_Loss: 0.005108\n",
      "Train Epoch: 9643/10000 (96%)\ttrain_Loss: 0.004785\tval_Loss: 0.005316\n",
      "Train Epoch: 9644/10000 (96%)\ttrain_Loss: 0.004704\tval_Loss: 0.005068\n",
      "Train Epoch: 9645/10000 (96%)\ttrain_Loss: 0.004636\tval_Loss: 0.005093\n",
      "Train Epoch: 9646/10000 (96%)\ttrain_Loss: 0.004618\tval_Loss: 0.005208\n",
      "Train Epoch: 9647/10000 (96%)\ttrain_Loss: 0.004648\tval_Loss: 0.005065\n",
      "Train Epoch: 9648/10000 (96%)\ttrain_Loss: 0.004691\tval_Loss: 0.005322\n",
      "Train Epoch: 9649/10000 (96%)\ttrain_Loss: 0.004709\tval_Loss: 0.005065\n",
      "Train Epoch: 9650/10000 (96%)\ttrain_Loss: 0.004688\tval_Loss: 0.005205\n",
      "Train Epoch: 9651/10000 (96%)\ttrain_Loss: 0.004646\tval_Loss: 0.005088\n",
      "Train Epoch: 9652/10000 (97%)\ttrain_Loss: 0.004619\tval_Loss: 0.005078\n",
      "Train Epoch: 9653/10000 (97%)\ttrain_Loss: 0.004623\tval_Loss: 0.005205\n",
      "Train Epoch: 9654/10000 (97%)\ttrain_Loss: 0.004646\tval_Loss: 0.005061\n",
      "Train Epoch: 9655/10000 (97%)\ttrain_Loss: 0.004663\tval_Loss: 0.005226\n",
      "Train Epoch: 9656/10000 (97%)\ttrain_Loss: 0.004657\tval_Loss: 0.005063\n",
      "Train Epoch: 9657/10000 (97%)\ttrain_Loss: 0.004636\tval_Loss: 0.005123\n",
      "Train Epoch: 9658/10000 (97%)\ttrain_Loss: 0.004619\tval_Loss: 0.005122\n",
      "Train Epoch: 9659/10000 (97%)\ttrain_Loss: 0.004618\tval_Loss: 0.005066\n",
      "Train Epoch: 9660/10000 (97%)\ttrain_Loss: 0.004630\tval_Loss: 0.005190\n",
      "Train Epoch: 9661/10000 (97%)\ttrain_Loss: 0.004640\tval_Loss: 0.005063\n",
      "Train Epoch: 9662/10000 (97%)\ttrain_Loss: 0.004638\tval_Loss: 0.005157\n",
      "Train Epoch: 9663/10000 (97%)\ttrain_Loss: 0.004627\tval_Loss: 0.005086\n",
      "Train Epoch: 9664/10000 (97%)\ttrain_Loss: 0.004617\tval_Loss: 0.005089\n",
      "Train Epoch: 9665/10000 (97%)\ttrain_Loss: 0.004616\tval_Loss: 0.005141\n",
      "Train Epoch: 9666/10000 (97%)\ttrain_Loss: 0.004623\tval_Loss: 0.005066\n",
      "Train Epoch: 9667/10000 (97%)\ttrain_Loss: 0.004628\tval_Loss: 0.005158\n",
      "Train Epoch: 9668/10000 (97%)\ttrain_Loss: 0.004628\tval_Loss: 0.005072\n",
      "Train Epoch: 9669/10000 (97%)\ttrain_Loss: 0.004622\tval_Loss: 0.005117\n",
      "Train Epoch: 9670/10000 (97%)\ttrain_Loss: 0.004617\tval_Loss: 0.005106\n",
      "Train Epoch: 9671/10000 (97%)\ttrain_Loss: 0.004615\tval_Loss: 0.005081\n",
      "Train Epoch: 9672/10000 (97%)\ttrain_Loss: 0.004618\tval_Loss: 0.005139\n",
      "Train Epoch: 9673/10000 (97%)\ttrain_Loss: 0.004621\tval_Loss: 0.005072\n",
      "Train Epoch: 9674/10000 (97%)\ttrain_Loss: 0.004622\tval_Loss: 0.005132\n",
      "Train Epoch: 9675/10000 (97%)\ttrain_Loss: 0.004619\tval_Loss: 0.005085\n",
      "Train Epoch: 9676/10000 (97%)\ttrain_Loss: 0.004616\tval_Loss: 0.005100\n",
      "Train Epoch: 9677/10000 (97%)\ttrain_Loss: 0.004615\tval_Loss: 0.005112\n",
      "Train Epoch: 9678/10000 (97%)\ttrain_Loss: 0.004616\tval_Loss: 0.005080\n",
      "Train Epoch: 9679/10000 (97%)\ttrain_Loss: 0.004617\tval_Loss: 0.005128\n",
      "Train Epoch: 9680/10000 (97%)\ttrain_Loss: 0.004618\tval_Loss: 0.005078\n",
      "Train Epoch: 9681/10000 (97%)\ttrain_Loss: 0.004618\tval_Loss: 0.005116\n",
      "Train Epoch: 9682/10000 (97%)\ttrain_Loss: 0.004616\tval_Loss: 0.005092\n",
      "Train Epoch: 9683/10000 (97%)\ttrain_Loss: 0.004614\tval_Loss: 0.005095\n",
      "Train Epoch: 9684/10000 (97%)\ttrain_Loss: 0.004614\tval_Loss: 0.005111\n",
      "Train Epoch: 9685/10000 (97%)\ttrain_Loss: 0.004615\tval_Loss: 0.005083\n",
      "Train Epoch: 9686/10000 (97%)\ttrain_Loss: 0.004616\tval_Loss: 0.005118\n",
      "Train Epoch: 9687/10000 (97%)\ttrain_Loss: 0.004616\tval_Loss: 0.005083\n",
      "Train Epoch: 9688/10000 (97%)\ttrain_Loss: 0.004615\tval_Loss: 0.005108\n",
      "Train Epoch: 9689/10000 (97%)\ttrain_Loss: 0.004614\tval_Loss: 0.005094\n",
      "Train Epoch: 9690/10000 (97%)\ttrain_Loss: 0.004614\tval_Loss: 0.005094\n",
      "Train Epoch: 9691/10000 (97%)\ttrain_Loss: 0.004614\tval_Loss: 0.005107\n",
      "Train Epoch: 9692/10000 (97%)\ttrain_Loss: 0.004614\tval_Loss: 0.005086\n",
      "Train Epoch: 9693/10000 (97%)\ttrain_Loss: 0.004614\tval_Loss: 0.005112\n",
      "Train Epoch: 9694/10000 (97%)\ttrain_Loss: 0.004614\tval_Loss: 0.005086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9695/10000 (97%)\ttrain_Loss: 0.004614\tval_Loss: 0.005106\n",
      "Train Epoch: 9696/10000 (97%)\ttrain_Loss: 0.004613\tval_Loss: 0.005093\n",
      "Train Epoch: 9697/10000 (97%)\ttrain_Loss: 0.004613\tval_Loss: 0.005095\n",
      "Train Epoch: 9698/10000 (97%)\ttrain_Loss: 0.004613\tval_Loss: 0.005101\n",
      "Train Epoch: 9699/10000 (97%)\ttrain_Loss: 0.004613\tval_Loss: 0.005089\n",
      "Train Epoch: 9700/10000 (97%)\ttrain_Loss: 0.004613\tval_Loss: 0.005105\n",
      "Train Epoch: 9701/10000 (97%)\ttrain_Loss: 0.004613\tval_Loss: 0.005088\n",
      "Train Epoch: 9702/10000 (97%)\ttrain_Loss: 0.004613\tval_Loss: 0.005104\n",
      "Train Epoch: 9703/10000 (97%)\ttrain_Loss: 0.004613\tval_Loss: 0.005092\n",
      "Train Epoch: 9704/10000 (97%)\ttrain_Loss: 0.004613\tval_Loss: 0.005098\n",
      "Train Epoch: 9705/10000 (97%)\ttrain_Loss: 0.004612\tval_Loss: 0.005097\n",
      "Train Epoch: 9706/10000 (97%)\ttrain_Loss: 0.004612\tval_Loss: 0.005092\n",
      "Train Epoch: 9707/10000 (97%)\ttrain_Loss: 0.004612\tval_Loss: 0.005101\n",
      "Train Epoch: 9708/10000 (97%)\ttrain_Loss: 0.004612\tval_Loss: 0.005089\n",
      "Train Epoch: 9709/10000 (97%)\ttrain_Loss: 0.004612\tval_Loss: 0.005101\n",
      "Train Epoch: 9710/10000 (97%)\ttrain_Loss: 0.004612\tval_Loss: 0.005090\n",
      "Train Epoch: 9711/10000 (97%)\ttrain_Loss: 0.004612\tval_Loss: 0.005099\n",
      "Train Epoch: 9712/10000 (97%)\ttrain_Loss: 0.004612\tval_Loss: 0.005093\n",
      "Train Epoch: 9713/10000 (97%)\ttrain_Loss: 0.004612\tval_Loss: 0.005096\n",
      "Train Epoch: 9714/10000 (97%)\ttrain_Loss: 0.004612\tval_Loss: 0.005096\n",
      "Train Epoch: 9715/10000 (97%)\ttrain_Loss: 0.004611\tval_Loss: 0.005093\n",
      "Train Epoch: 9716/10000 (97%)\ttrain_Loss: 0.004611\tval_Loss: 0.005098\n",
      "Train Epoch: 9717/10000 (97%)\ttrain_Loss: 0.004611\tval_Loss: 0.005090\n",
      "Train Epoch: 9718/10000 (97%)\ttrain_Loss: 0.004611\tval_Loss: 0.005098\n",
      "Train Epoch: 9719/10000 (97%)\ttrain_Loss: 0.004611\tval_Loss: 0.005090\n",
      "Train Epoch: 9720/10000 (97%)\ttrain_Loss: 0.004611\tval_Loss: 0.005097\n",
      "Train Epoch: 9721/10000 (97%)\ttrain_Loss: 0.004611\tval_Loss: 0.005092\n",
      "Train Epoch: 9722/10000 (97%)\ttrain_Loss: 0.004611\tval_Loss: 0.005095\n",
      "Train Epoch: 9723/10000 (97%)\ttrain_Loss: 0.004611\tval_Loss: 0.005093\n",
      "Train Epoch: 9724/10000 (97%)\ttrain_Loss: 0.004611\tval_Loss: 0.005093\n",
      "Train Epoch: 9725/10000 (97%)\ttrain_Loss: 0.004611\tval_Loss: 0.005094\n",
      "Train Epoch: 9726/10000 (97%)\ttrain_Loss: 0.004610\tval_Loss: 0.005091\n",
      "Train Epoch: 9727/10000 (97%)\ttrain_Loss: 0.004610\tval_Loss: 0.005094\n",
      "Train Epoch: 9728/10000 (97%)\ttrain_Loss: 0.004610\tval_Loss: 0.005091\n",
      "Train Epoch: 9729/10000 (97%)\ttrain_Loss: 0.004610\tval_Loss: 0.005095\n",
      "Train Epoch: 9730/10000 (97%)\ttrain_Loss: 0.004610\tval_Loss: 0.005091\n",
      "Train Epoch: 9731/10000 (97%)\ttrain_Loss: 0.004610\tval_Loss: 0.005095\n",
      "Train Epoch: 9732/10000 (97%)\ttrain_Loss: 0.004610\tval_Loss: 0.005091\n",
      "Train Epoch: 9733/10000 (97%)\ttrain_Loss: 0.004610\tval_Loss: 0.005094\n",
      "Train Epoch: 9734/10000 (97%)\ttrain_Loss: 0.004610\tval_Loss: 0.005091\n",
      "Train Epoch: 9735/10000 (97%)\ttrain_Loss: 0.004610\tval_Loss: 0.005092\n",
      "Train Epoch: 9736/10000 (97%)\ttrain_Loss: 0.004610\tval_Loss: 0.005091\n",
      "Train Epoch: 9737/10000 (97%)\ttrain_Loss: 0.004610\tval_Loss: 0.005093\n",
      "Train Epoch: 9738/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005091\n",
      "Train Epoch: 9739/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005094\n",
      "Train Epoch: 9740/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005090\n",
      "Train Epoch: 9741/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005094\n",
      "Train Epoch: 9742/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005089\n",
      "Train Epoch: 9743/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005094\n",
      "Train Epoch: 9744/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005088\n",
      "Train Epoch: 9745/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005094\n",
      "Train Epoch: 9746/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005087\n",
      "Train Epoch: 9747/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005095\n",
      "Train Epoch: 9748/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005087\n",
      "Train Epoch: 9749/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005095\n",
      "Train Epoch: 9750/10000 (97%)\ttrain_Loss: 0.004609\tval_Loss: 0.005086\n",
      "Train Epoch: 9751/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005095\n",
      "Train Epoch: 9752/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005085\n",
      "Train Epoch: 9753/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005095\n",
      "Train Epoch: 9754/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005084\n",
      "Train Epoch: 9755/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005095\n",
      "Train Epoch: 9756/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005083\n",
      "Train Epoch: 9757/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005096\n",
      "Train Epoch: 9758/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005081\n",
      "Train Epoch: 9759/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005098\n",
      "Train Epoch: 9760/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005079\n",
      "Train Epoch: 9761/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005101\n",
      "Train Epoch: 9762/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005077\n",
      "Train Epoch: 9763/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005105\n",
      "Train Epoch: 9764/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005073\n",
      "Train Epoch: 9765/10000 (98%)\ttrain_Loss: 0.004609\tval_Loss: 0.005110\n",
      "Train Epoch: 9766/10000 (98%)\ttrain_Loss: 0.004609\tval_Loss: 0.005068\n",
      "Train Epoch: 9767/10000 (98%)\ttrain_Loss: 0.004610\tval_Loss: 0.005118\n",
      "Train Epoch: 9768/10000 (98%)\ttrain_Loss: 0.004611\tval_Loss: 0.005062\n",
      "Train Epoch: 9769/10000 (98%)\ttrain_Loss: 0.004612\tval_Loss: 0.005133\n",
      "Train Epoch: 9770/10000 (98%)\ttrain_Loss: 0.004615\tval_Loss: 0.005055\n",
      "Train Epoch: 9771/10000 (98%)\ttrain_Loss: 0.004618\tval_Loss: 0.005155\n",
      "Train Epoch: 9772/10000 (98%)\ttrain_Loss: 0.004622\tval_Loss: 0.005048\n",
      "Train Epoch: 9773/10000 (98%)\ttrain_Loss: 0.004628\tval_Loss: 0.005191\n",
      "Train Epoch: 9774/10000 (98%)\ttrain_Loss: 0.004637\tval_Loss: 0.005045\n",
      "Train Epoch: 9775/10000 (98%)\ttrain_Loss: 0.004650\tval_Loss: 0.005245\n",
      "Train Epoch: 9776/10000 (98%)\ttrain_Loss: 0.004664\tval_Loss: 0.005050\n",
      "Train Epoch: 9777/10000 (98%)\ttrain_Loss: 0.004682\tval_Loss: 0.005309\n",
      "Train Epoch: 9778/10000 (98%)\ttrain_Loss: 0.004700\tval_Loss: 0.005062\n",
      "Train Epoch: 9779/10000 (98%)\ttrain_Loss: 0.004716\tval_Loss: 0.005349\n",
      "Train Epoch: 9780/10000 (98%)\ttrain_Loss: 0.004724\tval_Loss: 0.005064\n",
      "Train Epoch: 9781/10000 (98%)\ttrain_Loss: 0.004720\tval_Loss: 0.005308\n",
      "Train Epoch: 9782/10000 (98%)\ttrain_Loss: 0.004699\tval_Loss: 0.005048\n",
      "Train Epoch: 9783/10000 (98%)\ttrain_Loss: 0.004667\tval_Loss: 0.005185\n",
      "Train Epoch: 9784/10000 (98%)\ttrain_Loss: 0.004634\tval_Loss: 0.005060\n",
      "Train Epoch: 9785/10000 (98%)\ttrain_Loss: 0.004612\tval_Loss: 0.005080\n",
      "Train Epoch: 9786/10000 (98%)\ttrain_Loss: 0.004606\tval_Loss: 0.005134\n",
      "Train Epoch: 9787/10000 (98%)\ttrain_Loss: 0.004615\tval_Loss: 0.005046\n",
      "Train Epoch: 9788/10000 (98%)\ttrain_Loss: 0.004630\tval_Loss: 0.005206\n",
      "Train Epoch: 9789/10000 (98%)\ttrain_Loss: 0.004644\tval_Loss: 0.005045\n",
      "Train Epoch: 9790/10000 (98%)\ttrain_Loss: 0.004649\tval_Loss: 0.005204\n",
      "Train Epoch: 9791/10000 (98%)\ttrain_Loss: 0.004643\tval_Loss: 0.005047\n",
      "Train Epoch: 9792/10000 (98%)\ttrain_Loss: 0.004629\tval_Loss: 0.005137\n",
      "Train Epoch: 9793/10000 (98%)\ttrain_Loss: 0.004615\tval_Loss: 0.005071\n",
      "Train Epoch: 9794/10000 (98%)\ttrain_Loss: 0.004606\tval_Loss: 0.005074\n",
      "Train Epoch: 9795/10000 (98%)\ttrain_Loss: 0.004606\tval_Loss: 0.005122\n",
      "Train Epoch: 9796/10000 (98%)\ttrain_Loss: 0.004611\tval_Loss: 0.005050\n",
      "Train Epoch: 9797/10000 (98%)\ttrain_Loss: 0.004618\tval_Loss: 0.005157\n",
      "Train Epoch: 9798/10000 (98%)\ttrain_Loss: 0.004623\tval_Loss: 0.005048\n",
      "Train Epoch: 9799/10000 (98%)\ttrain_Loss: 0.004622\tval_Loss: 0.005147\n",
      "Train Epoch: 9800/10000 (98%)\ttrain_Loss: 0.004618\tval_Loss: 0.005058\n",
      "Train Epoch: 9801/10000 (98%)\ttrain_Loss: 0.004612\tval_Loss: 0.005105\n",
      "Train Epoch: 9802/10000 (98%)\ttrain_Loss: 0.004606\tval_Loss: 0.005083\n",
      "Train Epoch: 9803/10000 (98%)\ttrain_Loss: 0.004604\tval_Loss: 0.005069\n",
      "Train Epoch: 9804/10000 (98%)\ttrain_Loss: 0.004606\tval_Loss: 0.005114\n",
      "Train Epoch: 9805/10000 (98%)\ttrain_Loss: 0.004609\tval_Loss: 0.005054\n",
      "Train Epoch: 9806/10000 (98%)\ttrain_Loss: 0.004612\tval_Loss: 0.005132\n",
      "Train Epoch: 9807/10000 (98%)\ttrain_Loss: 0.004614\tval_Loss: 0.005054\n",
      "Train Epoch: 9808/10000 (98%)\ttrain_Loss: 0.004613\tval_Loss: 0.005126\n",
      "Train Epoch: 9809/10000 (98%)\ttrain_Loss: 0.004611\tval_Loss: 0.005063\n",
      "Train Epoch: 9810/10000 (98%)\ttrain_Loss: 0.004608\tval_Loss: 0.005105\n",
      "Train Epoch: 9811/10000 (98%)\ttrain_Loss: 0.004605\tval_Loss: 0.005079\n",
      "Train Epoch: 9812/10000 (98%)\ttrain_Loss: 0.004604\tval_Loss: 0.005081\n",
      "Train Epoch: 9813/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005098\n",
      "Train Epoch: 9814/10000 (98%)\ttrain_Loss: 0.004604\tval_Loss: 0.005066\n",
      "Train Epoch: 9815/10000 (98%)\ttrain_Loss: 0.004605\tval_Loss: 0.005110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9816/10000 (98%)\ttrain_Loss: 0.004607\tval_Loss: 0.005061\n",
      "Train Epoch: 9817/10000 (98%)\ttrain_Loss: 0.004607\tval_Loss: 0.005113\n",
      "Train Epoch: 9818/10000 (98%)\ttrain_Loss: 0.004607\tval_Loss: 0.005061\n",
      "Train Epoch: 9819/10000 (98%)\ttrain_Loss: 0.004606\tval_Loss: 0.005106\n",
      "Train Epoch: 9820/10000 (98%)\ttrain_Loss: 0.004606\tval_Loss: 0.005067\n",
      "Train Epoch: 9821/10000 (98%)\ttrain_Loss: 0.004604\tval_Loss: 0.005096\n",
      "Train Epoch: 9822/10000 (98%)\ttrain_Loss: 0.004604\tval_Loss: 0.005074\n",
      "Train Epoch: 9823/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005084\n",
      "Train Epoch: 9824/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005083\n",
      "Train Epoch: 9825/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005075\n",
      "Train Epoch: 9826/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005092\n",
      "Train Epoch: 9827/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005070\n",
      "Train Epoch: 9828/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005099\n",
      "Train Epoch: 9829/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005070\n",
      "Train Epoch: 9830/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005099\n",
      "Train Epoch: 9831/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005068\n",
      "Train Epoch: 9832/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005095\n",
      "Train Epoch: 9833/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005067\n",
      "Train Epoch: 9834/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005093\n",
      "Train Epoch: 9835/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005068\n",
      "Train Epoch: 9836/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005094\n",
      "Train Epoch: 9837/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005070\n",
      "Train Epoch: 9838/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005095\n",
      "Train Epoch: 9839/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005069\n",
      "Train Epoch: 9840/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005094\n",
      "Train Epoch: 9841/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005067\n",
      "Train Epoch: 9842/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005095\n",
      "Train Epoch: 9843/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005067\n",
      "Train Epoch: 9844/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005095\n",
      "Train Epoch: 9845/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005067\n",
      "Train Epoch: 9846/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005097\n",
      "Train Epoch: 9847/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005064\n",
      "Train Epoch: 9848/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005099\n",
      "Train Epoch: 9849/10000 (98%)\ttrain_Loss: 0.004602\tval_Loss: 0.005061\n",
      "Train Epoch: 9850/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005103\n",
      "Train Epoch: 9851/10000 (98%)\ttrain_Loss: 0.004603\tval_Loss: 0.005057\n",
      "Train Epoch: 9852/10000 (99%)\ttrain_Loss: 0.004604\tval_Loss: 0.005111\n",
      "Train Epoch: 9853/10000 (99%)\ttrain_Loss: 0.004605\tval_Loss: 0.005054\n",
      "Train Epoch: 9854/10000 (99%)\ttrain_Loss: 0.004606\tval_Loss: 0.005120\n",
      "Train Epoch: 9855/10000 (99%)\ttrain_Loss: 0.004607\tval_Loss: 0.005050\n",
      "Train Epoch: 9856/10000 (99%)\ttrain_Loss: 0.004608\tval_Loss: 0.005138\n",
      "Train Epoch: 9857/10000 (99%)\ttrain_Loss: 0.004612\tval_Loss: 0.005044\n",
      "Train Epoch: 9858/10000 (99%)\ttrain_Loss: 0.004617\tval_Loss: 0.005163\n",
      "Train Epoch: 9859/10000 (99%)\ttrain_Loss: 0.004622\tval_Loss: 0.005038\n",
      "Train Epoch: 9860/10000 (99%)\ttrain_Loss: 0.004627\tval_Loss: 0.005185\n",
      "Train Epoch: 9861/10000 (99%)\ttrain_Loss: 0.004633\tval_Loss: 0.005034\n",
      "Train Epoch: 9862/10000 (99%)\ttrain_Loss: 0.004638\tval_Loss: 0.005205\n",
      "Train Epoch: 9863/10000 (99%)\ttrain_Loss: 0.004643\tval_Loss: 0.005034\n",
      "Train Epoch: 9864/10000 (99%)\ttrain_Loss: 0.004645\tval_Loss: 0.005213\n",
      "Train Epoch: 9865/10000 (99%)\ttrain_Loss: 0.004646\tval_Loss: 0.005035\n",
      "Train Epoch: 9866/10000 (99%)\ttrain_Loss: 0.004644\tval_Loss: 0.005200\n",
      "Train Epoch: 9867/10000 (99%)\ttrain_Loss: 0.004639\tval_Loss: 0.005035\n",
      "Train Epoch: 9868/10000 (99%)\ttrain_Loss: 0.004632\tval_Loss: 0.005162\n",
      "Train Epoch: 9869/10000 (99%)\ttrain_Loss: 0.004622\tval_Loss: 0.005039\n",
      "Train Epoch: 9870/10000 (99%)\ttrain_Loss: 0.004613\tval_Loss: 0.005113\n",
      "Train Epoch: 9871/10000 (99%)\ttrain_Loss: 0.004605\tval_Loss: 0.005058\n",
      "Train Epoch: 9872/10000 (99%)\ttrain_Loss: 0.004600\tval_Loss: 0.005076\n",
      "Train Epoch: 9873/10000 (99%)\ttrain_Loss: 0.004599\tval_Loss: 0.005088\n",
      "Train Epoch: 9874/10000 (99%)\ttrain_Loss: 0.004599\tval_Loss: 0.005056\n",
      "Train Epoch: 9875/10000 (99%)\ttrain_Loss: 0.004602\tval_Loss: 0.005116\n",
      "Train Epoch: 9876/10000 (99%)\ttrain_Loss: 0.004605\tval_Loss: 0.005044\n",
      "Train Epoch: 9877/10000 (99%)\ttrain_Loss: 0.004608\tval_Loss: 0.005131\n",
      "Train Epoch: 9878/10000 (99%)\ttrain_Loss: 0.004610\tval_Loss: 0.005039\n",
      "Train Epoch: 9879/10000 (99%)\ttrain_Loss: 0.004611\tval_Loss: 0.005131\n",
      "Train Epoch: 9880/10000 (99%)\ttrain_Loss: 0.004611\tval_Loss: 0.005040\n",
      "Train Epoch: 9881/10000 (99%)\ttrain_Loss: 0.004609\tval_Loss: 0.005122\n",
      "Train Epoch: 9882/10000 (99%)\ttrain_Loss: 0.004607\tval_Loss: 0.005048\n",
      "Train Epoch: 9883/10000 (99%)\ttrain_Loss: 0.004604\tval_Loss: 0.005105\n",
      "Train Epoch: 9884/10000 (99%)\ttrain_Loss: 0.004602\tval_Loss: 0.005058\n",
      "Train Epoch: 9885/10000 (99%)\ttrain_Loss: 0.004600\tval_Loss: 0.005088\n",
      "Train Epoch: 9886/10000 (99%)\ttrain_Loss: 0.004598\tval_Loss: 0.005067\n",
      "Train Epoch: 9887/10000 (99%)\ttrain_Loss: 0.004598\tval_Loss: 0.005077\n",
      "Train Epoch: 9888/10000 (99%)\ttrain_Loss: 0.004597\tval_Loss: 0.005076\n",
      "Train Epoch: 9889/10000 (99%)\ttrain_Loss: 0.004597\tval_Loss: 0.005070\n",
      "Train Epoch: 9890/10000 (99%)\ttrain_Loss: 0.004597\tval_Loss: 0.005085\n",
      "Train Epoch: 9891/10000 (99%)\ttrain_Loss: 0.004598\tval_Loss: 0.005065\n",
      "Train Epoch: 9892/10000 (99%)\ttrain_Loss: 0.004598\tval_Loss: 0.005090\n",
      "Train Epoch: 9893/10000 (99%)\ttrain_Loss: 0.004598\tval_Loss: 0.005060\n",
      "Train Epoch: 9894/10000 (99%)\ttrain_Loss: 0.004598\tval_Loss: 0.005094\n",
      "Train Epoch: 9895/10000 (99%)\ttrain_Loss: 0.004599\tval_Loss: 0.005056\n",
      "Train Epoch: 9896/10000 (99%)\ttrain_Loss: 0.004599\tval_Loss: 0.005099\n",
      "Train Epoch: 9897/10000 (99%)\ttrain_Loss: 0.004600\tval_Loss: 0.005050\n",
      "Train Epoch: 9898/10000 (99%)\ttrain_Loss: 0.004601\tval_Loss: 0.005107\n",
      "Train Epoch: 9899/10000 (99%)\ttrain_Loss: 0.004602\tval_Loss: 0.005046\n",
      "Train Epoch: 9900/10000 (99%)\ttrain_Loss: 0.004603\tval_Loss: 0.005119\n",
      "Train Epoch: 9901/10000 (99%)\ttrain_Loss: 0.004605\tval_Loss: 0.005042\n",
      "Train Epoch: 9902/10000 (99%)\ttrain_Loss: 0.004607\tval_Loss: 0.005133\n",
      "Train Epoch: 9903/10000 (99%)\ttrain_Loss: 0.004609\tval_Loss: 0.005037\n",
      "Train Epoch: 9904/10000 (99%)\ttrain_Loss: 0.004613\tval_Loss: 0.005158\n",
      "Train Epoch: 9905/10000 (99%)\ttrain_Loss: 0.004619\tval_Loss: 0.005033\n",
      "Train Epoch: 9906/10000 (99%)\ttrain_Loss: 0.004626\tval_Loss: 0.005190\n",
      "Train Epoch: 9907/10000 (99%)\ttrain_Loss: 0.004633\tval_Loss: 0.005031\n",
      "Train Epoch: 9908/10000 (99%)\ttrain_Loss: 0.004640\tval_Loss: 0.005216\n",
      "Train Epoch: 9909/10000 (99%)\ttrain_Loss: 0.004647\tval_Loss: 0.005029\n",
      "Train Epoch: 9910/10000 (99%)\ttrain_Loss: 0.004651\tval_Loss: 0.005226\n",
      "Train Epoch: 9911/10000 (99%)\ttrain_Loss: 0.004653\tval_Loss: 0.005028\n",
      "Train Epoch: 9912/10000 (99%)\ttrain_Loss: 0.004651\tval_Loss: 0.005210\n",
      "Train Epoch: 9913/10000 (99%)\ttrain_Loss: 0.004644\tval_Loss: 0.005028\n",
      "Train Epoch: 9914/10000 (99%)\ttrain_Loss: 0.004634\tval_Loss: 0.005164\n",
      "Train Epoch: 9915/10000 (99%)\ttrain_Loss: 0.004622\tval_Loss: 0.005035\n",
      "Train Epoch: 9916/10000 (99%)\ttrain_Loss: 0.004610\tval_Loss: 0.005106\n",
      "Train Epoch: 9917/10000 (99%)\ttrain_Loss: 0.004601\tval_Loss: 0.005059\n",
      "Train Epoch: 9918/10000 (99%)\ttrain_Loss: 0.004596\tval_Loss: 0.005062\n",
      "Train Epoch: 9919/10000 (99%)\ttrain_Loss: 0.004595\tval_Loss: 0.005095\n",
      "Train Epoch: 9920/10000 (99%)\ttrain_Loss: 0.004598\tval_Loss: 0.005041\n",
      "Train Epoch: 9921/10000 (99%)\ttrain_Loss: 0.004603\tval_Loss: 0.005127\n",
      "Train Epoch: 9922/10000 (99%)\ttrain_Loss: 0.004607\tval_Loss: 0.005034\n",
      "Train Epoch: 9923/10000 (99%)\ttrain_Loss: 0.004610\tval_Loss: 0.005137\n",
      "Train Epoch: 9924/10000 (99%)\ttrain_Loss: 0.004611\tval_Loss: 0.005033\n",
      "Train Epoch: 9925/10000 (99%)\ttrain_Loss: 0.004610\tval_Loss: 0.005127\n",
      "Train Epoch: 9926/10000 (99%)\ttrain_Loss: 0.004607\tval_Loss: 0.005038\n",
      "Train Epoch: 9927/10000 (99%)\ttrain_Loss: 0.004603\tval_Loss: 0.005106\n",
      "Train Epoch: 9928/10000 (99%)\ttrain_Loss: 0.004600\tval_Loss: 0.005051\n",
      "Train Epoch: 9929/10000 (99%)\ttrain_Loss: 0.004597\tval_Loss: 0.005083\n",
      "Train Epoch: 9930/10000 (99%)\ttrain_Loss: 0.004595\tval_Loss: 0.005066\n",
      "Train Epoch: 9931/10000 (99%)\ttrain_Loss: 0.004594\tval_Loss: 0.005064\n",
      "Train Epoch: 9932/10000 (99%)\ttrain_Loss: 0.004594\tval_Loss: 0.005081\n",
      "Train Epoch: 9933/10000 (99%)\ttrain_Loss: 0.004595\tval_Loss: 0.005054\n",
      "Train Epoch: 9934/10000 (99%)\ttrain_Loss: 0.004596\tval_Loss: 0.005094\n",
      "Train Epoch: 9935/10000 (99%)\ttrain_Loss: 0.004597\tval_Loss: 0.005048\n",
      "Train Epoch: 9936/10000 (99%)\ttrain_Loss: 0.004598\tval_Loss: 0.005104\n",
      "Train Epoch: 9937/10000 (99%)\ttrain_Loss: 0.004599\tval_Loss: 0.005043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9938/10000 (99%)\ttrain_Loss: 0.004600\tval_Loss: 0.005117\n",
      "Train Epoch: 9939/10000 (99%)\ttrain_Loss: 0.004602\tval_Loss: 0.005038\n",
      "Train Epoch: 9940/10000 (99%)\ttrain_Loss: 0.004605\tval_Loss: 0.005130\n",
      "Train Epoch: 9941/10000 (99%)\ttrain_Loss: 0.004607\tval_Loss: 0.005033\n",
      "Train Epoch: 9942/10000 (99%)\ttrain_Loss: 0.004609\tval_Loss: 0.005135\n",
      "Train Epoch: 9943/10000 (99%)\ttrain_Loss: 0.004609\tval_Loss: 0.005031\n",
      "Train Epoch: 9944/10000 (99%)\ttrain_Loss: 0.004609\tval_Loss: 0.005132\n",
      "Train Epoch: 9945/10000 (99%)\ttrain_Loss: 0.004608\tval_Loss: 0.005033\n",
      "Train Epoch: 9946/10000 (99%)\ttrain_Loss: 0.004606\tval_Loss: 0.005123\n",
      "Train Epoch: 9947/10000 (99%)\ttrain_Loss: 0.004604\tval_Loss: 0.005038\n",
      "Train Epoch: 9948/10000 (99%)\ttrain_Loss: 0.004602\tval_Loss: 0.005107\n",
      "Train Epoch: 9949/10000 (99%)\ttrain_Loss: 0.004599\tval_Loss: 0.005045\n",
      "Train Epoch: 9950/10000 (99%)\ttrain_Loss: 0.004597\tval_Loss: 0.005086\n",
      "Train Epoch: 9951/10000 (100%)\ttrain_Loss: 0.004595\tval_Loss: 0.005055\n",
      "Train Epoch: 9952/10000 (100%)\ttrain_Loss: 0.004593\tval_Loss: 0.005068\n",
      "Train Epoch: 9953/10000 (100%)\ttrain_Loss: 0.004593\tval_Loss: 0.005068\n",
      "Train Epoch: 9954/10000 (100%)\ttrain_Loss: 0.004592\tval_Loss: 0.005057\n",
      "Train Epoch: 9955/10000 (100%)\ttrain_Loss: 0.004593\tval_Loss: 0.005080\n",
      "Train Epoch: 9956/10000 (100%)\ttrain_Loss: 0.004593\tval_Loss: 0.005049\n",
      "Train Epoch: 9957/10000 (100%)\ttrain_Loss: 0.004594\tval_Loss: 0.005093\n",
      "Train Epoch: 9958/10000 (100%)\ttrain_Loss: 0.004595\tval_Loss: 0.005044\n",
      "Train Epoch: 9959/10000 (100%)\ttrain_Loss: 0.004597\tval_Loss: 0.005110\n",
      "Train Epoch: 9960/10000 (100%)\ttrain_Loss: 0.004599\tval_Loss: 0.005038\n",
      "Train Epoch: 9961/10000 (100%)\ttrain_Loss: 0.004603\tval_Loss: 0.005139\n",
      "Train Epoch: 9962/10000 (100%)\ttrain_Loss: 0.004609\tval_Loss: 0.005030\n",
      "Train Epoch: 9963/10000 (100%)\ttrain_Loss: 0.004615\tval_Loss: 0.005169\n",
      "Train Epoch: 9964/10000 (100%)\ttrain_Loss: 0.004622\tval_Loss: 0.005022\n",
      "Train Epoch: 9965/10000 (100%)\ttrain_Loss: 0.004629\tval_Loss: 0.005194\n",
      "Train Epoch: 9966/10000 (100%)\ttrain_Loss: 0.004636\tval_Loss: 0.005020\n",
      "Train Epoch: 9967/10000 (100%)\ttrain_Loss: 0.004641\tval_Loss: 0.005207\n",
      "Train Epoch: 9968/10000 (100%)\ttrain_Loss: 0.004642\tval_Loss: 0.005022\n",
      "Train Epoch: 9969/10000 (100%)\ttrain_Loss: 0.004641\tval_Loss: 0.005196\n",
      "Train Epoch: 9970/10000 (100%)\ttrain_Loss: 0.004635\tval_Loss: 0.005023\n",
      "Train Epoch: 9971/10000 (100%)\ttrain_Loss: 0.004627\tval_Loss: 0.005156\n",
      "Train Epoch: 9972/10000 (100%)\ttrain_Loss: 0.004616\tval_Loss: 0.005029\n",
      "Train Epoch: 9973/10000 (100%)\ttrain_Loss: 0.004606\tval_Loss: 0.005106\n",
      "Train Epoch: 9974/10000 (100%)\ttrain_Loss: 0.004598\tval_Loss: 0.005046\n",
      "Train Epoch: 9975/10000 (100%)\ttrain_Loss: 0.004593\tval_Loss: 0.005066\n",
      "Train Epoch: 9976/10000 (100%)\ttrain_Loss: 0.004591\tval_Loss: 0.005075\n",
      "Train Epoch: 9977/10000 (100%)\ttrain_Loss: 0.004591\tval_Loss: 0.005044\n",
      "Train Epoch: 9978/10000 (100%)\ttrain_Loss: 0.004594\tval_Loss: 0.005105\n",
      "Train Epoch: 9979/10000 (100%)\ttrain_Loss: 0.004597\tval_Loss: 0.005034\n",
      "Train Epoch: 9980/10000 (100%)\ttrain_Loss: 0.004600\tval_Loss: 0.005124\n",
      "Train Epoch: 9981/10000 (100%)\ttrain_Loss: 0.004603\tval_Loss: 0.005029\n",
      "Train Epoch: 9982/10000 (100%)\ttrain_Loss: 0.004605\tval_Loss: 0.005127\n",
      "Train Epoch: 9983/10000 (100%)\ttrain_Loss: 0.004605\tval_Loss: 0.005029\n",
      "Train Epoch: 9984/10000 (100%)\ttrain_Loss: 0.004603\tval_Loss: 0.005116\n",
      "Train Epoch: 9985/10000 (100%)\ttrain_Loss: 0.004601\tval_Loss: 0.005034\n",
      "Train Epoch: 9986/10000 (100%)\ttrain_Loss: 0.004598\tval_Loss: 0.005097\n",
      "Train Epoch: 9987/10000 (100%)\ttrain_Loss: 0.004595\tval_Loss: 0.005046\n",
      "Train Epoch: 9988/10000 (100%)\ttrain_Loss: 0.004592\tval_Loss: 0.005077\n",
      "Train Epoch: 9989/10000 (100%)\ttrain_Loss: 0.004591\tval_Loss: 0.005061\n",
      "Train Epoch: 9990/10000 (100%)\ttrain_Loss: 0.004590\tval_Loss: 0.005060\n",
      "Train Epoch: 9991/10000 (100%)\ttrain_Loss: 0.004590\tval_Loss: 0.005076\n",
      "Train Epoch: 9992/10000 (100%)\ttrain_Loss: 0.004590\tval_Loss: 0.005049\n",
      "Train Epoch: 9993/10000 (100%)\ttrain_Loss: 0.004591\tval_Loss: 0.005091\n",
      "Train Epoch: 9994/10000 (100%)\ttrain_Loss: 0.004592\tval_Loss: 0.005043\n",
      "Train Epoch: 9995/10000 (100%)\ttrain_Loss: 0.004594\tval_Loss: 0.005107\n",
      "Train Epoch: 9996/10000 (100%)\ttrain_Loss: 0.004596\tval_Loss: 0.005035\n",
      "Train Epoch: 9997/10000 (100%)\ttrain_Loss: 0.004599\tval_Loss: 0.005131\n",
      "Train Epoch: 9998/10000 (100%)\ttrain_Loss: 0.004604\tval_Loss: 0.005027\n",
      "Train Epoch: 9999/10000 (100%)\ttrain_Loss: 0.004609\tval_Loss: 0.005153\n",
      "Train Epoch: 10000/10000 (100%)\ttrain_Loss: 0.004614\tval_Loss: 0.005021\n",
      "Train loss before training was: 0.8865188956260681\n",
      "Train loss after training is: 0.00461381021887064\n",
      "Val loss before training was: 0.8838438391685486\n",
      "Val loss after training is: 0.005020515061914921\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Training\n",
    "plt.figure() # monitor loss curve during training\n",
    "# for loop over epochs\n",
    "for epoch in range(num_epoch):\n",
    "    # classical forward pass -> predict new output from train data\n",
    "    Y_pred_train = net(X_train)\n",
    "    # compute loss    \n",
    "    loss_train = loss_func(Y_pred_train, Y_train)\n",
    "    \n",
    "    # Compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Calling .backward() mutiple times accumulates the gradient (by addition) for each parameter. This is why you should call optimizer.zero_grad() after each .step() call\n",
    "    # Note that following the first .backward call, a second call is only possible after you have performed another forward pass.\n",
    "    loss_train.backward()\n",
    "    # perform a parameter update based on the current gradient (stored in .grad attribute of a parameter)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # TODO**\n",
    "    # forward pass for validation\n",
    "    Y_pred_val = net(X_val)\n",
    "    loss_val = loss_func(Y_pred_val, Y_val)\n",
    "    # TODO**\n",
    "    \n",
    "    # plot train and val loss\n",
    "    plt.scatter(epoch, loss_train.data.item(), color='b', s=10, marker='o')    \n",
    "    plt.scatter(epoch, loss_val.data.item(), color='r', s=10, marker='o')\n",
    "    \n",
    "    # print message with actual losses\n",
    "    print('Train Epoch: {}/{} ({:.0f}%)\\ttrain_Loss: {:.6f}\\tval_Loss: {:.6f}'.format(\n",
    "    epoch+1, num_epoch, epoch/num_epoch*100, loss_train.item(), loss_val.item()))\n",
    "\n",
    "\n",
    "# show training and validation loss    \n",
    "plt.legend(['train-loss','val-loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(os.path.join(path, 'results/who_loss.png'))\n",
    "#plt.show()\n",
    "\n",
    "print('Train loss before training was:', loss_train_before.item())\n",
    "print('Train loss after training is:', loss_train.item())\n",
    "print('Val loss before training was:', loss_val_before.item())\n",
    "print('Val loss after training is:', loss_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eytU3EgRpowe"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Pred vs. Ref Figure Train/Val set\n",
    "# plot the prediction against the reference for the train/val points\n",
    "# if the prediction equals the reference the dots will appear at the 'perfect model' line\n",
    "plt.figure()\n",
    "plt.title('pred vs. ref: train/val points')\n",
    "plt.scatter(Y_train.cpu().numpy(), Y_pred_train.cpu().detach().numpy(), color='b', s=5, marker='o')\n",
    "plt.scatter(Y_val.cpu().numpy(), Y_pred_val.cpu().detach().numpy(), color='r', s=5, marker='o')\n",
    "plt.scatter(Y_val.cpu().numpy(), Y_pred_val_before.cpu().detach().numpy(), color='m', s=5, marker='^')\n",
    "plt.plot((0,1),(0,1), color='k')\n",
    "plt.xlabel('reference')\n",
    "plt.ylabel('prediction')\n",
    "plt.legend(['perfect model', 'train-sample after tr','val-sample after tr', 'val-sample before tr'])\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.savefig(os.path.join(path, 'results/who_pred_vs_ref_val.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SKwwQdL7powg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training was: 0.9002924561500549\n",
      "Test loss after training is: 0.004692372400313616\n",
      "Mean abs difference: [4.798153] years\n"
     ]
    }
   ],
   "source": [
    "#%% Test results\n",
    "# TODO**\n",
    "# forward pass \n",
    "# Y_pred_test_oh is on the GPU, because net and X_test are on the GPU, but we want it on the CPU from now on.\n",
    "Y_pred_test = net(X_test)\n",
    "loss_test = loss_func(Y_pred_test, Y_test)\n",
    "# TODO**\n",
    "print('Test loss before training was:', loss_test_before.item())\n",
    "print('Test loss after training is:', loss_test.item())\n",
    "\n",
    "# Plot mean abs difference between prediction and reference\n",
    "print('Mean abs difference:', np.mean(abs(Y_pred_test.cpu().detach().numpy()-Y_test.cpu().numpy()), axis=0)*scale_y, 'years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BoADQD3ipowk"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Pred vs. Ref Figure Test set\n",
    "# plot the prediction against the reference for the test points\n",
    "# if the prediction equals the reference the dots will appear at the 'perfect model' line\n",
    "plt.figure()\n",
    "plt.title('pred vs. ref: test points')\n",
    "plt.scatter(Y_test.cpu().numpy(), Y_pred_test.cpu().detach().numpy(), color='g', s=5, marker='o')\n",
    "plt.scatter(Y_test.cpu().numpy(), Y_pred_test_before.cpu().detach().numpy(), color='m', s=5, marker='^')\n",
    "plt.plot((0,1),(0,1), color='k')\n",
    "plt.xlabel('reference')\n",
    "plt.ylabel('prediction')\n",
    "plt.legend(['perfect model','test-sample after tr', 'test-sample before tr'])\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.savefig(os.path.join(path, 'results/who_pred_vs_ref_test.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
