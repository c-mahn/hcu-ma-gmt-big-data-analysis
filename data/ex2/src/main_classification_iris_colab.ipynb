{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rZ2v4qQjEcX_"
   },
   "outputs": [],
   "source": [
    "# Exercise_1\n",
    "# main simple classification Iris data\n",
    "# TODO**\n",
    "# Names of group members: Fabian Bloch and Christopher Mahn\n",
    "# Date: April 24th, 2023\n",
    "# TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ok0A5HyxEcYJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path of this exercise: /hdd/repository/hcu-ma-gmt-big-data-analysis/data/ex2\n"
     ]
    }
   ],
   "source": [
    "#%% Load modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import csv\n",
    "\n",
    "# torch modules\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# plot module\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# evaluate modele\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# module for interoperable file-operations\n",
    "import os\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "print(f'Path of this exercise: {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OGj-b40yEcYQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0 -- Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# %% CUDA for PyTorch\n",
    "# Right at the beginning: check if a cuda compatible GPU is available in your computer. \n",
    "# If so, set device = cuda:0 which means that later all calculations will be performed on the graphics card. \n",
    "# If no GPU is available, the calculations will run on the CPU, which is also absolutely sufficient for these exercises.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    device_num = 0\n",
    "    print('No GPU available.')\n",
    "else:\n",
    "    device_num = torch.cuda.device_count()\n",
    "    print('Device:', device, '-- Number of devices:', device_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sRMSsDMNEetF"
   },
   "outputs": [],
   "source": [
    "# Mounting Google Drive locally \n",
    "# from google.colab import drive\n",
    "#drive.mount(\"/content/drive\", force_remount=True)\n",
    "# drive.mount('/content/drive')\n",
    "# you can also choose one of the other options to load data\n",
    "# therefore see https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o7P9Je30EcYX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Größe des Datensatzes: 149\n",
      "Dimension des Datensatzes: 5\n"
     ]
    }
   ],
   "source": [
    "# %% read data\n",
    "# path to wine quality data\n",
    "data_path = os.path.join(path, 'data/iris_flowers/iris.data')\n",
    "\n",
    "# read csv sheet with pandas\n",
    "df = pd.read_csv(data_path, sep=',')\n",
    "\n",
    "# # drop nan data\n",
    "df = df.dropna()\n",
    "# drop each row where there is not at least 21 not nan data\n",
    "# df = df.dropna(thresh=21) # there are many nan in row 4 12 14, which can cause errors\n",
    "\n",
    "# get numpy out of pandas dataframe\n",
    "data = df.values\n",
    "# data=df.to_numpy()\n",
    "\n",
    "# get column names to see, which columns we have to extract as x and y\n",
    "column_names = np.array(df.columns[:], dtype=np.str_)\n",
    "\n",
    "# TODO**\n",
    "print(f'Größe des Datensatzes: {np.shape(data)[0]}')\n",
    "print(f'Dimension des Datensatzes: {np.shape(data)[1]}')\n",
    "# TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZBA_oVmrEcYc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (149, 4)\n",
      "y shape: (149,)\n",
      "number of classes 3\n"
     ]
    }
   ],
   "source": [
    "# %% split in X and Y\n",
    "# extract any feature you want as X \n",
    "# extract target values as Y\n",
    "x = np.array(data[:,:-1], dtype=np.float32)\n",
    "# x = np.array(data[:,6], dtype=np.float32)\n",
    "y = pd.factorize(data[:,-1])[0]\n",
    "\n",
    "class_names = np.unique(data[:,-1])\n",
    "\n",
    "# save number of classes\n",
    "nc = np.max(y)+1\n",
    "\n",
    "print('x shape:', x.shape)\n",
    "print('y shape:', y.shape)\n",
    "print('number of classes', nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3Gp1mNDeEcYi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale_x: [7.9 4.4 6.9 2.5]\n"
     ]
    }
   ],
   "source": [
    "# %% normalize X between (0,1). If multiple features in X are selected, each feature is normalized individually\n",
    "scale_x = np.max(x, axis=0)\n",
    "x = x/scale_x\n",
    "print('Scale_x:',scale_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "W7_CO8WTEcYr"
   },
   "outputs": [],
   "source": [
    "# %% convert to torch tensors\n",
    "# if tensors have only one dimension, an artificial dimension is created with unsqueeze (e.g. [10]->[10,1], so 1D->2D)\n",
    "Y = torch.from_numpy(y)\n",
    "Y = Y.long()\n",
    "\n",
    "# produce onehot target tensor\n",
    "# scatter_ mehtod fills the tensor with values from a source tensor along the indices provided as arguments\n",
    "# oh = one hot encoding\n",
    "Y_oh = torch.zeros(Y.shape[0], nc)\n",
    "Y_oh.scatter_(1,Y.unsqueeze(1), 1.0)\n",
    "\n",
    "X = torch.from_numpy(x)\n",
    "X = X.float()\n",
    "if len(X.shape)==1:\n",
    "    X = X.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "y5oQk8AfEcYv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input of first ten train Sample: tensor([[0.8481, 0.5682, 0.8406, 0.7200],\n",
      "        [0.7975, 0.6591, 0.8116, 0.7200],\n",
      "        [0.8861, 0.7273, 0.6812, 0.5600],\n",
      "        [0.5823, 0.7273, 0.2029, 0.0800],\n",
      "        [0.6582, 0.7955, 0.2174, 0.0800],\n",
      "        [0.7975, 0.6364, 0.7391, 0.6000],\n",
      "        [0.6962, 0.9545, 0.2029, 0.0800],\n",
      "        [0.7342, 0.6136, 0.5942, 0.4000],\n",
      "        [0.7215, 0.6591, 0.6087, 0.5200],\n",
      "        [0.8354, 0.6591, 0.6667, 0.5200]])\n",
      "Target of first ten train Sample: tensor([2, 2, 1, 0, 0, 2, 0, 1, 1, 1])\n",
      "One-Hot-Encoded Target of first ten train Sample: tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# %% Split dataset in training, validation and test tensors\n",
    "# TODO**\n",
    "prop_train = 0.5\n",
    "prop_val = 0.25\n",
    "prop_test = 0.25\n",
    "# TODO**\n",
    "\n",
    "sample_num = {'all': X.shape[0], \n",
    "              'train': round(prop_train*X.shape[0]),\n",
    "              'val': round(prop_val*X.shape[0]),\n",
    "              'test': round(prop_test*X.shape[0])}\n",
    "\n",
    "# idx shuffle\n",
    "idx = np.random.choice(sample_num['all'], sample_num['all'], replace=False)\n",
    "# assign idx to each sample\n",
    "sample_idx = {'all': idx[:], \n",
    "              'train': idx[0:sample_num['train']],\n",
    "              'val': idx[sample_num['train']:sample_num['train']+sample_num['val']],\n",
    "              'test': idx[sample_num['train']+sample_num['val']:]}\n",
    "\n",
    "# Create train data\n",
    "X_train = X[sample_idx['train']]\n",
    "Y_train_oh = Y_oh[sample_idx['train']]\n",
    "Y_train = Y[sample_idx['train']]\n",
    "\n",
    "# Create validation data\n",
    "X_val = X[sample_idx['val']]\n",
    "Y_val_oh = Y_oh[sample_idx['val']]\n",
    "Y_val = Y[sample_idx['val']]\n",
    "\n",
    "# Create test data\n",
    "X_test = X[sample_idx['test']]\n",
    "Y_test_oh = Y_oh[sample_idx['test']]\n",
    "Y_test = Y[sample_idx['test']]\n",
    "\n",
    "# Show data point\n",
    "print('Input of first ten train Sample:', X_train[0:10])\n",
    "print('Target of first ten train Sample:', Y_train[0:10])\n",
    "print('One-Hot-Encoded Target of first ten train Sample:', Y_train_oh[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dpVLx0wKEcYz"
   },
   "outputs": [],
   "source": [
    "#%% class of neural network 'ClassificationNet'\n",
    "# set up layer and architecture of network in constructor __init__\n",
    "# define operations on layer in forward pass method\n",
    "class ClassificationNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputSize, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, outputSize)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # max pooling over (2, 2) window\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DrZVTPROEcY3"
   },
   "outputs": [],
   "source": [
    "#%% Specify network parameter\n",
    "# TODO**  \n",
    "inputDim = 4\n",
    "outputDim = 3\n",
    " \n",
    "# Create instance of ClassificationNet\n",
    "net = ClassificationNet(inputDim, outputDim)\n",
    "# TODO** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IzMhr-6BEcY7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationNet(\n",
      "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#%% Send tensors and networks to GPU (if you have one which supports cuda) for faster computations\n",
    "# Note: Y is one-hot-encoded\n",
    "X_train, Y_train_oh = X_train.to(device), Y_train_oh.to(device)\n",
    "X_val, Y_val_oh = X_val.to(device), Y_val_oh.to(device)\n",
    "X_test, Y_test_oh = X_test.to(device), Y_test_oh.to(device)\n",
    "\n",
    "# The network itself must also be sent to the GPU. Either you write net = RegressNet() and then later net.to(device) or directly net = RegressNet().to(device)\n",
    "# The latter option may have the advantage that the instance net is created directly on the GPU, whereas in variant 1 it must first be sent to the GPU.\n",
    "if device_num>1:\n",
    "    print(\"Let's use\", device_num, \"GPU's\")\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device) \n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jM8VXCtcEcZA"
   },
   "outputs": [],
   "source": [
    "#%% Specify hyperparameter\n",
    "# hyperparemter: num_epoch, num_lr, loss_func, optimizer\n",
    "# TODO**  \n",
    "num_epoch = 1000\n",
    "num_lr = 1e-3\n",
    "# TODO**  \n",
    "# Loss and optimizer\n",
    "loss_func = nn.MSELoss() # -> one hot encoded 'target' to loss-function\n",
    "optimizer = optim.Adam(net.parameters(), lr=num_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MQar-KPtEcZE"
   },
   "outputs": [],
   "source": [
    "#%% Loss and Accuracy before training\n",
    "# Compute loss of test data before training the network (with random weights)\n",
    "Y_pred_test_before_oh = net(X_test)\n",
    "# loss function input looks as follows: loss_func(prediction, target)\n",
    "# Note: for CrossEntropyLoss(): prediction is one_hot_encoded, target has single dimension\n",
    "# for MSELoss(): target and loss has to be both one_hot_encoded \n",
    "loss_test_before = loss_func(Y_pred_test_before_oh, Y_test_oh)\n",
    "\n",
    "# Accuracy before training\n",
    "y_pred_test_before = np.argmax(Y_pred_test_before_oh.cpu().detach().numpy(), axis=1)\n",
    "correct_before = np.sum(y_pred_test_before == Y_test.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "U9NgsiAFEcZI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66738/2886834695.py:30: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  correct_val = np.sum(y_pred_val == Y_val.numpy())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/1000 (0%)\ttrain_Loss: 0.239582\tval_Loss: 0.244484\n",
      "Train Epoch: 2/1000 (0%)\ttrain_Loss: 0.232143\tval_Loss: 0.240843\n",
      "Train Epoch: 3/1000 (0%)\ttrain_Loss: 0.225706\tval_Loss: 0.237583\n",
      "Train Epoch: 4/1000 (0%)\ttrain_Loss: 0.219930\tval_Loss: 0.234790\n",
      "Train Epoch: 5/1000 (0%)\ttrain_Loss: 0.214850\tval_Loss: 0.232679\n",
      "Train Epoch: 6/1000 (0%)\ttrain_Loss: 0.210617\tval_Loss: 0.231327\n",
      "Train Epoch: 7/1000 (1%)\ttrain_Loss: 0.207593\tval_Loss: 0.230317\n",
      "Train Epoch: 8/1000 (1%)\ttrain_Loss: 0.205254\tval_Loss: 0.229489\n",
      "Train Epoch: 9/1000 (1%)\ttrain_Loss: 0.203376\tval_Loss: 0.228779\n",
      "Train Epoch: 10/1000 (1%)\ttrain_Loss: 0.201909\tval_Loss: 0.228106\n",
      "Train Epoch: 11/1000 (1%)\ttrain_Loss: 0.200775\tval_Loss: 0.227365\n",
      "Train Epoch: 12/1000 (1%)\ttrain_Loss: 0.199879\tval_Loss: 0.226461\n",
      "Train Epoch: 13/1000 (1%)\ttrain_Loss: 0.199131\tval_Loss: 0.225317\n",
      "Train Epoch: 14/1000 (1%)\ttrain_Loss: 0.198435\tval_Loss: 0.223881\n",
      "Train Epoch: 15/1000 (1%)\ttrain_Loss: 0.197734\tval_Loss: 0.222153\n",
      "Train Epoch: 16/1000 (2%)\ttrain_Loss: 0.196948\tval_Loss: 0.220110\n",
      "Train Epoch: 17/1000 (2%)\ttrain_Loss: 0.196027\tval_Loss: 0.217791\n",
      "Train Epoch: 18/1000 (2%)\ttrain_Loss: 0.194955\tval_Loss: 0.215302\n",
      "Train Epoch: 19/1000 (2%)\ttrain_Loss: 0.193771\tval_Loss: 0.212657\n",
      "Train Epoch: 20/1000 (2%)\ttrain_Loss: 0.192471\tval_Loss: 0.209876\n",
      "Train Epoch: 21/1000 (2%)\ttrain_Loss: 0.191075\tval_Loss: 0.207023\n",
      "Train Epoch: 22/1000 (2%)\ttrain_Loss: 0.189617\tval_Loss: 0.204151\n",
      "Train Epoch: 23/1000 (2%)\ttrain_Loss: 0.188148\tval_Loss: 0.201333\n",
      "Train Epoch: 24/1000 (2%)\ttrain_Loss: 0.186716\tval_Loss: 0.198645\n",
      "Train Epoch: 25/1000 (2%)\ttrain_Loss: 0.185364\tval_Loss: 0.196148\n",
      "Train Epoch: 26/1000 (2%)\ttrain_Loss: 0.184086\tval_Loss: 0.193778\n",
      "Train Epoch: 27/1000 (3%)\ttrain_Loss: 0.182834\tval_Loss: 0.191557\n",
      "Train Epoch: 28/1000 (3%)\ttrain_Loss: 0.181609\tval_Loss: 0.189663\n",
      "Train Epoch: 29/1000 (3%)\ttrain_Loss: 0.180490\tval_Loss: 0.187995\n",
      "Train Epoch: 30/1000 (3%)\ttrain_Loss: 0.179333\tval_Loss: 0.186407\n",
      "Train Epoch: 31/1000 (3%)\ttrain_Loss: 0.178153\tval_Loss: 0.184865\n",
      "Train Epoch: 32/1000 (3%)\ttrain_Loss: 0.176947\tval_Loss: 0.183335\n",
      "Train Epoch: 33/1000 (3%)\ttrain_Loss: 0.175692\tval_Loss: 0.181803\n",
      "Train Epoch: 34/1000 (3%)\ttrain_Loss: 0.174393\tval_Loss: 0.180251\n",
      "Train Epoch: 35/1000 (3%)\ttrain_Loss: 0.173058\tval_Loss: 0.178673\n",
      "Train Epoch: 36/1000 (4%)\ttrain_Loss: 0.171686\tval_Loss: 0.177080\n",
      "Train Epoch: 37/1000 (4%)\ttrain_Loss: 0.170298\tval_Loss: 0.175496\n",
      "Train Epoch: 38/1000 (4%)\ttrain_Loss: 0.168909\tval_Loss: 0.173904\n",
      "Train Epoch: 39/1000 (4%)\ttrain_Loss: 0.167550\tval_Loss: 0.172304\n",
      "Train Epoch: 40/1000 (4%)\ttrain_Loss: 0.166215\tval_Loss: 0.170642\n",
      "Train Epoch: 41/1000 (4%)\ttrain_Loss: 0.164926\tval_Loss: 0.168835\n",
      "Train Epoch: 42/1000 (4%)\ttrain_Loss: 0.163617\tval_Loss: 0.166886\n",
      "Train Epoch: 43/1000 (4%)\ttrain_Loss: 0.162283\tval_Loss: 0.164799\n",
      "Train Epoch: 44/1000 (4%)\ttrain_Loss: 0.160925\tval_Loss: 0.162583\n",
      "Train Epoch: 45/1000 (4%)\ttrain_Loss: 0.159537\tval_Loss: 0.160221\n",
      "Train Epoch: 46/1000 (4%)\ttrain_Loss: 0.158090\tval_Loss: 0.157582\n",
      "Train Epoch: 47/1000 (5%)\ttrain_Loss: 0.156505\tval_Loss: 0.154723\n",
      "Train Epoch: 48/1000 (5%)\ttrain_Loss: 0.154830\tval_Loss: 0.151706\n",
      "Train Epoch: 49/1000 (5%)\ttrain_Loss: 0.153068\tval_Loss: 0.148479\n",
      "Train Epoch: 50/1000 (5%)\ttrain_Loss: 0.151391\tval_Loss: 0.145639\n",
      "Train Epoch: 51/1000 (5%)\ttrain_Loss: 0.149870\tval_Loss: 0.142972\n",
      "Train Epoch: 52/1000 (5%)\ttrain_Loss: 0.148334\tval_Loss: 0.140327\n",
      "Train Epoch: 53/1000 (5%)\ttrain_Loss: 0.146746\tval_Loss: 0.137728\n",
      "Train Epoch: 54/1000 (5%)\ttrain_Loss: 0.145111\tval_Loss: 0.135212\n",
      "Train Epoch: 55/1000 (5%)\ttrain_Loss: 0.143454\tval_Loss: 0.132812\n",
      "Train Epoch: 56/1000 (6%)\ttrain_Loss: 0.141877\tval_Loss: 0.130402\n",
      "Train Epoch: 57/1000 (6%)\ttrain_Loss: 0.140269\tval_Loss: 0.127990\n",
      "Train Epoch: 58/1000 (6%)\ttrain_Loss: 0.138639\tval_Loss: 0.125569\n",
      "Train Epoch: 59/1000 (6%)\ttrain_Loss: 0.136988\tval_Loss: 0.123135\n",
      "Train Epoch: 60/1000 (6%)\ttrain_Loss: 0.135322\tval_Loss: 0.120706\n",
      "Train Epoch: 61/1000 (6%)\ttrain_Loss: 0.133655\tval_Loss: 0.118321\n",
      "Train Epoch: 62/1000 (6%)\ttrain_Loss: 0.132021\tval_Loss: 0.115995\n",
      "Train Epoch: 63/1000 (6%)\ttrain_Loss: 0.130357\tval_Loss: 0.113740\n",
      "Train Epoch: 64/1000 (6%)\ttrain_Loss: 0.128699\tval_Loss: 0.111570\n",
      "Train Epoch: 65/1000 (6%)\ttrain_Loss: 0.127074\tval_Loss: 0.109438\n",
      "Train Epoch: 66/1000 (6%)\ttrain_Loss: 0.125458\tval_Loss: 0.107341\n",
      "Train Epoch: 67/1000 (7%)\ttrain_Loss: 0.123843\tval_Loss: 0.105279\n",
      "Train Epoch: 68/1000 (7%)\ttrain_Loss: 0.122226\tval_Loss: 0.103254\n",
      "Train Epoch: 69/1000 (7%)\ttrain_Loss: 0.120607\tval_Loss: 0.101265\n",
      "Train Epoch: 70/1000 (7%)\ttrain_Loss: 0.118989\tval_Loss: 0.099313\n",
      "Train Epoch: 71/1000 (7%)\ttrain_Loss: 0.117380\tval_Loss: 0.097406\n",
      "Train Epoch: 72/1000 (7%)\ttrain_Loss: 0.115781\tval_Loss: 0.095551\n",
      "Train Epoch: 73/1000 (7%)\ttrain_Loss: 0.114196\tval_Loss: 0.093747\n",
      "Train Epoch: 74/1000 (7%)\ttrain_Loss: 0.112618\tval_Loss: 0.091990\n",
      "Train Epoch: 75/1000 (7%)\ttrain_Loss: 0.111052\tval_Loss: 0.090278\n",
      "Train Epoch: 76/1000 (8%)\ttrain_Loss: 0.109502\tval_Loss: 0.088603\n",
      "Train Epoch: 77/1000 (8%)\ttrain_Loss: 0.107967\tval_Loss: 0.086963\n",
      "Train Epoch: 78/1000 (8%)\ttrain_Loss: 0.106448\tval_Loss: 0.085358\n",
      "Train Epoch: 79/1000 (8%)\ttrain_Loss: 0.104946\tval_Loss: 0.083802\n",
      "Train Epoch: 80/1000 (8%)\ttrain_Loss: 0.103465\tval_Loss: 0.082299\n",
      "Train Epoch: 81/1000 (8%)\ttrain_Loss: 0.101998\tval_Loss: 0.080852\n",
      "Train Epoch: 82/1000 (8%)\ttrain_Loss: 0.100546\tval_Loss: 0.079466\n",
      "Train Epoch: 83/1000 (8%)\ttrain_Loss: 0.099111\tval_Loss: 0.078138\n",
      "Train Epoch: 84/1000 (8%)\ttrain_Loss: 0.097690\tval_Loss: 0.076871\n",
      "Train Epoch: 85/1000 (8%)\ttrain_Loss: 0.096282\tval_Loss: 0.075663\n",
      "Train Epoch: 86/1000 (8%)\ttrain_Loss: 0.094886\tval_Loss: 0.074533\n",
      "Train Epoch: 87/1000 (9%)\ttrain_Loss: 0.093499\tval_Loss: 0.073454\n",
      "Train Epoch: 88/1000 (9%)\ttrain_Loss: 0.092125\tval_Loss: 0.072420\n",
      "Train Epoch: 89/1000 (9%)\ttrain_Loss: 0.090781\tval_Loss: 0.071426\n",
      "Train Epoch: 90/1000 (9%)\ttrain_Loss: 0.089443\tval_Loss: 0.070455\n",
      "Train Epoch: 91/1000 (9%)\ttrain_Loss: 0.088112\tval_Loss: 0.069501\n",
      "Train Epoch: 92/1000 (9%)\ttrain_Loss: 0.086789\tval_Loss: 0.068567\n",
      "Train Epoch: 93/1000 (9%)\ttrain_Loss: 0.085481\tval_Loss: 0.067649\n",
      "Train Epoch: 94/1000 (9%)\ttrain_Loss: 0.084177\tval_Loss: 0.066740\n",
      "Train Epoch: 95/1000 (9%)\ttrain_Loss: 0.082863\tval_Loss: 0.065809\n",
      "Train Epoch: 96/1000 (10%)\ttrain_Loss: 0.081520\tval_Loss: 0.064846\n",
      "Train Epoch: 97/1000 (10%)\ttrain_Loss: 0.080130\tval_Loss: 0.063831\n",
      "Train Epoch: 98/1000 (10%)\ttrain_Loss: 0.078660\tval_Loss: 0.062780\n",
      "Train Epoch: 99/1000 (10%)\ttrain_Loss: 0.077159\tval_Loss: 0.061824\n",
      "Train Epoch: 100/1000 (10%)\ttrain_Loss: 0.075757\tval_Loss: 0.060897\n",
      "Train Epoch: 101/1000 (10%)\ttrain_Loss: 0.074492\tval_Loss: 0.059945\n",
      "Train Epoch: 102/1000 (10%)\ttrain_Loss: 0.073271\tval_Loss: 0.058970\n",
      "Train Epoch: 103/1000 (10%)\ttrain_Loss: 0.072043\tval_Loss: 0.057969\n",
      "Train Epoch: 104/1000 (10%)\ttrain_Loss: 0.070806\tval_Loss: 0.056954\n",
      "Train Epoch: 105/1000 (10%)\ttrain_Loss: 0.069565\tval_Loss: 0.055920\n",
      "Train Epoch: 106/1000 (10%)\ttrain_Loss: 0.068324\tval_Loss: 0.054868\n",
      "Train Epoch: 107/1000 (11%)\ttrain_Loss: 0.067080\tval_Loss: 0.053793\n",
      "Train Epoch: 108/1000 (11%)\ttrain_Loss: 0.065835\tval_Loss: 0.052706\n",
      "Train Epoch: 109/1000 (11%)\ttrain_Loss: 0.064591\tval_Loss: 0.051614\n",
      "Train Epoch: 110/1000 (11%)\ttrain_Loss: 0.063361\tval_Loss: 0.050529\n",
      "Train Epoch: 111/1000 (11%)\ttrain_Loss: 0.062139\tval_Loss: 0.049462\n",
      "Train Epoch: 112/1000 (11%)\ttrain_Loss: 0.060929\tval_Loss: 0.048413\n",
      "Train Epoch: 113/1000 (11%)\ttrain_Loss: 0.059735\tval_Loss: 0.047384\n",
      "Train Epoch: 114/1000 (11%)\ttrain_Loss: 0.058565\tval_Loss: 0.046378\n",
      "Train Epoch: 115/1000 (11%)\ttrain_Loss: 0.057411\tval_Loss: 0.045388\n",
      "Train Epoch: 116/1000 (12%)\ttrain_Loss: 0.056280\tval_Loss: 0.044414\n",
      "Train Epoch: 117/1000 (12%)\ttrain_Loss: 0.055164\tval_Loss: 0.043460\n",
      "Train Epoch: 118/1000 (12%)\ttrain_Loss: 0.054060\tval_Loss: 0.042541\n",
      "Train Epoch: 119/1000 (12%)\ttrain_Loss: 0.052972\tval_Loss: 0.041671\n",
      "Train Epoch: 120/1000 (12%)\ttrain_Loss: 0.051899\tval_Loss: 0.040859\n",
      "Train Epoch: 121/1000 (12%)\ttrain_Loss: 0.050848\tval_Loss: 0.040086\n",
      "Train Epoch: 122/1000 (12%)\ttrain_Loss: 0.049819\tval_Loss: 0.039326\n",
      "Train Epoch: 123/1000 (12%)\ttrain_Loss: 0.048814\tval_Loss: 0.038547\n",
      "Train Epoch: 124/1000 (12%)\ttrain_Loss: 0.047832\tval_Loss: 0.037737\n",
      "Train Epoch: 125/1000 (12%)\ttrain_Loss: 0.046871\tval_Loss: 0.036907\n",
      "Train Epoch: 126/1000 (12%)\ttrain_Loss: 0.045933\tval_Loss: 0.036067\n",
      "Train Epoch: 127/1000 (13%)\ttrain_Loss: 0.045021\tval_Loss: 0.035239\n",
      "Train Epoch: 128/1000 (13%)\ttrain_Loss: 0.044137\tval_Loss: 0.034451\n",
      "Train Epoch: 129/1000 (13%)\ttrain_Loss: 0.043282\tval_Loss: 0.033710\n",
      "Train Epoch: 130/1000 (13%)\ttrain_Loss: 0.042459\tval_Loss: 0.033033\n",
      "Train Epoch: 131/1000 (13%)\ttrain_Loss: 0.041666\tval_Loss: 0.032429\n",
      "Train Epoch: 132/1000 (13%)\ttrain_Loss: 0.040903\tval_Loss: 0.031880\n",
      "Train Epoch: 133/1000 (13%)\ttrain_Loss: 0.040168\tval_Loss: 0.031357\n",
      "Train Epoch: 134/1000 (13%)\ttrain_Loss: 0.039461\tval_Loss: 0.030830\n",
      "Train Epoch: 135/1000 (13%)\ttrain_Loss: 0.038782\tval_Loss: 0.030283\n",
      "Train Epoch: 136/1000 (14%)\ttrain_Loss: 0.038132\tval_Loss: 0.029738\n",
      "Train Epoch: 137/1000 (14%)\ttrain_Loss: 0.037515\tval_Loss: 0.029224\n",
      "Train Epoch: 138/1000 (14%)\ttrain_Loss: 0.036926\tval_Loss: 0.028752\n",
      "Train Epoch: 139/1000 (14%)\ttrain_Loss: 0.036360\tval_Loss: 0.028319\n",
      "Train Epoch: 140/1000 (14%)\ttrain_Loss: 0.035819\tval_Loss: 0.027919\n",
      "Train Epoch: 141/1000 (14%)\ttrain_Loss: 0.035302\tval_Loss: 0.027547\n",
      "Train Epoch: 142/1000 (14%)\ttrain_Loss: 0.034809\tval_Loss: 0.027201\n",
      "Train Epoch: 143/1000 (14%)\ttrain_Loss: 0.034340\tval_Loss: 0.026879\n",
      "Train Epoch: 144/1000 (14%)\ttrain_Loss: 0.033898\tval_Loss: 0.026584\n",
      "Train Epoch: 145/1000 (14%)\ttrain_Loss: 0.033477\tval_Loss: 0.026314\n",
      "Train Epoch: 146/1000 (14%)\ttrain_Loss: 0.033071\tval_Loss: 0.026065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 147/1000 (15%)\ttrain_Loss: 0.032685\tval_Loss: 0.025850\n",
      "Train Epoch: 148/1000 (15%)\ttrain_Loss: 0.032318\tval_Loss: 0.025658\n",
      "Train Epoch: 149/1000 (15%)\ttrain_Loss: 0.031965\tval_Loss: 0.025487\n",
      "Train Epoch: 150/1000 (15%)\ttrain_Loss: 0.031633\tval_Loss: 0.025351\n",
      "Train Epoch: 151/1000 (15%)\ttrain_Loss: 0.031324\tval_Loss: 0.025228\n",
      "Train Epoch: 152/1000 (15%)\ttrain_Loss: 0.031018\tval_Loss: 0.025106\n",
      "Train Epoch: 153/1000 (15%)\ttrain_Loss: 0.030715\tval_Loss: 0.024984\n",
      "Train Epoch: 154/1000 (15%)\ttrain_Loss: 0.030426\tval_Loss: 0.024843\n",
      "Train Epoch: 155/1000 (15%)\ttrain_Loss: 0.030149\tval_Loss: 0.024703\n",
      "Train Epoch: 156/1000 (16%)\ttrain_Loss: 0.029883\tval_Loss: 0.024581\n",
      "Train Epoch: 157/1000 (16%)\ttrain_Loss: 0.029649\tval_Loss: 0.024477\n",
      "Train Epoch: 158/1000 (16%)\ttrain_Loss: 0.029425\tval_Loss: 0.024372\n",
      "Train Epoch: 159/1000 (16%)\ttrain_Loss: 0.029201\tval_Loss: 0.024261\n",
      "Train Epoch: 160/1000 (16%)\ttrain_Loss: 0.028982\tval_Loss: 0.024155\n",
      "Train Epoch: 161/1000 (16%)\ttrain_Loss: 0.028773\tval_Loss: 0.024046\n",
      "Train Epoch: 162/1000 (16%)\ttrain_Loss: 0.028579\tval_Loss: 0.023952\n",
      "Train Epoch: 163/1000 (16%)\ttrain_Loss: 0.028393\tval_Loss: 0.023871\n",
      "Train Epoch: 164/1000 (16%)\ttrain_Loss: 0.028214\tval_Loss: 0.023804\n",
      "Train Epoch: 165/1000 (16%)\ttrain_Loss: 0.028041\tval_Loss: 0.023754\n",
      "Train Epoch: 166/1000 (16%)\ttrain_Loss: 0.027875\tval_Loss: 0.023718\n",
      "Train Epoch: 167/1000 (17%)\ttrain_Loss: 0.027733\tval_Loss: 0.023670\n",
      "Train Epoch: 168/1000 (17%)\ttrain_Loss: 0.027605\tval_Loss: 0.023601\n",
      "Train Epoch: 169/1000 (17%)\ttrain_Loss: 0.027478\tval_Loss: 0.023537\n",
      "Train Epoch: 170/1000 (17%)\ttrain_Loss: 0.027355\tval_Loss: 0.023483\n",
      "Train Epoch: 171/1000 (17%)\ttrain_Loss: 0.027238\tval_Loss: 0.023443\n",
      "Train Epoch: 172/1000 (17%)\ttrain_Loss: 0.027122\tval_Loss: 0.023392\n",
      "Train Epoch: 173/1000 (17%)\ttrain_Loss: 0.027009\tval_Loss: 0.023337\n",
      "Train Epoch: 174/1000 (17%)\ttrain_Loss: 0.026897\tval_Loss: 0.023269\n",
      "Train Epoch: 175/1000 (17%)\ttrain_Loss: 0.026787\tval_Loss: 0.023207\n",
      "Train Epoch: 176/1000 (18%)\ttrain_Loss: 0.026692\tval_Loss: 0.023168\n",
      "Train Epoch: 177/1000 (18%)\ttrain_Loss: 0.026603\tval_Loss: 0.023142\n",
      "Train Epoch: 178/1000 (18%)\ttrain_Loss: 0.026515\tval_Loss: 0.023123\n",
      "Train Epoch: 179/1000 (18%)\ttrain_Loss: 0.026427\tval_Loss: 0.023105\n",
      "Train Epoch: 180/1000 (18%)\ttrain_Loss: 0.026338\tval_Loss: 0.023085\n",
      "Train Epoch: 181/1000 (18%)\ttrain_Loss: 0.026250\tval_Loss: 0.023065\n",
      "Train Epoch: 182/1000 (18%)\ttrain_Loss: 0.026169\tval_Loss: 0.023038\n",
      "Train Epoch: 183/1000 (18%)\ttrain_Loss: 0.026098\tval_Loss: 0.023014\n",
      "Train Epoch: 184/1000 (18%)\ttrain_Loss: 0.026028\tval_Loss: 0.023016\n",
      "Train Epoch: 185/1000 (18%)\ttrain_Loss: 0.025953\tval_Loss: 0.023023\n",
      "Train Epoch: 186/1000 (18%)\ttrain_Loss: 0.025878\tval_Loss: 0.023012\n",
      "Train Epoch: 187/1000 (19%)\ttrain_Loss: 0.025807\tval_Loss: 0.022973\n",
      "Train Epoch: 188/1000 (19%)\ttrain_Loss: 0.025739\tval_Loss: 0.022920\n",
      "Train Epoch: 189/1000 (19%)\ttrain_Loss: 0.025670\tval_Loss: 0.022877\n",
      "Train Epoch: 190/1000 (19%)\ttrain_Loss: 0.025605\tval_Loss: 0.022883\n",
      "Train Epoch: 191/1000 (19%)\ttrain_Loss: 0.025542\tval_Loss: 0.022921\n",
      "Train Epoch: 192/1000 (19%)\ttrain_Loss: 0.025476\tval_Loss: 0.022964\n",
      "Train Epoch: 193/1000 (19%)\ttrain_Loss: 0.025414\tval_Loss: 0.022970\n",
      "Train Epoch: 194/1000 (19%)\ttrain_Loss: 0.025351\tval_Loss: 0.022952\n",
      "Train Epoch: 195/1000 (19%)\ttrain_Loss: 0.025284\tval_Loss: 0.022908\n",
      "Train Epoch: 196/1000 (20%)\ttrain_Loss: 0.025216\tval_Loss: 0.022877\n",
      "Train Epoch: 197/1000 (20%)\ttrain_Loss: 0.025148\tval_Loss: 0.022873\n",
      "Train Epoch: 198/1000 (20%)\ttrain_Loss: 0.025078\tval_Loss: 0.022887\n",
      "Train Epoch: 199/1000 (20%)\ttrain_Loss: 0.025007\tval_Loss: 0.022897\n",
      "Train Epoch: 200/1000 (20%)\ttrain_Loss: 0.024937\tval_Loss: 0.022896\n",
      "Train Epoch: 201/1000 (20%)\ttrain_Loss: 0.024871\tval_Loss: 0.022873\n",
      "Train Epoch: 202/1000 (20%)\ttrain_Loss: 0.024805\tval_Loss: 0.022843\n",
      "Train Epoch: 203/1000 (20%)\ttrain_Loss: 0.024740\tval_Loss: 0.022820\n",
      "Train Epoch: 204/1000 (20%)\ttrain_Loss: 0.024676\tval_Loss: 0.022811\n",
      "Train Epoch: 205/1000 (20%)\ttrain_Loss: 0.024614\tval_Loss: 0.022825\n",
      "Train Epoch: 206/1000 (20%)\ttrain_Loss: 0.024559\tval_Loss: 0.022838\n",
      "Train Epoch: 207/1000 (21%)\ttrain_Loss: 0.024504\tval_Loss: 0.022831\n",
      "Train Epoch: 208/1000 (21%)\ttrain_Loss: 0.024451\tval_Loss: 0.022813\n",
      "Train Epoch: 209/1000 (21%)\ttrain_Loss: 0.024397\tval_Loss: 0.022790\n",
      "Train Epoch: 210/1000 (21%)\ttrain_Loss: 0.024339\tval_Loss: 0.022775\n",
      "Train Epoch: 211/1000 (21%)\ttrain_Loss: 0.024288\tval_Loss: 0.022769\n",
      "Train Epoch: 212/1000 (21%)\ttrain_Loss: 0.024242\tval_Loss: 0.022781\n",
      "Train Epoch: 213/1000 (21%)\ttrain_Loss: 0.024202\tval_Loss: 0.022796\n",
      "Train Epoch: 214/1000 (21%)\ttrain_Loss: 0.024162\tval_Loss: 0.022794\n",
      "Train Epoch: 215/1000 (21%)\ttrain_Loss: 0.024120\tval_Loss: 0.022794\n",
      "Train Epoch: 216/1000 (22%)\ttrain_Loss: 0.024076\tval_Loss: 0.022801\n",
      "Train Epoch: 217/1000 (22%)\ttrain_Loss: 0.024032\tval_Loss: 0.022806\n",
      "Train Epoch: 218/1000 (22%)\ttrain_Loss: 0.023989\tval_Loss: 0.022801\n",
      "Train Epoch: 219/1000 (22%)\ttrain_Loss: 0.023947\tval_Loss: 0.022778\n",
      "Train Epoch: 220/1000 (22%)\ttrain_Loss: 0.023904\tval_Loss: 0.022783\n",
      "Train Epoch: 221/1000 (22%)\ttrain_Loss: 0.023863\tval_Loss: 0.022787\n",
      "Train Epoch: 222/1000 (22%)\ttrain_Loss: 0.023822\tval_Loss: 0.022798\n",
      "Train Epoch: 223/1000 (22%)\ttrain_Loss: 0.023780\tval_Loss: 0.022790\n",
      "Train Epoch: 224/1000 (22%)\ttrain_Loss: 0.023738\tval_Loss: 0.022775\n",
      "Train Epoch: 225/1000 (22%)\ttrain_Loss: 0.023697\tval_Loss: 0.022752\n",
      "Train Epoch: 226/1000 (22%)\ttrain_Loss: 0.023657\tval_Loss: 0.022737\n",
      "Train Epoch: 227/1000 (23%)\ttrain_Loss: 0.023618\tval_Loss: 0.022737\n",
      "Train Epoch: 228/1000 (23%)\ttrain_Loss: 0.023577\tval_Loss: 0.022723\n",
      "Train Epoch: 229/1000 (23%)\ttrain_Loss: 0.023535\tval_Loss: 0.022707\n",
      "Train Epoch: 230/1000 (23%)\ttrain_Loss: 0.023493\tval_Loss: 0.022706\n",
      "Train Epoch: 231/1000 (23%)\ttrain_Loss: 0.023452\tval_Loss: 0.022713\n",
      "Train Epoch: 232/1000 (23%)\ttrain_Loss: 0.023409\tval_Loss: 0.022708\n",
      "Train Epoch: 233/1000 (23%)\ttrain_Loss: 0.023370\tval_Loss: 0.022696\n",
      "Train Epoch: 234/1000 (23%)\ttrain_Loss: 0.023331\tval_Loss: 0.022669\n",
      "Train Epoch: 235/1000 (23%)\ttrain_Loss: 0.023293\tval_Loss: 0.022637\n",
      "Train Epoch: 236/1000 (24%)\ttrain_Loss: 0.023259\tval_Loss: 0.022608\n",
      "Train Epoch: 237/1000 (24%)\ttrain_Loss: 0.023226\tval_Loss: 0.022601\n",
      "Train Epoch: 238/1000 (24%)\ttrain_Loss: 0.023191\tval_Loss: 0.022603\n",
      "Train Epoch: 239/1000 (24%)\ttrain_Loss: 0.023154\tval_Loss: 0.022603\n",
      "Train Epoch: 240/1000 (24%)\ttrain_Loss: 0.023117\tval_Loss: 0.022591\n",
      "Train Epoch: 241/1000 (24%)\ttrain_Loss: 0.023081\tval_Loss: 0.022567\n",
      "Train Epoch: 242/1000 (24%)\ttrain_Loss: 0.023045\tval_Loss: 0.022543\n",
      "Train Epoch: 243/1000 (24%)\ttrain_Loss: 0.023010\tval_Loss: 0.022537\n",
      "Train Epoch: 244/1000 (24%)\ttrain_Loss: 0.022974\tval_Loss: 0.022543\n",
      "Train Epoch: 245/1000 (24%)\ttrain_Loss: 0.022938\tval_Loss: 0.022547\n",
      "Train Epoch: 246/1000 (24%)\ttrain_Loss: 0.022904\tval_Loss: 0.022524\n",
      "Train Epoch: 247/1000 (25%)\ttrain_Loss: 0.022870\tval_Loss: 0.022496\n",
      "Train Epoch: 248/1000 (25%)\ttrain_Loss: 0.022834\tval_Loss: 0.022489\n",
      "Train Epoch: 249/1000 (25%)\ttrain_Loss: 0.022800\tval_Loss: 0.022513\n",
      "Train Epoch: 250/1000 (25%)\ttrain_Loss: 0.022766\tval_Loss: 0.022528\n",
      "Train Epoch: 251/1000 (25%)\ttrain_Loss: 0.022729\tval_Loss: 0.022534\n",
      "Train Epoch: 252/1000 (25%)\ttrain_Loss: 0.022692\tval_Loss: 0.022513\n",
      "Train Epoch: 253/1000 (25%)\ttrain_Loss: 0.022655\tval_Loss: 0.022494\n",
      "Train Epoch: 254/1000 (25%)\ttrain_Loss: 0.022616\tval_Loss: 0.022496\n",
      "Train Epoch: 255/1000 (25%)\ttrain_Loss: 0.022577\tval_Loss: 0.022514\n",
      "Train Epoch: 256/1000 (26%)\ttrain_Loss: 0.022538\tval_Loss: 0.022529\n",
      "Train Epoch: 257/1000 (26%)\ttrain_Loss: 0.022498\tval_Loss: 0.022519\n",
      "Train Epoch: 258/1000 (26%)\ttrain_Loss: 0.022459\tval_Loss: 0.022483\n",
      "Train Epoch: 259/1000 (26%)\ttrain_Loss: 0.022418\tval_Loss: 0.022457\n",
      "Train Epoch: 260/1000 (26%)\ttrain_Loss: 0.022379\tval_Loss: 0.022470\n",
      "Train Epoch: 261/1000 (26%)\ttrain_Loss: 0.022337\tval_Loss: 0.022512\n",
      "Train Epoch: 262/1000 (26%)\ttrain_Loss: 0.022289\tval_Loss: 0.022544\n",
      "Train Epoch: 263/1000 (26%)\ttrain_Loss: 0.022238\tval_Loss: 0.022532\n",
      "Train Epoch: 264/1000 (26%)\ttrain_Loss: 0.022183\tval_Loss: 0.022495\n",
      "Train Epoch: 265/1000 (26%)\ttrain_Loss: 0.022126\tval_Loss: 0.022470\n",
      "Train Epoch: 266/1000 (26%)\ttrain_Loss: 0.022070\tval_Loss: 0.022467\n",
      "Train Epoch: 267/1000 (27%)\ttrain_Loss: 0.022006\tval_Loss: 0.022489\n",
      "Train Epoch: 268/1000 (27%)\ttrain_Loss: 0.021933\tval_Loss: 0.022511\n",
      "Train Epoch: 269/1000 (27%)\ttrain_Loss: 0.021866\tval_Loss: 0.022523\n",
      "Train Epoch: 270/1000 (27%)\ttrain_Loss: 0.021812\tval_Loss: 0.022519\n",
      "Train Epoch: 271/1000 (27%)\ttrain_Loss: 0.021787\tval_Loss: 0.022514\n",
      "Train Epoch: 272/1000 (27%)\ttrain_Loss: 0.021778\tval_Loss: 0.022522\n",
      "Train Epoch: 273/1000 (27%)\ttrain_Loss: 0.021771\tval_Loss: 0.022539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 274/1000 (27%)\ttrain_Loss: 0.021755\tval_Loss: 0.022573\n",
      "Train Epoch: 275/1000 (27%)\ttrain_Loss: 0.021726\tval_Loss: 0.022583\n",
      "Train Epoch: 276/1000 (28%)\ttrain_Loss: 0.021687\tval_Loss: 0.022563\n",
      "Train Epoch: 277/1000 (28%)\ttrain_Loss: 0.021640\tval_Loss: 0.022537\n",
      "Train Epoch: 278/1000 (28%)\ttrain_Loss: 0.021595\tval_Loss: 0.022524\n",
      "Train Epoch: 279/1000 (28%)\ttrain_Loss: 0.021554\tval_Loss: 0.022524\n",
      "Train Epoch: 280/1000 (28%)\ttrain_Loss: 0.021512\tval_Loss: 0.022519\n",
      "Train Epoch: 281/1000 (28%)\ttrain_Loss: 0.021481\tval_Loss: 0.022500\n",
      "Train Epoch: 282/1000 (28%)\ttrain_Loss: 0.021460\tval_Loss: 0.022477\n",
      "Train Epoch: 283/1000 (28%)\ttrain_Loss: 0.021439\tval_Loss: 0.022465\n",
      "Train Epoch: 284/1000 (28%)\ttrain_Loss: 0.021414\tval_Loss: 0.022463\n",
      "Train Epoch: 285/1000 (28%)\ttrain_Loss: 0.021387\tval_Loss: 0.022473\n",
      "Train Epoch: 286/1000 (28%)\ttrain_Loss: 0.021356\tval_Loss: 0.022473\n",
      "Train Epoch: 287/1000 (29%)\ttrain_Loss: 0.021319\tval_Loss: 0.022476\n",
      "Train Epoch: 288/1000 (29%)\ttrain_Loss: 0.021278\tval_Loss: 0.022461\n",
      "Train Epoch: 289/1000 (29%)\ttrain_Loss: 0.021237\tval_Loss: 0.022439\n",
      "Train Epoch: 290/1000 (29%)\ttrain_Loss: 0.021196\tval_Loss: 0.022424\n",
      "Train Epoch: 291/1000 (29%)\ttrain_Loss: 0.021157\tval_Loss: 0.022426\n",
      "Train Epoch: 292/1000 (29%)\ttrain_Loss: 0.021126\tval_Loss: 0.022445\n",
      "Train Epoch: 293/1000 (29%)\ttrain_Loss: 0.021100\tval_Loss: 0.022455\n",
      "Train Epoch: 294/1000 (29%)\ttrain_Loss: 0.021069\tval_Loss: 0.022465\n",
      "Train Epoch: 295/1000 (29%)\ttrain_Loss: 0.021025\tval_Loss: 0.022469\n",
      "Train Epoch: 296/1000 (30%)\ttrain_Loss: 0.020975\tval_Loss: 0.022456\n",
      "Train Epoch: 297/1000 (30%)\ttrain_Loss: 0.020926\tval_Loss: 0.022442\n",
      "Train Epoch: 298/1000 (30%)\ttrain_Loss: 0.020895\tval_Loss: 0.022421\n",
      "Train Epoch: 299/1000 (30%)\ttrain_Loss: 0.020877\tval_Loss: 0.022394\n",
      "Train Epoch: 300/1000 (30%)\ttrain_Loss: 0.020849\tval_Loss: 0.022372\n",
      "Train Epoch: 301/1000 (30%)\ttrain_Loss: 0.020811\tval_Loss: 0.022373\n",
      "Train Epoch: 302/1000 (30%)\ttrain_Loss: 0.020767\tval_Loss: 0.022378\n",
      "Train Epoch: 303/1000 (30%)\ttrain_Loss: 0.020730\tval_Loss: 0.022373\n",
      "Train Epoch: 304/1000 (30%)\ttrain_Loss: 0.020698\tval_Loss: 0.022360\n",
      "Train Epoch: 305/1000 (30%)\ttrain_Loss: 0.020671\tval_Loss: 0.022346\n",
      "Train Epoch: 306/1000 (30%)\ttrain_Loss: 0.020637\tval_Loss: 0.022342\n",
      "Train Epoch: 307/1000 (31%)\ttrain_Loss: 0.020597\tval_Loss: 0.022359\n",
      "Train Epoch: 308/1000 (31%)\ttrain_Loss: 0.020557\tval_Loss: 0.022378\n",
      "Train Epoch: 309/1000 (31%)\ttrain_Loss: 0.020521\tval_Loss: 0.022388\n",
      "Train Epoch: 310/1000 (31%)\ttrain_Loss: 0.020486\tval_Loss: 0.022397\n",
      "Train Epoch: 311/1000 (31%)\ttrain_Loss: 0.020458\tval_Loss: 0.022382\n",
      "Train Epoch: 312/1000 (31%)\ttrain_Loss: 0.020434\tval_Loss: 0.022353\n",
      "Train Epoch: 313/1000 (31%)\ttrain_Loss: 0.020409\tval_Loss: 0.022329\n",
      "Train Epoch: 314/1000 (31%)\ttrain_Loss: 0.020379\tval_Loss: 0.022321\n",
      "Train Epoch: 315/1000 (31%)\ttrain_Loss: 0.020343\tval_Loss: 0.022319\n",
      "Train Epoch: 316/1000 (32%)\ttrain_Loss: 0.020309\tval_Loss: 0.022310\n",
      "Train Epoch: 317/1000 (32%)\ttrain_Loss: 0.020277\tval_Loss: 0.022281\n",
      "Train Epoch: 318/1000 (32%)\ttrain_Loss: 0.020246\tval_Loss: 0.022257\n",
      "Train Epoch: 319/1000 (32%)\ttrain_Loss: 0.020220\tval_Loss: 0.022259\n",
      "Train Epoch: 320/1000 (32%)\ttrain_Loss: 0.020191\tval_Loss: 0.022274\n",
      "Train Epoch: 321/1000 (32%)\ttrain_Loss: 0.020160\tval_Loss: 0.022266\n",
      "Train Epoch: 322/1000 (32%)\ttrain_Loss: 0.020128\tval_Loss: 0.022232\n",
      "Train Epoch: 323/1000 (32%)\ttrain_Loss: 0.020099\tval_Loss: 0.022207\n",
      "Train Epoch: 324/1000 (32%)\ttrain_Loss: 0.020070\tval_Loss: 0.022211\n",
      "Train Epoch: 325/1000 (32%)\ttrain_Loss: 0.020042\tval_Loss: 0.022238\n",
      "Train Epoch: 326/1000 (32%)\ttrain_Loss: 0.020013\tval_Loss: 0.022260\n",
      "Train Epoch: 327/1000 (33%)\ttrain_Loss: 0.019985\tval_Loss: 0.022260\n",
      "Train Epoch: 328/1000 (33%)\ttrain_Loss: 0.019954\tval_Loss: 0.022244\n",
      "Train Epoch: 329/1000 (33%)\ttrain_Loss: 0.019921\tval_Loss: 0.022223\n",
      "Train Epoch: 330/1000 (33%)\ttrain_Loss: 0.019888\tval_Loss: 0.022209\n",
      "Train Epoch: 331/1000 (33%)\ttrain_Loss: 0.019855\tval_Loss: 0.022202\n",
      "Train Epoch: 332/1000 (33%)\ttrain_Loss: 0.019822\tval_Loss: 0.022207\n",
      "Train Epoch: 333/1000 (33%)\ttrain_Loss: 0.019790\tval_Loss: 0.022206\n",
      "Train Epoch: 334/1000 (33%)\ttrain_Loss: 0.019758\tval_Loss: 0.022200\n",
      "Train Epoch: 335/1000 (33%)\ttrain_Loss: 0.019721\tval_Loss: 0.022193\n",
      "Train Epoch: 336/1000 (34%)\ttrain_Loss: 0.019682\tval_Loss: 0.022191\n",
      "Train Epoch: 337/1000 (34%)\ttrain_Loss: 0.019644\tval_Loss: 0.022177\n",
      "Train Epoch: 338/1000 (34%)\ttrain_Loss: 0.019605\tval_Loss: 0.022154\n",
      "Train Epoch: 339/1000 (34%)\ttrain_Loss: 0.019566\tval_Loss: 0.022139\n",
      "Train Epoch: 340/1000 (34%)\ttrain_Loss: 0.019525\tval_Loss: 0.022128\n",
      "Train Epoch: 341/1000 (34%)\ttrain_Loss: 0.019482\tval_Loss: 0.022124\n",
      "Train Epoch: 342/1000 (34%)\ttrain_Loss: 0.019433\tval_Loss: 0.022112\n",
      "Train Epoch: 343/1000 (34%)\ttrain_Loss: 0.019379\tval_Loss: 0.022095\n",
      "Train Epoch: 344/1000 (34%)\ttrain_Loss: 0.019319\tval_Loss: 0.022078\n",
      "Train Epoch: 345/1000 (34%)\ttrain_Loss: 0.019257\tval_Loss: 0.022067\n",
      "Train Epoch: 346/1000 (34%)\ttrain_Loss: 0.019195\tval_Loss: 0.022072\n",
      "Train Epoch: 347/1000 (35%)\ttrain_Loss: 0.019135\tval_Loss: 0.022081\n",
      "Train Epoch: 348/1000 (35%)\ttrain_Loss: 0.019075\tval_Loss: 0.022095\n",
      "Train Epoch: 349/1000 (35%)\ttrain_Loss: 0.019018\tval_Loss: 0.022106\n",
      "Train Epoch: 350/1000 (35%)\ttrain_Loss: 0.018971\tval_Loss: 0.022110\n",
      "Train Epoch: 351/1000 (35%)\ttrain_Loss: 0.018942\tval_Loss: 0.022116\n",
      "Train Epoch: 352/1000 (35%)\ttrain_Loss: 0.018924\tval_Loss: 0.022128\n",
      "Train Epoch: 353/1000 (35%)\ttrain_Loss: 0.018902\tval_Loss: 0.022149\n",
      "Train Epoch: 354/1000 (35%)\ttrain_Loss: 0.018881\tval_Loss: 0.022171\n",
      "Train Epoch: 355/1000 (35%)\ttrain_Loss: 0.018857\tval_Loss: 0.022183\n",
      "Train Epoch: 356/1000 (36%)\ttrain_Loss: 0.018833\tval_Loss: 0.022177\n",
      "Train Epoch: 357/1000 (36%)\ttrain_Loss: 0.018806\tval_Loss: 0.022165\n",
      "Train Epoch: 358/1000 (36%)\ttrain_Loss: 0.018779\tval_Loss: 0.022149\n",
      "Train Epoch: 359/1000 (36%)\ttrain_Loss: 0.018751\tval_Loss: 0.022136\n",
      "Train Epoch: 360/1000 (36%)\ttrain_Loss: 0.018721\tval_Loss: 0.022120\n",
      "Train Epoch: 361/1000 (36%)\ttrain_Loss: 0.018690\tval_Loss: 0.022098\n",
      "Train Epoch: 362/1000 (36%)\ttrain_Loss: 0.018657\tval_Loss: 0.022071\n",
      "Train Epoch: 363/1000 (36%)\ttrain_Loss: 0.018624\tval_Loss: 0.022037\n",
      "Train Epoch: 364/1000 (36%)\ttrain_Loss: 0.018592\tval_Loss: 0.022006\n",
      "Train Epoch: 365/1000 (36%)\ttrain_Loss: 0.018559\tval_Loss: 0.021976\n",
      "Train Epoch: 366/1000 (36%)\ttrain_Loss: 0.018527\tval_Loss: 0.021960\n",
      "Train Epoch: 367/1000 (37%)\ttrain_Loss: 0.018494\tval_Loss: 0.021941\n",
      "Train Epoch: 368/1000 (37%)\ttrain_Loss: 0.018461\tval_Loss: 0.021897\n",
      "Train Epoch: 369/1000 (37%)\ttrain_Loss: 0.018428\tval_Loss: 0.021845\n",
      "Train Epoch: 370/1000 (37%)\ttrain_Loss: 0.018396\tval_Loss: 0.021818\n",
      "Train Epoch: 371/1000 (37%)\ttrain_Loss: 0.018362\tval_Loss: 0.021812\n",
      "Train Epoch: 372/1000 (37%)\ttrain_Loss: 0.018329\tval_Loss: 0.021813\n",
      "Train Epoch: 373/1000 (37%)\ttrain_Loss: 0.018296\tval_Loss: 0.021797\n",
      "Train Epoch: 374/1000 (37%)\ttrain_Loss: 0.018269\tval_Loss: 0.021758\n",
      "Train Epoch: 375/1000 (37%)\ttrain_Loss: 0.018244\tval_Loss: 0.021736\n",
      "Train Epoch: 376/1000 (38%)\ttrain_Loss: 0.018219\tval_Loss: 0.021730\n",
      "Train Epoch: 377/1000 (38%)\ttrain_Loss: 0.018191\tval_Loss: 0.021729\n",
      "Train Epoch: 378/1000 (38%)\ttrain_Loss: 0.018162\tval_Loss: 0.021731\n",
      "Train Epoch: 379/1000 (38%)\ttrain_Loss: 0.018133\tval_Loss: 0.021727\n",
      "Train Epoch: 380/1000 (38%)\ttrain_Loss: 0.018102\tval_Loss: 0.021696\n",
      "Train Epoch: 381/1000 (38%)\ttrain_Loss: 0.018069\tval_Loss: 0.021681\n",
      "Train Epoch: 382/1000 (38%)\ttrain_Loss: 0.018037\tval_Loss: 0.021677\n",
      "Train Epoch: 383/1000 (38%)\ttrain_Loss: 0.018008\tval_Loss: 0.021680\n",
      "Train Epoch: 384/1000 (38%)\ttrain_Loss: 0.017981\tval_Loss: 0.021686\n",
      "Train Epoch: 385/1000 (38%)\ttrain_Loss: 0.017953\tval_Loss: 0.021684\n",
      "Train Epoch: 386/1000 (38%)\ttrain_Loss: 0.017926\tval_Loss: 0.021667\n",
      "Train Epoch: 387/1000 (39%)\ttrain_Loss: 0.017898\tval_Loss: 0.021648\n",
      "Train Epoch: 388/1000 (39%)\ttrain_Loss: 0.017869\tval_Loss: 0.021645\n",
      "Train Epoch: 389/1000 (39%)\ttrain_Loss: 0.017840\tval_Loss: 0.021648\n",
      "Train Epoch: 390/1000 (39%)\ttrain_Loss: 0.017811\tval_Loss: 0.021655\n",
      "Train Epoch: 391/1000 (39%)\ttrain_Loss: 0.017782\tval_Loss: 0.021623\n",
      "Train Epoch: 392/1000 (39%)\ttrain_Loss: 0.017751\tval_Loss: 0.021607\n",
      "Train Epoch: 393/1000 (39%)\ttrain_Loss: 0.017721\tval_Loss: 0.021615\n",
      "Train Epoch: 394/1000 (39%)\ttrain_Loss: 0.017693\tval_Loss: 0.021613\n",
      "Train Epoch: 395/1000 (39%)\ttrain_Loss: 0.017665\tval_Loss: 0.021585\n",
      "Train Epoch: 396/1000 (40%)\ttrain_Loss: 0.017637\tval_Loss: 0.021557\n",
      "Train Epoch: 397/1000 (40%)\ttrain_Loss: 0.017609\tval_Loss: 0.021547\n",
      "Train Epoch: 398/1000 (40%)\ttrain_Loss: 0.017580\tval_Loss: 0.021524\n",
      "Train Epoch: 399/1000 (40%)\ttrain_Loss: 0.017551\tval_Loss: 0.021498\n",
      "Train Epoch: 400/1000 (40%)\ttrain_Loss: 0.017520\tval_Loss: 0.021499\n",
      "Train Epoch: 401/1000 (40%)\ttrain_Loss: 0.017490\tval_Loss: 0.021511\n",
      "Train Epoch: 402/1000 (40%)\ttrain_Loss: 0.017460\tval_Loss: 0.021528\n",
      "Train Epoch: 403/1000 (40%)\ttrain_Loss: 0.017433\tval_Loss: 0.021521\n",
      "Train Epoch: 404/1000 (40%)\ttrain_Loss: 0.017403\tval_Loss: 0.021496\n",
      "Train Epoch: 405/1000 (40%)\ttrain_Loss: 0.017373\tval_Loss: 0.021470\n",
      "Train Epoch: 406/1000 (40%)\ttrain_Loss: 0.017345\tval_Loss: 0.021462\n",
      "Train Epoch: 407/1000 (41%)\ttrain_Loss: 0.017315\tval_Loss: 0.021450\n",
      "Train Epoch: 408/1000 (41%)\ttrain_Loss: 0.017286\tval_Loss: 0.021447\n",
      "Train Epoch: 409/1000 (41%)\ttrain_Loss: 0.017257\tval_Loss: 0.021444\n",
      "Train Epoch: 410/1000 (41%)\ttrain_Loss: 0.017228\tval_Loss: 0.021449\n",
      "Train Epoch: 411/1000 (41%)\ttrain_Loss: 0.017199\tval_Loss: 0.021427\n",
      "Train Epoch: 412/1000 (41%)\ttrain_Loss: 0.017169\tval_Loss: 0.021395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 413/1000 (41%)\ttrain_Loss: 0.017140\tval_Loss: 0.021387\n",
      "Train Epoch: 414/1000 (41%)\ttrain_Loss: 0.017112\tval_Loss: 0.021364\n",
      "Train Epoch: 415/1000 (41%)\ttrain_Loss: 0.017083\tval_Loss: 0.021346\n",
      "Train Epoch: 416/1000 (42%)\ttrain_Loss: 0.017055\tval_Loss: 0.021331\n",
      "Train Epoch: 417/1000 (42%)\ttrain_Loss: 0.017027\tval_Loss: 0.021328\n",
      "Train Epoch: 418/1000 (42%)\ttrain_Loss: 0.016998\tval_Loss: 0.021332\n",
      "Train Epoch: 419/1000 (42%)\ttrain_Loss: 0.016968\tval_Loss: 0.021353\n",
      "Train Epoch: 420/1000 (42%)\ttrain_Loss: 0.016942\tval_Loss: 0.021344\n",
      "Train Epoch: 421/1000 (42%)\ttrain_Loss: 0.016913\tval_Loss: 0.021300\n",
      "Train Epoch: 422/1000 (42%)\ttrain_Loss: 0.016883\tval_Loss: 0.021271\n",
      "Train Epoch: 423/1000 (42%)\ttrain_Loss: 0.016856\tval_Loss: 0.021282\n",
      "Train Epoch: 424/1000 (42%)\ttrain_Loss: 0.016827\tval_Loss: 0.021295\n",
      "Train Epoch: 425/1000 (42%)\ttrain_Loss: 0.016800\tval_Loss: 0.021283\n",
      "Train Epoch: 426/1000 (42%)\ttrain_Loss: 0.016772\tval_Loss: 0.021261\n",
      "Train Epoch: 427/1000 (43%)\ttrain_Loss: 0.016743\tval_Loss: 0.021253\n",
      "Train Epoch: 428/1000 (43%)\ttrain_Loss: 0.016715\tval_Loss: 0.021252\n",
      "Train Epoch: 429/1000 (43%)\ttrain_Loss: 0.016687\tval_Loss: 0.021249\n",
      "Train Epoch: 430/1000 (43%)\ttrain_Loss: 0.016658\tval_Loss: 0.021220\n",
      "Train Epoch: 431/1000 (43%)\ttrain_Loss: 0.016630\tval_Loss: 0.021213\n",
      "Train Epoch: 432/1000 (43%)\ttrain_Loss: 0.016603\tval_Loss: 0.021206\n",
      "Train Epoch: 433/1000 (43%)\ttrain_Loss: 0.016574\tval_Loss: 0.021185\n",
      "Train Epoch: 434/1000 (43%)\ttrain_Loss: 0.016546\tval_Loss: 0.021179\n",
      "Train Epoch: 435/1000 (43%)\ttrain_Loss: 0.016519\tval_Loss: 0.021188\n",
      "Train Epoch: 436/1000 (44%)\ttrain_Loss: 0.016489\tval_Loss: 0.021197\n",
      "Train Epoch: 437/1000 (44%)\ttrain_Loss: 0.016461\tval_Loss: 0.021195\n",
      "Train Epoch: 438/1000 (44%)\ttrain_Loss: 0.016435\tval_Loss: 0.021163\n",
      "Train Epoch: 439/1000 (44%)\ttrain_Loss: 0.016406\tval_Loss: 0.021141\n",
      "Train Epoch: 440/1000 (44%)\ttrain_Loss: 0.016379\tval_Loss: 0.021133\n",
      "Train Epoch: 441/1000 (44%)\ttrain_Loss: 0.016351\tval_Loss: 0.021118\n",
      "Train Epoch: 442/1000 (44%)\ttrain_Loss: 0.016323\tval_Loss: 0.021107\n",
      "Train Epoch: 443/1000 (44%)\ttrain_Loss: 0.016294\tval_Loss: 0.021096\n",
      "Train Epoch: 444/1000 (44%)\ttrain_Loss: 0.016267\tval_Loss: 0.021114\n",
      "Train Epoch: 445/1000 (44%)\ttrain_Loss: 0.016240\tval_Loss: 0.021101\n",
      "Train Epoch: 446/1000 (44%)\ttrain_Loss: 0.016211\tval_Loss: 0.021072\n",
      "Train Epoch: 447/1000 (45%)\ttrain_Loss: 0.016183\tval_Loss: 0.021051\n",
      "Train Epoch: 448/1000 (45%)\ttrain_Loss: 0.016156\tval_Loss: 0.021037\n",
      "Train Epoch: 449/1000 (45%)\ttrain_Loss: 0.016129\tval_Loss: 0.021052\n",
      "Train Epoch: 450/1000 (45%)\ttrain_Loss: 0.016101\tval_Loss: 0.021075\n",
      "Train Epoch: 451/1000 (45%)\ttrain_Loss: 0.016074\tval_Loss: 0.021063\n",
      "Train Epoch: 452/1000 (45%)\ttrain_Loss: 0.016046\tval_Loss: 0.021008\n",
      "Train Epoch: 453/1000 (45%)\ttrain_Loss: 0.016019\tval_Loss: 0.020989\n",
      "Train Epoch: 454/1000 (45%)\ttrain_Loss: 0.015993\tval_Loss: 0.021014\n",
      "Train Epoch: 455/1000 (45%)\ttrain_Loss: 0.015965\tval_Loss: 0.021039\n",
      "Train Epoch: 456/1000 (46%)\ttrain_Loss: 0.015940\tval_Loss: 0.021029\n",
      "Train Epoch: 457/1000 (46%)\ttrain_Loss: 0.015913\tval_Loss: 0.020995\n",
      "Train Epoch: 458/1000 (46%)\ttrain_Loss: 0.015886\tval_Loss: 0.020981\n",
      "Train Epoch: 459/1000 (46%)\ttrain_Loss: 0.015859\tval_Loss: 0.020981\n",
      "Train Epoch: 460/1000 (46%)\ttrain_Loss: 0.015831\tval_Loss: 0.020969\n",
      "Train Epoch: 461/1000 (46%)\ttrain_Loss: 0.015805\tval_Loss: 0.020956\n",
      "Train Epoch: 462/1000 (46%)\ttrain_Loss: 0.015779\tval_Loss: 0.020936\n",
      "Train Epoch: 463/1000 (46%)\ttrain_Loss: 0.015750\tval_Loss: 0.020918\n",
      "Train Epoch: 464/1000 (46%)\ttrain_Loss: 0.015723\tval_Loss: 0.020934\n",
      "Train Epoch: 465/1000 (46%)\ttrain_Loss: 0.015697\tval_Loss: 0.020958\n",
      "Train Epoch: 466/1000 (46%)\ttrain_Loss: 0.015670\tval_Loss: 0.020932\n",
      "Train Epoch: 467/1000 (47%)\ttrain_Loss: 0.015642\tval_Loss: 0.020897\n",
      "Train Epoch: 468/1000 (47%)\ttrain_Loss: 0.015619\tval_Loss: 0.020911\n",
      "Train Epoch: 469/1000 (47%)\ttrain_Loss: 0.015592\tval_Loss: 0.020932\n",
      "Train Epoch: 470/1000 (47%)\ttrain_Loss: 0.015567\tval_Loss: 0.020913\n",
      "Train Epoch: 471/1000 (47%)\ttrain_Loss: 0.015541\tval_Loss: 0.020897\n",
      "Train Epoch: 472/1000 (47%)\ttrain_Loss: 0.015515\tval_Loss: 0.020905\n",
      "Train Epoch: 473/1000 (47%)\ttrain_Loss: 0.015489\tval_Loss: 0.020915\n",
      "Train Epoch: 474/1000 (47%)\ttrain_Loss: 0.015463\tval_Loss: 0.020903\n",
      "Train Epoch: 475/1000 (47%)\ttrain_Loss: 0.015437\tval_Loss: 0.020867\n",
      "Train Epoch: 476/1000 (48%)\ttrain_Loss: 0.015411\tval_Loss: 0.020863\n",
      "Train Epoch: 477/1000 (48%)\ttrain_Loss: 0.015383\tval_Loss: 0.020912\n",
      "Train Epoch: 478/1000 (48%)\ttrain_Loss: 0.015359\tval_Loss: 0.020892\n",
      "Train Epoch: 479/1000 (48%)\ttrain_Loss: 0.015335\tval_Loss: 0.020837\n",
      "Train Epoch: 480/1000 (48%)\ttrain_Loss: 0.015310\tval_Loss: 0.020808\n",
      "Train Epoch: 481/1000 (48%)\ttrain_Loss: 0.015284\tval_Loss: 0.020821\n",
      "Train Epoch: 482/1000 (48%)\ttrain_Loss: 0.015257\tval_Loss: 0.020845\n",
      "Train Epoch: 483/1000 (48%)\ttrain_Loss: 0.015231\tval_Loss: 0.020845\n",
      "Train Epoch: 484/1000 (48%)\ttrain_Loss: 0.015208\tval_Loss: 0.020836\n",
      "Train Epoch: 485/1000 (48%)\ttrain_Loss: 0.015182\tval_Loss: 0.020797\n",
      "Train Epoch: 486/1000 (48%)\ttrain_Loss: 0.015156\tval_Loss: 0.020773\n",
      "Train Epoch: 487/1000 (49%)\ttrain_Loss: 0.015131\tval_Loss: 0.020782\n",
      "Train Epoch: 488/1000 (49%)\ttrain_Loss: 0.015105\tval_Loss: 0.020797\n",
      "Train Epoch: 489/1000 (49%)\ttrain_Loss: 0.015081\tval_Loss: 0.020783\n",
      "Train Epoch: 490/1000 (49%)\ttrain_Loss: 0.015057\tval_Loss: 0.020778\n",
      "Train Epoch: 491/1000 (49%)\ttrain_Loss: 0.015030\tval_Loss: 0.020787\n",
      "Train Epoch: 492/1000 (49%)\ttrain_Loss: 0.015005\tval_Loss: 0.020809\n",
      "Train Epoch: 493/1000 (49%)\ttrain_Loss: 0.014982\tval_Loss: 0.020804\n",
      "Train Epoch: 494/1000 (49%)\ttrain_Loss: 0.014959\tval_Loss: 0.020779\n",
      "Train Epoch: 495/1000 (49%)\ttrain_Loss: 0.014934\tval_Loss: 0.020754\n",
      "Train Epoch: 496/1000 (50%)\ttrain_Loss: 0.014908\tval_Loss: 0.020734\n",
      "Train Epoch: 497/1000 (50%)\ttrain_Loss: 0.014881\tval_Loss: 0.020726\n",
      "Train Epoch: 498/1000 (50%)\ttrain_Loss: 0.014856\tval_Loss: 0.020717\n",
      "Train Epoch: 499/1000 (50%)\ttrain_Loss: 0.014834\tval_Loss: 0.020737\n",
      "Train Epoch: 500/1000 (50%)\ttrain_Loss: 0.014808\tval_Loss: 0.020728\n",
      "Train Epoch: 501/1000 (50%)\ttrain_Loss: 0.014783\tval_Loss: 0.020724\n",
      "Train Epoch: 502/1000 (50%)\ttrain_Loss: 0.014760\tval_Loss: 0.020756\n",
      "Train Epoch: 503/1000 (50%)\ttrain_Loss: 0.014736\tval_Loss: 0.020768\n",
      "Train Epoch: 504/1000 (50%)\ttrain_Loss: 0.014711\tval_Loss: 0.020744\n",
      "Train Epoch: 505/1000 (50%)\ttrain_Loss: 0.014688\tval_Loss: 0.020701\n",
      "Train Epoch: 506/1000 (50%)\ttrain_Loss: 0.014663\tval_Loss: 0.020683\n",
      "Train Epoch: 507/1000 (51%)\ttrain_Loss: 0.014637\tval_Loss: 0.020671\n",
      "Train Epoch: 508/1000 (51%)\ttrain_Loss: 0.014614\tval_Loss: 0.020656\n",
      "Train Epoch: 509/1000 (51%)\ttrain_Loss: 0.014590\tval_Loss: 0.020649\n",
      "Train Epoch: 510/1000 (51%)\ttrain_Loss: 0.014563\tval_Loss: 0.020659\n",
      "Train Epoch: 511/1000 (51%)\ttrain_Loss: 0.014537\tval_Loss: 0.020661\n",
      "Train Epoch: 512/1000 (51%)\ttrain_Loss: 0.014508\tval_Loss: 0.020658\n",
      "Train Epoch: 513/1000 (51%)\ttrain_Loss: 0.014479\tval_Loss: 0.020646\n",
      "Train Epoch: 514/1000 (51%)\ttrain_Loss: 0.014448\tval_Loss: 0.020642\n",
      "Train Epoch: 515/1000 (51%)\ttrain_Loss: 0.014416\tval_Loss: 0.020640\n",
      "Train Epoch: 516/1000 (52%)\ttrain_Loss: 0.014384\tval_Loss: 0.020610\n",
      "Train Epoch: 517/1000 (52%)\ttrain_Loss: 0.014352\tval_Loss: 0.020625\n",
      "Train Epoch: 518/1000 (52%)\ttrain_Loss: 0.014317\tval_Loss: 0.020653\n",
      "Train Epoch: 519/1000 (52%)\ttrain_Loss: 0.014281\tval_Loss: 0.020617\n",
      "Train Epoch: 520/1000 (52%)\ttrain_Loss: 0.014246\tval_Loss: 0.020594\n",
      "Train Epoch: 521/1000 (52%)\ttrain_Loss: 0.014215\tval_Loss: 0.020601\n",
      "Train Epoch: 522/1000 (52%)\ttrain_Loss: 0.014185\tval_Loss: 0.020628\n",
      "Train Epoch: 523/1000 (52%)\ttrain_Loss: 0.014158\tval_Loss: 0.020661\n",
      "Train Epoch: 524/1000 (52%)\ttrain_Loss: 0.014131\tval_Loss: 0.020657\n",
      "Train Epoch: 525/1000 (52%)\ttrain_Loss: 0.014105\tval_Loss: 0.020643\n",
      "Train Epoch: 526/1000 (52%)\ttrain_Loss: 0.014079\tval_Loss: 0.020651\n",
      "Train Epoch: 527/1000 (53%)\ttrain_Loss: 0.014055\tval_Loss: 0.020711\n",
      "Train Epoch: 528/1000 (53%)\ttrain_Loss: 0.014033\tval_Loss: 0.020736\n",
      "Train Epoch: 529/1000 (53%)\ttrain_Loss: 0.014017\tval_Loss: 0.020747\n",
      "Train Epoch: 530/1000 (53%)\ttrain_Loss: 0.014001\tval_Loss: 0.020712\n",
      "Train Epoch: 531/1000 (53%)\ttrain_Loss: 0.013981\tval_Loss: 0.020666\n",
      "Train Epoch: 532/1000 (53%)\ttrain_Loss: 0.013956\tval_Loss: 0.020669\n",
      "Train Epoch: 533/1000 (53%)\ttrain_Loss: 0.013931\tval_Loss: 0.020684\n",
      "Train Epoch: 534/1000 (53%)\ttrain_Loss: 0.013905\tval_Loss: 0.020653\n",
      "Train Epoch: 535/1000 (53%)\ttrain_Loss: 0.013877\tval_Loss: 0.020614\n",
      "Train Epoch: 536/1000 (54%)\ttrain_Loss: 0.013851\tval_Loss: 0.020596\n",
      "Train Epoch: 537/1000 (54%)\ttrain_Loss: 0.013829\tval_Loss: 0.020590\n",
      "Train Epoch: 538/1000 (54%)\ttrain_Loss: 0.013806\tval_Loss: 0.020561\n",
      "Train Epoch: 539/1000 (54%)\ttrain_Loss: 0.013782\tval_Loss: 0.020521\n",
      "Train Epoch: 540/1000 (54%)\ttrain_Loss: 0.013757\tval_Loss: 0.020497\n",
      "Train Epoch: 541/1000 (54%)\ttrain_Loss: 0.013738\tval_Loss: 0.020465\n",
      "Train Epoch: 542/1000 (54%)\ttrain_Loss: 0.013720\tval_Loss: 0.020441\n",
      "Train Epoch: 543/1000 (54%)\ttrain_Loss: 0.013699\tval_Loss: 0.020436\n",
      "Train Epoch: 544/1000 (54%)\ttrain_Loss: 0.013676\tval_Loss: 0.020462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 545/1000 (54%)\ttrain_Loss: 0.013652\tval_Loss: 0.020488\n",
      "Train Epoch: 546/1000 (55%)\ttrain_Loss: 0.013633\tval_Loss: 0.020477\n",
      "Train Epoch: 547/1000 (55%)\ttrain_Loss: 0.013611\tval_Loss: 0.020425\n",
      "Train Epoch: 548/1000 (55%)\ttrain_Loss: 0.013591\tval_Loss: 0.020423\n",
      "Train Epoch: 549/1000 (55%)\ttrain_Loss: 0.013570\tval_Loss: 0.020437\n",
      "Train Epoch: 550/1000 (55%)\ttrain_Loss: 0.013548\tval_Loss: 0.020426\n",
      "Train Epoch: 551/1000 (55%)\ttrain_Loss: 0.013526\tval_Loss: 0.020402\n",
      "Train Epoch: 552/1000 (55%)\ttrain_Loss: 0.013503\tval_Loss: 0.020398\n",
      "Train Epoch: 553/1000 (55%)\ttrain_Loss: 0.013482\tval_Loss: 0.020424\n",
      "Train Epoch: 554/1000 (55%)\ttrain_Loss: 0.013462\tval_Loss: 0.020462\n",
      "Train Epoch: 555/1000 (55%)\ttrain_Loss: 0.013440\tval_Loss: 0.020475\n",
      "Train Epoch: 556/1000 (56%)\ttrain_Loss: 0.013419\tval_Loss: 0.020458\n",
      "Train Epoch: 557/1000 (56%)\ttrain_Loss: 0.013397\tval_Loss: 0.020461\n",
      "Train Epoch: 558/1000 (56%)\ttrain_Loss: 0.013377\tval_Loss: 0.020476\n",
      "Train Epoch: 559/1000 (56%)\ttrain_Loss: 0.013357\tval_Loss: 0.020481\n",
      "Train Epoch: 560/1000 (56%)\ttrain_Loss: 0.013338\tval_Loss: 0.020477\n",
      "Train Epoch: 561/1000 (56%)\ttrain_Loss: 0.013316\tval_Loss: 0.020463\n",
      "Train Epoch: 562/1000 (56%)\ttrain_Loss: 0.013296\tval_Loss: 0.020480\n",
      "Train Epoch: 563/1000 (56%)\ttrain_Loss: 0.013277\tval_Loss: 0.020496\n",
      "Train Epoch: 564/1000 (56%)\ttrain_Loss: 0.013257\tval_Loss: 0.020506\n",
      "Train Epoch: 565/1000 (56%)\ttrain_Loss: 0.013238\tval_Loss: 0.020482\n",
      "Train Epoch: 566/1000 (56%)\ttrain_Loss: 0.013218\tval_Loss: 0.020465\n",
      "Train Epoch: 567/1000 (57%)\ttrain_Loss: 0.013198\tval_Loss: 0.020486\n",
      "Train Epoch: 568/1000 (57%)\ttrain_Loss: 0.013178\tval_Loss: 0.020487\n",
      "Train Epoch: 569/1000 (57%)\ttrain_Loss: 0.013158\tval_Loss: 0.020445\n",
      "Train Epoch: 570/1000 (57%)\ttrain_Loss: 0.013139\tval_Loss: 0.020439\n",
      "Train Epoch: 571/1000 (57%)\ttrain_Loss: 0.013121\tval_Loss: 0.020464\n",
      "Train Epoch: 572/1000 (57%)\ttrain_Loss: 0.013102\tval_Loss: 0.020479\n",
      "Train Epoch: 573/1000 (57%)\ttrain_Loss: 0.013082\tval_Loss: 0.020445\n",
      "Train Epoch: 574/1000 (57%)\ttrain_Loss: 0.013064\tval_Loss: 0.020427\n",
      "Train Epoch: 575/1000 (57%)\ttrain_Loss: 0.013045\tval_Loss: 0.020439\n",
      "Train Epoch: 576/1000 (57%)\ttrain_Loss: 0.013025\tval_Loss: 0.020470\n",
      "Train Epoch: 577/1000 (58%)\ttrain_Loss: 0.013007\tval_Loss: 0.020470\n",
      "Train Epoch: 578/1000 (58%)\ttrain_Loss: 0.012989\tval_Loss: 0.020435\n",
      "Train Epoch: 579/1000 (58%)\ttrain_Loss: 0.012970\tval_Loss: 0.020416\n",
      "Train Epoch: 580/1000 (58%)\ttrain_Loss: 0.012951\tval_Loss: 0.020421\n",
      "Train Epoch: 581/1000 (58%)\ttrain_Loss: 0.012933\tval_Loss: 0.020428\n",
      "Train Epoch: 582/1000 (58%)\ttrain_Loss: 0.012915\tval_Loss: 0.020412\n",
      "Train Epoch: 583/1000 (58%)\ttrain_Loss: 0.012896\tval_Loss: 0.020380\n",
      "Train Epoch: 584/1000 (58%)\ttrain_Loss: 0.012877\tval_Loss: 0.020383\n",
      "Train Epoch: 585/1000 (58%)\ttrain_Loss: 0.012860\tval_Loss: 0.020409\n",
      "Train Epoch: 586/1000 (58%)\ttrain_Loss: 0.012841\tval_Loss: 0.020393\n",
      "Train Epoch: 587/1000 (59%)\ttrain_Loss: 0.012822\tval_Loss: 0.020352\n",
      "Train Epoch: 588/1000 (59%)\ttrain_Loss: 0.012804\tval_Loss: 0.020350\n",
      "Train Epoch: 589/1000 (59%)\ttrain_Loss: 0.012787\tval_Loss: 0.020425\n",
      "Train Epoch: 590/1000 (59%)\ttrain_Loss: 0.012769\tval_Loss: 0.020423\n",
      "Train Epoch: 591/1000 (59%)\ttrain_Loss: 0.012752\tval_Loss: 0.020381\n",
      "Train Epoch: 592/1000 (59%)\ttrain_Loss: 0.012732\tval_Loss: 0.020359\n",
      "Train Epoch: 593/1000 (59%)\ttrain_Loss: 0.012716\tval_Loss: 0.020353\n",
      "Train Epoch: 594/1000 (59%)\ttrain_Loss: 0.012698\tval_Loss: 0.020357\n",
      "Train Epoch: 595/1000 (59%)\ttrain_Loss: 0.012680\tval_Loss: 0.020319\n",
      "Train Epoch: 596/1000 (60%)\ttrain_Loss: 0.012664\tval_Loss: 0.020302\n",
      "Train Epoch: 597/1000 (60%)\ttrain_Loss: 0.012647\tval_Loss: 0.020325\n",
      "Train Epoch: 598/1000 (60%)\ttrain_Loss: 0.012630\tval_Loss: 0.020310\n",
      "Train Epoch: 599/1000 (60%)\ttrain_Loss: 0.012611\tval_Loss: 0.020289\n",
      "Train Epoch: 600/1000 (60%)\ttrain_Loss: 0.012594\tval_Loss: 0.020295\n",
      "Train Epoch: 601/1000 (60%)\ttrain_Loss: 0.012579\tval_Loss: 0.020344\n",
      "Train Epoch: 602/1000 (60%)\ttrain_Loss: 0.012561\tval_Loss: 0.020375\n",
      "Train Epoch: 603/1000 (60%)\ttrain_Loss: 0.012542\tval_Loss: 0.020339\n",
      "Train Epoch: 604/1000 (60%)\ttrain_Loss: 0.012526\tval_Loss: 0.020343\n",
      "Train Epoch: 605/1000 (60%)\ttrain_Loss: 0.012507\tval_Loss: 0.020328\n",
      "Train Epoch: 606/1000 (60%)\ttrain_Loss: 0.012489\tval_Loss: 0.020304\n",
      "Train Epoch: 607/1000 (61%)\ttrain_Loss: 0.012473\tval_Loss: 0.020285\n",
      "Train Epoch: 608/1000 (61%)\ttrain_Loss: 0.012456\tval_Loss: 0.020311\n",
      "Train Epoch: 609/1000 (61%)\ttrain_Loss: 0.012439\tval_Loss: 0.020300\n",
      "Train Epoch: 610/1000 (61%)\ttrain_Loss: 0.012422\tval_Loss: 0.020284\n",
      "Train Epoch: 611/1000 (61%)\ttrain_Loss: 0.012407\tval_Loss: 0.020333\n",
      "Train Epoch: 612/1000 (61%)\ttrain_Loss: 0.012390\tval_Loss: 0.020335\n",
      "Train Epoch: 613/1000 (61%)\ttrain_Loss: 0.012373\tval_Loss: 0.020302\n",
      "Train Epoch: 614/1000 (61%)\ttrain_Loss: 0.012357\tval_Loss: 0.020308\n",
      "Train Epoch: 615/1000 (61%)\ttrain_Loss: 0.012339\tval_Loss: 0.020305\n",
      "Train Epoch: 616/1000 (62%)\ttrain_Loss: 0.012324\tval_Loss: 0.020313\n",
      "Train Epoch: 617/1000 (62%)\ttrain_Loss: 0.012307\tval_Loss: 0.020345\n",
      "Train Epoch: 618/1000 (62%)\ttrain_Loss: 0.012291\tval_Loss: 0.020335\n",
      "Train Epoch: 619/1000 (62%)\ttrain_Loss: 0.012272\tval_Loss: 0.020332\n",
      "Train Epoch: 620/1000 (62%)\ttrain_Loss: 0.012256\tval_Loss: 0.020327\n",
      "Train Epoch: 621/1000 (62%)\ttrain_Loss: 0.012243\tval_Loss: 0.020349\n",
      "Train Epoch: 622/1000 (62%)\ttrain_Loss: 0.012225\tval_Loss: 0.020328\n",
      "Train Epoch: 623/1000 (62%)\ttrain_Loss: 0.012208\tval_Loss: 0.020255\n",
      "Train Epoch: 624/1000 (62%)\ttrain_Loss: 0.012191\tval_Loss: 0.020243\n",
      "Train Epoch: 625/1000 (62%)\ttrain_Loss: 0.012175\tval_Loss: 0.020246\n",
      "Train Epoch: 626/1000 (62%)\ttrain_Loss: 0.012161\tval_Loss: 0.020230\n",
      "Train Epoch: 627/1000 (63%)\ttrain_Loss: 0.012143\tval_Loss: 0.020234\n",
      "Train Epoch: 628/1000 (63%)\ttrain_Loss: 0.012127\tval_Loss: 0.020250\n",
      "Train Epoch: 629/1000 (63%)\ttrain_Loss: 0.012111\tval_Loss: 0.020236\n",
      "Train Epoch: 630/1000 (63%)\ttrain_Loss: 0.012096\tval_Loss: 0.020199\n",
      "Train Epoch: 631/1000 (63%)\ttrain_Loss: 0.012080\tval_Loss: 0.020174\n",
      "Train Epoch: 632/1000 (63%)\ttrain_Loss: 0.012065\tval_Loss: 0.020151\n",
      "Train Epoch: 633/1000 (63%)\ttrain_Loss: 0.012048\tval_Loss: 0.020192\n",
      "Train Epoch: 634/1000 (63%)\ttrain_Loss: 0.012031\tval_Loss: 0.020198\n",
      "Train Epoch: 635/1000 (63%)\ttrain_Loss: 0.012013\tval_Loss: 0.020224\n",
      "Train Epoch: 636/1000 (64%)\ttrain_Loss: 0.011997\tval_Loss: 0.020211\n",
      "Train Epoch: 637/1000 (64%)\ttrain_Loss: 0.011983\tval_Loss: 0.020200\n",
      "Train Epoch: 638/1000 (64%)\ttrain_Loss: 0.011967\tval_Loss: 0.020188\n",
      "Train Epoch: 639/1000 (64%)\ttrain_Loss: 0.011952\tval_Loss: 0.020175\n",
      "Train Epoch: 640/1000 (64%)\ttrain_Loss: 0.011938\tval_Loss: 0.020168\n",
      "Train Epoch: 641/1000 (64%)\ttrain_Loss: 0.011923\tval_Loss: 0.020132\n",
      "Train Epoch: 642/1000 (64%)\ttrain_Loss: 0.011905\tval_Loss: 0.020131\n",
      "Train Epoch: 643/1000 (64%)\ttrain_Loss: 0.011889\tval_Loss: 0.020164\n",
      "Train Epoch: 644/1000 (64%)\ttrain_Loss: 0.011872\tval_Loss: 0.020188\n",
      "Train Epoch: 645/1000 (64%)\ttrain_Loss: 0.011859\tval_Loss: 0.020167\n",
      "Train Epoch: 646/1000 (64%)\ttrain_Loss: 0.011845\tval_Loss: 0.020150\n",
      "Train Epoch: 647/1000 (65%)\ttrain_Loss: 0.011830\tval_Loss: 0.020134\n",
      "Train Epoch: 648/1000 (65%)\ttrain_Loss: 0.011811\tval_Loss: 0.020125\n",
      "Train Epoch: 649/1000 (65%)\ttrain_Loss: 0.011797\tval_Loss: 0.020105\n",
      "Train Epoch: 650/1000 (65%)\ttrain_Loss: 0.011782\tval_Loss: 0.020080\n",
      "Train Epoch: 651/1000 (65%)\ttrain_Loss: 0.011765\tval_Loss: 0.020082\n",
      "Train Epoch: 652/1000 (65%)\ttrain_Loss: 0.011751\tval_Loss: 0.020080\n",
      "Train Epoch: 653/1000 (65%)\ttrain_Loss: 0.011736\tval_Loss: 0.020111\n",
      "Train Epoch: 654/1000 (65%)\ttrain_Loss: 0.011720\tval_Loss: 0.020088\n",
      "Train Epoch: 655/1000 (65%)\ttrain_Loss: 0.011705\tval_Loss: 0.020043\n",
      "Train Epoch: 656/1000 (66%)\ttrain_Loss: 0.011689\tval_Loss: 0.020011\n",
      "Train Epoch: 657/1000 (66%)\ttrain_Loss: 0.011676\tval_Loss: 0.020003\n",
      "Train Epoch: 658/1000 (66%)\ttrain_Loss: 0.011661\tval_Loss: 0.020010\n",
      "Train Epoch: 659/1000 (66%)\ttrain_Loss: 0.011645\tval_Loss: 0.020023\n",
      "Train Epoch: 660/1000 (66%)\ttrain_Loss: 0.011629\tval_Loss: 0.020023\n",
      "Train Epoch: 661/1000 (66%)\ttrain_Loss: 0.011615\tval_Loss: 0.020007\n",
      "Train Epoch: 662/1000 (66%)\ttrain_Loss: 0.011599\tval_Loss: 0.019966\n",
      "Train Epoch: 663/1000 (66%)\ttrain_Loss: 0.011584\tval_Loss: 0.019958\n",
      "Train Epoch: 664/1000 (66%)\ttrain_Loss: 0.011570\tval_Loss: 0.019979\n",
      "Train Epoch: 665/1000 (66%)\ttrain_Loss: 0.011556\tval_Loss: 0.019980\n",
      "Train Epoch: 666/1000 (66%)\ttrain_Loss: 0.011540\tval_Loss: 0.019936\n",
      "Train Epoch: 667/1000 (67%)\ttrain_Loss: 0.011524\tval_Loss: 0.019943\n",
      "Train Epoch: 668/1000 (67%)\ttrain_Loss: 0.011511\tval_Loss: 0.019962\n",
      "Train Epoch: 669/1000 (67%)\ttrain_Loss: 0.011497\tval_Loss: 0.019952\n",
      "Train Epoch: 670/1000 (67%)\ttrain_Loss: 0.011483\tval_Loss: 0.019914\n",
      "Train Epoch: 671/1000 (67%)\ttrain_Loss: 0.011466\tval_Loss: 0.019892\n",
      "Train Epoch: 672/1000 (67%)\ttrain_Loss: 0.011452\tval_Loss: 0.019906\n",
      "Train Epoch: 673/1000 (67%)\ttrain_Loss: 0.011438\tval_Loss: 0.019921\n",
      "Train Epoch: 674/1000 (67%)\ttrain_Loss: 0.011424\tval_Loss: 0.019929\n",
      "Train Epoch: 675/1000 (67%)\ttrain_Loss: 0.011410\tval_Loss: 0.019882\n",
      "Train Epoch: 676/1000 (68%)\ttrain_Loss: 0.011396\tval_Loss: 0.019858\n",
      "Train Epoch: 677/1000 (68%)\ttrain_Loss: 0.011383\tval_Loss: 0.019852\n",
      "Train Epoch: 678/1000 (68%)\ttrain_Loss: 0.011369\tval_Loss: 0.019856\n",
      "Train Epoch: 679/1000 (68%)\ttrain_Loss: 0.011352\tval_Loss: 0.019852\n",
      "Train Epoch: 680/1000 (68%)\ttrain_Loss: 0.011339\tval_Loss: 0.019856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 681/1000 (68%)\ttrain_Loss: 0.011323\tval_Loss: 0.019845\n",
      "Train Epoch: 682/1000 (68%)\ttrain_Loss: 0.011314\tval_Loss: 0.019797\n",
      "Train Epoch: 683/1000 (68%)\ttrain_Loss: 0.011298\tval_Loss: 0.019780\n",
      "Train Epoch: 684/1000 (68%)\ttrain_Loss: 0.011281\tval_Loss: 0.019810\n",
      "Train Epoch: 685/1000 (68%)\ttrain_Loss: 0.011269\tval_Loss: 0.019823\n",
      "Train Epoch: 686/1000 (68%)\ttrain_Loss: 0.011255\tval_Loss: 0.019803\n",
      "Train Epoch: 687/1000 (69%)\ttrain_Loss: 0.011241\tval_Loss: 0.019767\n",
      "Train Epoch: 688/1000 (69%)\ttrain_Loss: 0.011226\tval_Loss: 0.019752\n",
      "Train Epoch: 689/1000 (69%)\ttrain_Loss: 0.011209\tval_Loss: 0.019730\n",
      "Train Epoch: 690/1000 (69%)\ttrain_Loss: 0.011193\tval_Loss: 0.019717\n",
      "Train Epoch: 691/1000 (69%)\ttrain_Loss: 0.011176\tval_Loss: 0.019717\n",
      "Train Epoch: 692/1000 (69%)\ttrain_Loss: 0.011163\tval_Loss: 0.019694\n",
      "Train Epoch: 693/1000 (69%)\ttrain_Loss: 0.011148\tval_Loss: 0.019669\n",
      "Train Epoch: 694/1000 (69%)\ttrain_Loss: 0.011130\tval_Loss: 0.019665\n",
      "Train Epoch: 695/1000 (69%)\ttrain_Loss: 0.011116\tval_Loss: 0.019655\n",
      "Train Epoch: 696/1000 (70%)\ttrain_Loss: 0.011100\tval_Loss: 0.019656\n",
      "Train Epoch: 697/1000 (70%)\ttrain_Loss: 0.011088\tval_Loss: 0.019654\n",
      "Train Epoch: 698/1000 (70%)\ttrain_Loss: 0.011073\tval_Loss: 0.019623\n",
      "Train Epoch: 699/1000 (70%)\ttrain_Loss: 0.011057\tval_Loss: 0.019608\n",
      "Train Epoch: 700/1000 (70%)\ttrain_Loss: 0.011044\tval_Loss: 0.019607\n",
      "Train Epoch: 701/1000 (70%)\ttrain_Loss: 0.011029\tval_Loss: 0.019595\n",
      "Train Epoch: 702/1000 (70%)\ttrain_Loss: 0.011014\tval_Loss: 0.019591\n",
      "Train Epoch: 703/1000 (70%)\ttrain_Loss: 0.010999\tval_Loss: 0.019567\n",
      "Train Epoch: 704/1000 (70%)\ttrain_Loss: 0.010987\tval_Loss: 0.019568\n",
      "Train Epoch: 705/1000 (70%)\ttrain_Loss: 0.010973\tval_Loss: 0.019592\n",
      "Train Epoch: 706/1000 (70%)\ttrain_Loss: 0.010960\tval_Loss: 0.019560\n",
      "Train Epoch: 707/1000 (71%)\ttrain_Loss: 0.010949\tval_Loss: 0.019516\n",
      "Train Epoch: 708/1000 (71%)\ttrain_Loss: 0.010935\tval_Loss: 0.019529\n",
      "Train Epoch: 709/1000 (71%)\ttrain_Loss: 0.010916\tval_Loss: 0.019570\n",
      "Train Epoch: 710/1000 (71%)\ttrain_Loss: 0.010905\tval_Loss: 0.019566\n",
      "Train Epoch: 711/1000 (71%)\ttrain_Loss: 0.010894\tval_Loss: 0.019511\n",
      "Train Epoch: 712/1000 (71%)\ttrain_Loss: 0.010881\tval_Loss: 0.019486\n",
      "Train Epoch: 713/1000 (71%)\ttrain_Loss: 0.010865\tval_Loss: 0.019537\n",
      "Train Epoch: 714/1000 (71%)\ttrain_Loss: 0.010855\tval_Loss: 0.019552\n",
      "Train Epoch: 715/1000 (71%)\ttrain_Loss: 0.010845\tval_Loss: 0.019545\n",
      "Train Epoch: 716/1000 (72%)\ttrain_Loss: 0.010834\tval_Loss: 0.019536\n",
      "Train Epoch: 717/1000 (72%)\ttrain_Loss: 0.010820\tval_Loss: 0.019534\n",
      "Train Epoch: 718/1000 (72%)\ttrain_Loss: 0.010803\tval_Loss: 0.019529\n",
      "Train Epoch: 719/1000 (72%)\ttrain_Loss: 0.010791\tval_Loss: 0.019550\n",
      "Train Epoch: 720/1000 (72%)\ttrain_Loss: 0.010778\tval_Loss: 0.019546\n",
      "Train Epoch: 721/1000 (72%)\ttrain_Loss: 0.010766\tval_Loss: 0.019514\n",
      "Train Epoch: 722/1000 (72%)\ttrain_Loss: 0.010748\tval_Loss: 0.019501\n",
      "Train Epoch: 723/1000 (72%)\ttrain_Loss: 0.010741\tval_Loss: 0.019520\n",
      "Train Epoch: 724/1000 (72%)\ttrain_Loss: 0.010731\tval_Loss: 0.019538\n",
      "Train Epoch: 725/1000 (72%)\ttrain_Loss: 0.010714\tval_Loss: 0.019539\n",
      "Train Epoch: 726/1000 (72%)\ttrain_Loss: 0.010702\tval_Loss: 0.019532\n",
      "Train Epoch: 727/1000 (73%)\ttrain_Loss: 0.010690\tval_Loss: 0.019550\n",
      "Train Epoch: 728/1000 (73%)\ttrain_Loss: 0.010680\tval_Loss: 0.019530\n",
      "Train Epoch: 729/1000 (73%)\ttrain_Loss: 0.010661\tval_Loss: 0.019496\n",
      "Train Epoch: 730/1000 (73%)\ttrain_Loss: 0.010654\tval_Loss: 0.019488\n",
      "Train Epoch: 731/1000 (73%)\ttrain_Loss: 0.010645\tval_Loss: 0.019509\n",
      "Train Epoch: 732/1000 (73%)\ttrain_Loss: 0.010629\tval_Loss: 0.019512\n",
      "Train Epoch: 733/1000 (73%)\ttrain_Loss: 0.010614\tval_Loss: 0.019503\n",
      "Train Epoch: 734/1000 (73%)\ttrain_Loss: 0.010611\tval_Loss: 0.019510\n",
      "Train Epoch: 735/1000 (73%)\ttrain_Loss: 0.010600\tval_Loss: 0.019511\n",
      "Train Epoch: 736/1000 (74%)\ttrain_Loss: 0.010587\tval_Loss: 0.019484\n",
      "Train Epoch: 737/1000 (74%)\ttrain_Loss: 0.010568\tval_Loss: 0.019445\n",
      "Train Epoch: 738/1000 (74%)\ttrain_Loss: 0.010557\tval_Loss: 0.019445\n",
      "Train Epoch: 739/1000 (74%)\ttrain_Loss: 0.010553\tval_Loss: 0.019446\n",
      "Train Epoch: 740/1000 (74%)\ttrain_Loss: 0.010546\tval_Loss: 0.019465\n",
      "Train Epoch: 741/1000 (74%)\ttrain_Loss: 0.010519\tval_Loss: 0.019484\n",
      "Train Epoch: 742/1000 (74%)\ttrain_Loss: 0.010508\tval_Loss: 0.019462\n",
      "Train Epoch: 743/1000 (74%)\ttrain_Loss: 0.010497\tval_Loss: 0.019414\n",
      "Train Epoch: 744/1000 (74%)\ttrain_Loss: 0.010482\tval_Loss: 0.019413\n",
      "Train Epoch: 745/1000 (74%)\ttrain_Loss: 0.010472\tval_Loss: 0.019447\n",
      "Train Epoch: 746/1000 (74%)\ttrain_Loss: 0.010459\tval_Loss: 0.019462\n",
      "Train Epoch: 747/1000 (75%)\ttrain_Loss: 0.010450\tval_Loss: 0.019445\n",
      "Train Epoch: 748/1000 (75%)\ttrain_Loss: 0.010438\tval_Loss: 0.019453\n",
      "Train Epoch: 749/1000 (75%)\ttrain_Loss: 0.010425\tval_Loss: 0.019478\n",
      "Train Epoch: 750/1000 (75%)\ttrain_Loss: 0.010414\tval_Loss: 0.019447\n",
      "Train Epoch: 751/1000 (75%)\ttrain_Loss: 0.010404\tval_Loss: 0.019397\n",
      "Train Epoch: 752/1000 (75%)\ttrain_Loss: 0.010389\tval_Loss: 0.019395\n",
      "Train Epoch: 753/1000 (75%)\ttrain_Loss: 0.010380\tval_Loss: 0.019441\n",
      "Train Epoch: 754/1000 (75%)\ttrain_Loss: 0.010372\tval_Loss: 0.019450\n",
      "Train Epoch: 755/1000 (75%)\ttrain_Loss: 0.010356\tval_Loss: 0.019432\n",
      "Train Epoch: 756/1000 (76%)\ttrain_Loss: 0.010348\tval_Loss: 0.019427\n",
      "Train Epoch: 757/1000 (76%)\ttrain_Loss: 0.010334\tval_Loss: 0.019440\n",
      "Train Epoch: 758/1000 (76%)\ttrain_Loss: 0.010326\tval_Loss: 0.019429\n",
      "Train Epoch: 759/1000 (76%)\ttrain_Loss: 0.010317\tval_Loss: 0.019401\n",
      "Train Epoch: 760/1000 (76%)\ttrain_Loss: 0.010306\tval_Loss: 0.019409\n",
      "Train Epoch: 761/1000 (76%)\ttrain_Loss: 0.010296\tval_Loss: 0.019429\n",
      "Train Epoch: 762/1000 (76%)\ttrain_Loss: 0.010283\tval_Loss: 0.019429\n",
      "Train Epoch: 763/1000 (76%)\ttrain_Loss: 0.010265\tval_Loss: 0.019394\n",
      "Train Epoch: 764/1000 (76%)\ttrain_Loss: 0.010258\tval_Loss: 0.019383\n",
      "Train Epoch: 765/1000 (76%)\ttrain_Loss: 0.010248\tval_Loss: 0.019420\n",
      "Train Epoch: 766/1000 (76%)\ttrain_Loss: 0.010232\tval_Loss: 0.019453\n",
      "Train Epoch: 767/1000 (77%)\ttrain_Loss: 0.010223\tval_Loss: 0.019397\n",
      "Train Epoch: 768/1000 (77%)\ttrain_Loss: 0.010212\tval_Loss: 0.019379\n",
      "Train Epoch: 769/1000 (77%)\ttrain_Loss: 0.010202\tval_Loss: 0.019393\n",
      "Train Epoch: 770/1000 (77%)\ttrain_Loss: 0.010186\tval_Loss: 0.019390\n",
      "Train Epoch: 771/1000 (77%)\ttrain_Loss: 0.010183\tval_Loss: 0.019391\n",
      "Train Epoch: 772/1000 (77%)\ttrain_Loss: 0.010174\tval_Loss: 0.019393\n",
      "Train Epoch: 773/1000 (77%)\ttrain_Loss: 0.010155\tval_Loss: 0.019406\n",
      "Train Epoch: 774/1000 (77%)\ttrain_Loss: 0.010145\tval_Loss: 0.019391\n",
      "Train Epoch: 775/1000 (77%)\ttrain_Loss: 0.010136\tval_Loss: 0.019373\n",
      "Train Epoch: 776/1000 (78%)\ttrain_Loss: 0.010124\tval_Loss: 0.019369\n",
      "Train Epoch: 777/1000 (78%)\ttrain_Loss: 0.010114\tval_Loss: 0.019377\n",
      "Train Epoch: 778/1000 (78%)\ttrain_Loss: 0.010100\tval_Loss: 0.019381\n",
      "Train Epoch: 779/1000 (78%)\ttrain_Loss: 0.010096\tval_Loss: 0.019381\n",
      "Train Epoch: 780/1000 (78%)\ttrain_Loss: 0.010089\tval_Loss: 0.019359\n",
      "Train Epoch: 781/1000 (78%)\ttrain_Loss: 0.010077\tval_Loss: 0.019348\n",
      "Train Epoch: 782/1000 (78%)\ttrain_Loss: 0.010062\tval_Loss: 0.019362\n",
      "Train Epoch: 783/1000 (78%)\ttrain_Loss: 0.010047\tval_Loss: 0.019381\n",
      "Train Epoch: 784/1000 (78%)\ttrain_Loss: 0.010033\tval_Loss: 0.019393\n",
      "Train Epoch: 785/1000 (78%)\ttrain_Loss: 0.010034\tval_Loss: 0.019357\n",
      "Train Epoch: 786/1000 (78%)\ttrain_Loss: 0.010026\tval_Loss: 0.019345\n",
      "Train Epoch: 787/1000 (79%)\ttrain_Loss: 0.010009\tval_Loss: 0.019354\n",
      "Train Epoch: 788/1000 (79%)\ttrain_Loss: 0.009994\tval_Loss: 0.019359\n",
      "Train Epoch: 789/1000 (79%)\ttrain_Loss: 0.009991\tval_Loss: 0.019349\n",
      "Train Epoch: 790/1000 (79%)\ttrain_Loss: 0.009981\tval_Loss: 0.019355\n",
      "Train Epoch: 791/1000 (79%)\ttrain_Loss: 0.009971\tval_Loss: 0.019391\n",
      "Train Epoch: 792/1000 (79%)\ttrain_Loss: 0.009957\tval_Loss: 0.019417\n",
      "Train Epoch: 793/1000 (79%)\ttrain_Loss: 0.009944\tval_Loss: 0.019380\n",
      "Train Epoch: 794/1000 (79%)\ttrain_Loss: 0.009928\tval_Loss: 0.019349\n",
      "Train Epoch: 795/1000 (79%)\ttrain_Loss: 0.009919\tval_Loss: 0.019349\n",
      "Train Epoch: 796/1000 (80%)\ttrain_Loss: 0.009908\tval_Loss: 0.019381\n",
      "Train Epoch: 797/1000 (80%)\ttrain_Loss: 0.009901\tval_Loss: 0.019383\n",
      "Train Epoch: 798/1000 (80%)\ttrain_Loss: 0.009890\tval_Loss: 0.019362\n",
      "Train Epoch: 799/1000 (80%)\ttrain_Loss: 0.009876\tval_Loss: 0.019346\n",
      "Train Epoch: 800/1000 (80%)\ttrain_Loss: 0.009870\tval_Loss: 0.019360\n",
      "Train Epoch: 801/1000 (80%)\ttrain_Loss: 0.009860\tval_Loss: 0.019360\n",
      "Train Epoch: 802/1000 (80%)\ttrain_Loss: 0.009846\tval_Loss: 0.019343\n",
      "Train Epoch: 803/1000 (80%)\ttrain_Loss: 0.009836\tval_Loss: 0.019336\n",
      "Train Epoch: 804/1000 (80%)\ttrain_Loss: 0.009826\tval_Loss: 0.019329\n",
      "Train Epoch: 805/1000 (80%)\ttrain_Loss: 0.009814\tval_Loss: 0.019324\n",
      "Train Epoch: 806/1000 (80%)\ttrain_Loss: 0.009808\tval_Loss: 0.019318\n",
      "Train Epoch: 807/1000 (81%)\ttrain_Loss: 0.009795\tval_Loss: 0.019324\n",
      "Train Epoch: 808/1000 (81%)\ttrain_Loss: 0.009782\tval_Loss: 0.019331\n",
      "Train Epoch: 809/1000 (81%)\ttrain_Loss: 0.009774\tval_Loss: 0.019335\n",
      "Train Epoch: 810/1000 (81%)\ttrain_Loss: 0.009765\tval_Loss: 0.019304\n",
      "Train Epoch: 811/1000 (81%)\ttrain_Loss: 0.009749\tval_Loss: 0.019275\n",
      "Train Epoch: 812/1000 (81%)\ttrain_Loss: 0.009743\tval_Loss: 0.019260\n",
      "Train Epoch: 813/1000 (81%)\ttrain_Loss: 0.009734\tval_Loss: 0.019290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 814/1000 (81%)\ttrain_Loss: 0.009723\tval_Loss: 0.019313\n",
      "Train Epoch: 815/1000 (81%)\ttrain_Loss: 0.009713\tval_Loss: 0.019327\n",
      "Train Epoch: 816/1000 (82%)\ttrain_Loss: 0.009703\tval_Loss: 0.019304\n",
      "Train Epoch: 817/1000 (82%)\ttrain_Loss: 0.009691\tval_Loss: 0.019273\n",
      "Train Epoch: 818/1000 (82%)\ttrain_Loss: 0.009681\tval_Loss: 0.019254\n",
      "Train Epoch: 819/1000 (82%)\ttrain_Loss: 0.009671\tval_Loss: 0.019263\n",
      "Train Epoch: 820/1000 (82%)\ttrain_Loss: 0.009664\tval_Loss: 0.019274\n",
      "Train Epoch: 821/1000 (82%)\ttrain_Loss: 0.009651\tval_Loss: 0.019271\n",
      "Train Epoch: 822/1000 (82%)\ttrain_Loss: 0.009638\tval_Loss: 0.019277\n",
      "Train Epoch: 823/1000 (82%)\ttrain_Loss: 0.009631\tval_Loss: 0.019294\n",
      "Train Epoch: 824/1000 (82%)\ttrain_Loss: 0.009622\tval_Loss: 0.019304\n",
      "Train Epoch: 825/1000 (82%)\ttrain_Loss: 0.009612\tval_Loss: 0.019286\n",
      "Train Epoch: 826/1000 (82%)\ttrain_Loss: 0.009604\tval_Loss: 0.019268\n",
      "Train Epoch: 827/1000 (83%)\ttrain_Loss: 0.009589\tval_Loss: 0.019302\n",
      "Train Epoch: 828/1000 (83%)\ttrain_Loss: 0.009586\tval_Loss: 0.019328\n",
      "Train Epoch: 829/1000 (83%)\ttrain_Loss: 0.009574\tval_Loss: 0.019299\n",
      "Train Epoch: 830/1000 (83%)\ttrain_Loss: 0.009559\tval_Loss: 0.019301\n",
      "Train Epoch: 831/1000 (83%)\ttrain_Loss: 0.009558\tval_Loss: 0.019355\n",
      "Train Epoch: 832/1000 (83%)\ttrain_Loss: 0.009553\tval_Loss: 0.019348\n",
      "Train Epoch: 833/1000 (83%)\ttrain_Loss: 0.009539\tval_Loss: 0.019309\n",
      "Train Epoch: 834/1000 (83%)\ttrain_Loss: 0.009526\tval_Loss: 0.019309\n",
      "Train Epoch: 835/1000 (83%)\ttrain_Loss: 0.009509\tval_Loss: 0.019335\n",
      "Train Epoch: 836/1000 (84%)\ttrain_Loss: 0.009514\tval_Loss: 0.019322\n",
      "Train Epoch: 837/1000 (84%)\ttrain_Loss: 0.009506\tval_Loss: 0.019308\n",
      "Train Epoch: 838/1000 (84%)\ttrain_Loss: 0.009492\tval_Loss: 0.019322\n",
      "Train Epoch: 839/1000 (84%)\ttrain_Loss: 0.009479\tval_Loss: 0.019355\n",
      "Train Epoch: 840/1000 (84%)\ttrain_Loss: 0.009472\tval_Loss: 0.019342\n",
      "Train Epoch: 841/1000 (84%)\ttrain_Loss: 0.009458\tval_Loss: 0.019321\n",
      "Train Epoch: 842/1000 (84%)\ttrain_Loss: 0.009450\tval_Loss: 0.019328\n",
      "Train Epoch: 843/1000 (84%)\ttrain_Loss: 0.009447\tval_Loss: 0.019343\n",
      "Train Epoch: 844/1000 (84%)\ttrain_Loss: 0.009431\tval_Loss: 0.019322\n",
      "Train Epoch: 845/1000 (84%)\ttrain_Loss: 0.009421\tval_Loss: 0.019314\n",
      "Train Epoch: 846/1000 (84%)\ttrain_Loss: 0.009418\tval_Loss: 0.019340\n",
      "Train Epoch: 847/1000 (85%)\ttrain_Loss: 0.009410\tval_Loss: 0.019359\n",
      "Train Epoch: 848/1000 (85%)\ttrain_Loss: 0.009397\tval_Loss: 0.019350\n",
      "Train Epoch: 849/1000 (85%)\ttrain_Loss: 0.009383\tval_Loss: 0.019350\n",
      "Train Epoch: 850/1000 (85%)\ttrain_Loss: 0.009372\tval_Loss: 0.019340\n",
      "Train Epoch: 851/1000 (85%)\ttrain_Loss: 0.009365\tval_Loss: 0.019303\n",
      "Train Epoch: 852/1000 (85%)\ttrain_Loss: 0.009351\tval_Loss: 0.019307\n",
      "Train Epoch: 853/1000 (85%)\ttrain_Loss: 0.009345\tval_Loss: 0.019317\n",
      "Train Epoch: 854/1000 (85%)\ttrain_Loss: 0.009342\tval_Loss: 0.019314\n",
      "Train Epoch: 855/1000 (85%)\ttrain_Loss: 0.009335\tval_Loss: 0.019316\n",
      "Train Epoch: 856/1000 (86%)\ttrain_Loss: 0.009318\tval_Loss: 0.019326\n",
      "Train Epoch: 857/1000 (86%)\ttrain_Loss: 0.009303\tval_Loss: 0.019345\n",
      "Train Epoch: 858/1000 (86%)\ttrain_Loss: 0.009303\tval_Loss: 0.019347\n",
      "Train Epoch: 859/1000 (86%)\ttrain_Loss: 0.009292\tval_Loss: 0.019319\n",
      "Train Epoch: 860/1000 (86%)\ttrain_Loss: 0.009282\tval_Loss: 0.019300\n",
      "Train Epoch: 861/1000 (86%)\ttrain_Loss: 0.009275\tval_Loss: 0.019308\n",
      "Train Epoch: 862/1000 (86%)\ttrain_Loss: 0.009262\tval_Loss: 0.019293\n",
      "Train Epoch: 863/1000 (86%)\ttrain_Loss: 0.009255\tval_Loss: 0.019303\n",
      "Train Epoch: 864/1000 (86%)\ttrain_Loss: 0.009247\tval_Loss: 0.019328\n",
      "Train Epoch: 865/1000 (86%)\ttrain_Loss: 0.009238\tval_Loss: 0.019336\n",
      "Train Epoch: 866/1000 (86%)\ttrain_Loss: 0.009229\tval_Loss: 0.019315\n",
      "Train Epoch: 867/1000 (87%)\ttrain_Loss: 0.009221\tval_Loss: 0.019295\n",
      "Train Epoch: 868/1000 (87%)\ttrain_Loss: 0.009212\tval_Loss: 0.019279\n",
      "Train Epoch: 869/1000 (87%)\ttrain_Loss: 0.009202\tval_Loss: 0.019275\n",
      "Train Epoch: 870/1000 (87%)\ttrain_Loss: 0.009194\tval_Loss: 0.019262\n",
      "Train Epoch: 871/1000 (87%)\ttrain_Loss: 0.009184\tval_Loss: 0.019254\n",
      "Train Epoch: 872/1000 (87%)\ttrain_Loss: 0.009168\tval_Loss: 0.019253\n",
      "Train Epoch: 873/1000 (87%)\ttrain_Loss: 0.009159\tval_Loss: 0.019281\n",
      "Train Epoch: 874/1000 (87%)\ttrain_Loss: 0.009155\tval_Loss: 0.019297\n",
      "Train Epoch: 875/1000 (87%)\ttrain_Loss: 0.009146\tval_Loss: 0.019271\n",
      "Train Epoch: 876/1000 (88%)\ttrain_Loss: 0.009136\tval_Loss: 0.019259\n",
      "Train Epoch: 877/1000 (88%)\ttrain_Loss: 0.009133\tval_Loss: 0.019258\n",
      "Train Epoch: 878/1000 (88%)\ttrain_Loss: 0.009126\tval_Loss: 0.019270\n",
      "Train Epoch: 879/1000 (88%)\ttrain_Loss: 0.009115\tval_Loss: 0.019267\n",
      "Train Epoch: 880/1000 (88%)\ttrain_Loss: 0.009108\tval_Loss: 0.019239\n",
      "Train Epoch: 881/1000 (88%)\ttrain_Loss: 0.009103\tval_Loss: 0.019243\n",
      "Train Epoch: 882/1000 (88%)\ttrain_Loss: 0.009095\tval_Loss: 0.019270\n",
      "Train Epoch: 883/1000 (88%)\ttrain_Loss: 0.009078\tval_Loss: 0.019241\n",
      "Train Epoch: 884/1000 (88%)\ttrain_Loss: 0.009069\tval_Loss: 0.019229\n",
      "Train Epoch: 885/1000 (88%)\ttrain_Loss: 0.009065\tval_Loss: 0.019241\n",
      "Train Epoch: 886/1000 (88%)\ttrain_Loss: 0.009054\tval_Loss: 0.019245\n",
      "Train Epoch: 887/1000 (89%)\ttrain_Loss: 0.009048\tval_Loss: 0.019246\n",
      "Train Epoch: 888/1000 (89%)\ttrain_Loss: 0.009033\tval_Loss: 0.019261\n",
      "Train Epoch: 889/1000 (89%)\ttrain_Loss: 0.009024\tval_Loss: 0.019275\n",
      "Train Epoch: 890/1000 (89%)\ttrain_Loss: 0.009012\tval_Loss: 0.019266\n",
      "Train Epoch: 891/1000 (89%)\ttrain_Loss: 0.009012\tval_Loss: 0.019250\n",
      "Train Epoch: 892/1000 (89%)\ttrain_Loss: 0.009005\tval_Loss: 0.019237\n",
      "Train Epoch: 893/1000 (89%)\ttrain_Loss: 0.008999\tval_Loss: 0.019247\n",
      "Train Epoch: 894/1000 (89%)\ttrain_Loss: 0.008977\tval_Loss: 0.019262\n",
      "Train Epoch: 895/1000 (89%)\ttrain_Loss: 0.008974\tval_Loss: 0.019270\n",
      "Train Epoch: 896/1000 (90%)\ttrain_Loss: 0.008970\tval_Loss: 0.019302\n",
      "Train Epoch: 897/1000 (90%)\ttrain_Loss: 0.008960\tval_Loss: 0.019326\n",
      "Train Epoch: 898/1000 (90%)\ttrain_Loss: 0.008950\tval_Loss: 0.019311\n",
      "Train Epoch: 899/1000 (90%)\ttrain_Loss: 0.008944\tval_Loss: 0.019280\n",
      "Train Epoch: 900/1000 (90%)\ttrain_Loss: 0.008935\tval_Loss: 0.019277\n",
      "Train Epoch: 901/1000 (90%)\ttrain_Loss: 0.008929\tval_Loss: 0.019282\n",
      "Train Epoch: 902/1000 (90%)\ttrain_Loss: 0.008912\tval_Loss: 0.019287\n",
      "Train Epoch: 903/1000 (90%)\ttrain_Loss: 0.008906\tval_Loss: 0.019321\n",
      "Train Epoch: 904/1000 (90%)\ttrain_Loss: 0.008900\tval_Loss: 0.019357\n",
      "Train Epoch: 905/1000 (90%)\ttrain_Loss: 0.008897\tval_Loss: 0.019341\n",
      "Train Epoch: 906/1000 (90%)\ttrain_Loss: 0.008880\tval_Loss: 0.019323\n",
      "Train Epoch: 907/1000 (91%)\ttrain_Loss: 0.008869\tval_Loss: 0.019323\n",
      "Train Epoch: 908/1000 (91%)\ttrain_Loss: 0.008862\tval_Loss: 0.019321\n",
      "Train Epoch: 909/1000 (91%)\ttrain_Loss: 0.008852\tval_Loss: 0.019314\n",
      "Train Epoch: 910/1000 (91%)\ttrain_Loss: 0.008841\tval_Loss: 0.019319\n",
      "Train Epoch: 911/1000 (91%)\ttrain_Loss: 0.008831\tval_Loss: 0.019321\n",
      "Train Epoch: 912/1000 (91%)\ttrain_Loss: 0.008819\tval_Loss: 0.019320\n",
      "Train Epoch: 913/1000 (91%)\ttrain_Loss: 0.008815\tval_Loss: 0.019343\n",
      "Train Epoch: 914/1000 (91%)\ttrain_Loss: 0.008813\tval_Loss: 0.019349\n",
      "Train Epoch: 915/1000 (91%)\ttrain_Loss: 0.008799\tval_Loss: 0.019342\n",
      "Train Epoch: 916/1000 (92%)\ttrain_Loss: 0.008784\tval_Loss: 0.019334\n",
      "Train Epoch: 917/1000 (92%)\ttrain_Loss: 0.008780\tval_Loss: 0.019321\n",
      "Train Epoch: 918/1000 (92%)\ttrain_Loss: 0.008774\tval_Loss: 0.019311\n",
      "Train Epoch: 919/1000 (92%)\ttrain_Loss: 0.008768\tval_Loss: 0.019338\n",
      "Train Epoch: 920/1000 (92%)\ttrain_Loss: 0.008754\tval_Loss: 0.019345\n",
      "Train Epoch: 921/1000 (92%)\ttrain_Loss: 0.008743\tval_Loss: 0.019354\n",
      "Train Epoch: 922/1000 (92%)\ttrain_Loss: 0.008743\tval_Loss: 0.019352\n",
      "Train Epoch: 923/1000 (92%)\ttrain_Loss: 0.008741\tval_Loss: 0.019359\n",
      "Train Epoch: 924/1000 (92%)\ttrain_Loss: 0.008720\tval_Loss: 0.019363\n",
      "Train Epoch: 925/1000 (92%)\ttrain_Loss: 0.008712\tval_Loss: 0.019377\n",
      "Train Epoch: 926/1000 (92%)\ttrain_Loss: 0.008702\tval_Loss: 0.019389\n",
      "Train Epoch: 927/1000 (93%)\ttrain_Loss: 0.008704\tval_Loss: 0.019373\n",
      "Train Epoch: 928/1000 (93%)\ttrain_Loss: 0.008698\tval_Loss: 0.019337\n",
      "Train Epoch: 929/1000 (93%)\ttrain_Loss: 0.008685\tval_Loss: 0.019338\n",
      "Train Epoch: 930/1000 (93%)\ttrain_Loss: 0.008666\tval_Loss: 0.019358\n",
      "Train Epoch: 931/1000 (93%)\ttrain_Loss: 0.008662\tval_Loss: 0.019382\n",
      "Train Epoch: 932/1000 (93%)\ttrain_Loss: 0.008657\tval_Loss: 0.019386\n",
      "Train Epoch: 933/1000 (93%)\ttrain_Loss: 0.008644\tval_Loss: 0.019384\n",
      "Train Epoch: 934/1000 (93%)\ttrain_Loss: 0.008634\tval_Loss: 0.019382\n",
      "Train Epoch: 935/1000 (93%)\ttrain_Loss: 0.008626\tval_Loss: 0.019422\n",
      "Train Epoch: 936/1000 (94%)\ttrain_Loss: 0.008613\tval_Loss: 0.019480\n",
      "Train Epoch: 937/1000 (94%)\ttrain_Loss: 0.008608\tval_Loss: 0.019537\n",
      "Train Epoch: 938/1000 (94%)\ttrain_Loss: 0.008601\tval_Loss: 0.019588\n",
      "Train Epoch: 939/1000 (94%)\ttrain_Loss: 0.008594\tval_Loss: 0.019591\n",
      "Train Epoch: 940/1000 (94%)\ttrain_Loss: 0.008589\tval_Loss: 0.019619\n",
      "Train Epoch: 941/1000 (94%)\ttrain_Loss: 0.008587\tval_Loss: 0.019627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 942/1000 (94%)\ttrain_Loss: 0.008576\tval_Loss: 0.019613\n",
      "Train Epoch: 943/1000 (94%)\ttrain_Loss: 0.008557\tval_Loss: 0.019632\n",
      "Train Epoch: 944/1000 (94%)\ttrain_Loss: 0.008560\tval_Loss: 0.019674\n",
      "Train Epoch: 945/1000 (94%)\ttrain_Loss: 0.008553\tval_Loss: 0.019666\n",
      "Train Epoch: 946/1000 (94%)\ttrain_Loss: 0.008534\tval_Loss: 0.019637\n",
      "Train Epoch: 947/1000 (95%)\ttrain_Loss: 0.008526\tval_Loss: 0.019632\n",
      "Train Epoch: 948/1000 (95%)\ttrain_Loss: 0.008521\tval_Loss: 0.019598\n",
      "Train Epoch: 949/1000 (95%)\ttrain_Loss: 0.008514\tval_Loss: 0.019551\n",
      "Train Epoch: 950/1000 (95%)\ttrain_Loss: 0.008502\tval_Loss: 0.019541\n",
      "Train Epoch: 951/1000 (95%)\ttrain_Loss: 0.008495\tval_Loss: 0.019543\n",
      "Train Epoch: 952/1000 (95%)\ttrain_Loss: 0.008493\tval_Loss: 0.019522\n",
      "Train Epoch: 953/1000 (95%)\ttrain_Loss: 0.008482\tval_Loss: 0.019550\n",
      "Train Epoch: 954/1000 (95%)\ttrain_Loss: 0.008473\tval_Loss: 0.019633\n",
      "Train Epoch: 955/1000 (95%)\ttrain_Loss: 0.008470\tval_Loss: 0.019663\n",
      "Train Epoch: 956/1000 (96%)\ttrain_Loss: 0.008461\tval_Loss: 0.019644\n",
      "Train Epoch: 957/1000 (96%)\ttrain_Loss: 0.008450\tval_Loss: 0.019667\n",
      "Train Epoch: 958/1000 (96%)\ttrain_Loss: 0.008443\tval_Loss: 0.019704\n",
      "Train Epoch: 959/1000 (96%)\ttrain_Loss: 0.008441\tval_Loss: 0.019711\n",
      "Train Epoch: 960/1000 (96%)\ttrain_Loss: 0.008431\tval_Loss: 0.019747\n",
      "Train Epoch: 961/1000 (96%)\ttrain_Loss: 0.008414\tval_Loss: 0.019776\n",
      "Train Epoch: 962/1000 (96%)\ttrain_Loss: 0.008414\tval_Loss: 0.019701\n",
      "Train Epoch: 963/1000 (96%)\ttrain_Loss: 0.008409\tval_Loss: 0.019662\n",
      "Train Epoch: 964/1000 (96%)\ttrain_Loss: 0.008397\tval_Loss: 0.019673\n",
      "Train Epoch: 965/1000 (96%)\ttrain_Loss: 0.008387\tval_Loss: 0.019640\n",
      "Train Epoch: 966/1000 (96%)\ttrain_Loss: 0.008377\tval_Loss: 0.019693\n",
      "Train Epoch: 967/1000 (97%)\ttrain_Loss: 0.008368\tval_Loss: 0.019642\n",
      "Train Epoch: 968/1000 (97%)\ttrain_Loss: 0.008361\tval_Loss: 0.019617\n",
      "Train Epoch: 969/1000 (97%)\ttrain_Loss: 0.008358\tval_Loss: 0.019619\n",
      "Train Epoch: 970/1000 (97%)\ttrain_Loss: 0.008353\tval_Loss: 0.019637\n",
      "Train Epoch: 971/1000 (97%)\ttrain_Loss: 0.008347\tval_Loss: 0.019706\n",
      "Train Epoch: 972/1000 (97%)\ttrain_Loss: 0.008339\tval_Loss: 0.019764\n",
      "Train Epoch: 973/1000 (97%)\ttrain_Loss: 0.008323\tval_Loss: 0.019762\n",
      "Train Epoch: 974/1000 (97%)\ttrain_Loss: 0.008327\tval_Loss: 0.019784\n",
      "Train Epoch: 975/1000 (97%)\ttrain_Loss: 0.008319\tval_Loss: 0.019923\n",
      "Train Epoch: 976/1000 (98%)\ttrain_Loss: 0.008317\tval_Loss: 0.019819\n",
      "Train Epoch: 977/1000 (98%)\ttrain_Loss: 0.008309\tval_Loss: 0.019772\n",
      "Train Epoch: 978/1000 (98%)\ttrain_Loss: 0.008299\tval_Loss: 0.019832\n",
      "Train Epoch: 979/1000 (98%)\ttrain_Loss: 0.008291\tval_Loss: 0.019769\n",
      "Train Epoch: 980/1000 (98%)\ttrain_Loss: 0.008280\tval_Loss: 0.019770\n",
      "Train Epoch: 981/1000 (98%)\ttrain_Loss: 0.008275\tval_Loss: 0.019849\n",
      "Train Epoch: 982/1000 (98%)\ttrain_Loss: 0.008263\tval_Loss: 0.019806\n",
      "Train Epoch: 983/1000 (98%)\ttrain_Loss: 0.008256\tval_Loss: 0.019694\n",
      "Train Epoch: 984/1000 (98%)\ttrain_Loss: 0.008254\tval_Loss: 0.019710\n",
      "Train Epoch: 985/1000 (98%)\ttrain_Loss: 0.008243\tval_Loss: 0.019750\n",
      "Train Epoch: 986/1000 (98%)\ttrain_Loss: 0.008242\tval_Loss: 0.019710\n",
      "Train Epoch: 987/1000 (99%)\ttrain_Loss: 0.008232\tval_Loss: 0.019680\n",
      "Train Epoch: 988/1000 (99%)\ttrain_Loss: 0.008224\tval_Loss: 0.019714\n",
      "Train Epoch: 989/1000 (99%)\ttrain_Loss: 0.008222\tval_Loss: 0.019732\n",
      "Train Epoch: 990/1000 (99%)\ttrain_Loss: 0.008208\tval_Loss: 0.019862\n",
      "Train Epoch: 991/1000 (99%)\ttrain_Loss: 0.008206\tval_Loss: 0.019892\n",
      "Train Epoch: 992/1000 (99%)\ttrain_Loss: 0.008206\tval_Loss: 0.019877\n",
      "Train Epoch: 993/1000 (99%)\ttrain_Loss: 0.008182\tval_Loss: 0.019898\n",
      "Train Epoch: 994/1000 (99%)\ttrain_Loss: 0.008188\tval_Loss: 0.019901\n",
      "Train Epoch: 995/1000 (99%)\ttrain_Loss: 0.008180\tval_Loss: 0.019919\n",
      "Train Epoch: 996/1000 (100%)\ttrain_Loss: 0.008169\tval_Loss: 0.019940\n",
      "Train Epoch: 997/1000 (100%)\ttrain_Loss: 0.008166\tval_Loss: 0.019918\n",
      "Train Epoch: 998/1000 (100%)\ttrain_Loss: 0.008149\tval_Loss: 0.019878\n",
      "Train Epoch: 999/1000 (100%)\ttrain_Loss: 0.008145\tval_Loss: 0.019864\n",
      "Train Epoch: 1000/1000 (100%)\ttrain_Loss: 0.008143\tval_Loss: 0.019890\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Train the network\n",
    "plt.figure() # monitor loss curve during training\n",
    "# for loop over epochs\n",
    "for epoch in range(num_epoch):\n",
    "    # classical forward pass -> predict new output from train data\n",
    "    Y_pred_train_oh = net(X_train)\n",
    "    # compute loss    \n",
    "    loss_train = loss_func(Y_pred_train_oh, Y_train_oh)\n",
    "    \n",
    "    # Compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Note: Calling .backward() mutiple times accumulates the gradient (by addition) for each parameter. This is why you should call optimizer.zero_grad() after each .step() call\n",
    "    # Note that following the first .backward call, a second call is only possible after you have performed another forward pass.\n",
    "    loss_train.backward()\n",
    "    # perform a parameter update based on the current gradient (stored in .grad attribute of a parameter)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # TODO**\n",
    "    # forward pass for validation\n",
    "    Y_pred_val_oh = net(X_test)\n",
    "    loss_val = loss_func(Y_pred_val_oh, Y_test_oh)\n",
    "    # TODO**\n",
    "    \n",
    "    # compute actual train accuracy\n",
    "    y_pred_train = np.argmax(Y_pred_train_oh.cpu().detach().numpy(), axis=1)\n",
    "    correct_train = np.sum(y_pred_train == Y_train.numpy())\n",
    "    \n",
    "    # compute actual val accuracy\n",
    "    y_pred_val = np.argmax(Y_pred_val_oh.cpu().detach().numpy(), axis=1)\n",
    "    correct_val = np.sum(y_pred_val == Y_val.numpy())\n",
    "    \n",
    "    # plot train and val loss and accuracies\n",
    "    plt.scatter(epoch, loss_train.data.item(), color='r', s=10, marker='o')\n",
    "    plt.scatter(epoch, loss_val.data.item(), color='b', s=10, marker='o')\n",
    "    plt.scatter(epoch, correct_train/Y_train.shape[0], color='m', s=10, marker='o') \n",
    "    plt.scatter(epoch, correct_val/Y_val.shape[0], color='c', s=10, marker='o')\n",
    "    \n",
    "    # print message with actual losses\n",
    "    print('Train Epoch: {}/{} ({:.0f}%)\\ttrain_Loss: {:.6f}\\tval_Loss: {:.6f}'.format(\n",
    "    epoch+1, num_epoch, epoch/num_epoch*100, loss_train.item(), loss_val.item()))\n",
    "       \n",
    "\n",
    "# show training and validation loss    \n",
    "plt.legend(['train-loss','val-loss','train-acc','val-acc'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(os.path.join(path, 'results/irisflower_loss.png'))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "JY4k91wyEcZM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training was: 0.28438523411750793\n",
      "Test loss after training is: 0.019889995455741882\n",
      "Test accuracy before training:  0.0 %\n",
      "Test accuracy after training:  94.73684210526315 %\n"
     ]
    }
   ],
   "source": [
    "# %% Test results\n",
    "# TODO**\n",
    "# forward pass \n",
    "# Y_pred_test_oh is on the GPU, because net and X_test are on the GPU, but we want it on the CPU from now on.\n",
    "Y_pred_test_oh = net(X_test)\n",
    "# compute and print losses\n",
    "loss_test = loss_func(Y_pred_test_oh, Y_test_oh)\n",
    "# TODO** \n",
    "print('Test loss before training was:', loss_test_before.item())\n",
    "print('Test loss after training is:', loss_test.item())\n",
    "\n",
    "# compute and print accuracies\n",
    "y_pred_test = np.argmax(Y_pred_test_oh.cpu().detach().numpy(), axis=1)\n",
    "correct = np.sum(y_pred_test == Y_test.numpy())\n",
    "print('Test accuracy before training: ', correct_before/Y_test.shape[0]*100, '%')\n",
    "print('Test accuracy after training: ', correct/Y_test.shape[0]*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "nFSpOlPJEcZP"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train\u001b[38;5;241m.\u001b[39mcpu(), y_pred_train)\n\u001b[1;32m      4\u001b[0m SVC(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#%% plot test confusion matrix\u001b[39;00m\n\u001b[1;32m      8\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_pred_test, predictions, labels\u001b[38;5;241m=\u001b[39mclf\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:820\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    818\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:433\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:613\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    610\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[0;32m--> 613\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39misspmatrix(X):\n\u001b[1;32m    623\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:972\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# evaluation module\n",
    "clf = SVC(random_state=0)\n",
    "clf.fit(X_train, y_pred_train)\n",
    "SVC(random_state=0)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "#%% plot test confusion matrix\n",
    "cm = confusion_matrix(y_pred_test, predictions, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(path, 'results/irisflower_confusion.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNrGgSWrLU3E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
